## 摘要

多模态大型语言模型发展迅速，各类模型层出不穷。然而，视觉 - 语言大型模型（LVLMs）的可解释性仍是一个有待深入探索的领域。尤其在面对思维链推理等更复杂任务时，其内部机制仍像难以解读的 “黑箱”。通过研究图像与文本之间的交互关系及信息流，我们发现：在 LLaVA1.5 等模型中，**与文本存在语义关联的图像令牌，在大型语言模型（LLM）解码层更易出现信息流汇聚，且这类图像令牌会获得更高的注意力分数；而与文本关联性较弱的图像令牌则无此信息流汇聚现象，仅能获得极低的注意力分数**。为高效利用图像信息，我们提出一种新的图像令牌缩减方法 ——Simignore。该方法通过**计算图像与文本嵌入之间的相似度**，忽略与文本无关且不重要的图像令牌，旨在提升视觉 - 语言大型模型的复杂推理能力。大量实验结果表明，我们的方法在复杂推理任务中具有有效性。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508271140637.png)
图一：简易框架图

代码链接：[https://github.com/FanshuoZeng/Simignore](https://github.com/FanshuoZeng/Simignore)

## 引言

近年来，视觉 - 语言大型模型（LVLMs）发展迅猛，已成为计算机视觉与自然语言处理领域的研究热点。多模态大型语言模型（LLMs）借助思维链（CoT）提示生成中间推理链作为推理依据来推导答案，在复杂推理任务中展现出优异性能。视觉 - 语言大型模型利用 CLIP 等视觉编码器对图像块进行处理，得到视觉令牌，并将其作为视觉信息的上下文，以完成视觉 - 文本推理任务。视觉编码器会将这些图像块处理成数百个令牌，例如，CLIP（Radford 等人，2021）会生成 576 个令牌，siglip（Zhai 等人，2023）会生成 729 个令牌。

这些额外的图像令牌会增加计算量。已有部分研究（Chu 等人，2023a；Yuan、Li 与 Sun，2023）通过采用参数更少的小型 LLM 来降低推理成本，但此类方法也会导致 LLM 的推理能力下降（Chu 等人，2024）。因此，更优的方案是通过**缩短输入令牌长度来降低计算成本**。同时，相关研究（Shang 等人，2024；Chen 等人，2024）表明，**并非所有图像令牌对模型推理都至关重要，适当缩减图像令牌数量可提升 LLM 的推理能力**，显著提高其在视觉复杂推理任务中的准确率。

目前已有一些图像令牌缩减相关研究：FastV（Chen 等人，2024）通过可视化 LLM 推理过程中系统令牌、图像令牌与用户令牌的注意力分数，发现图像令牌的权重在第二层之后变得极低，因此基于注意力分数排序，在第二层舍弃了一半分数较低的图像令牌；LLaVA-PruMerge（Shang 等人，2024）利用类别令牌与视觉令牌之间的自注意力机制，观察二者间的注意力分布，发现大多数视觉令牌与类别令牌的注意力值接近零，这表明这些令牌在图像表示中并非关键。因此，该方法**基于视觉令牌与 CLS 令牌的空间相似度选择合适的图像令牌，随后对筛选后的令牌进行聚类，并将聚类后的令牌与未筛选的令牌融合**，以保持视觉信息的完整性与丰富性。

针对上述两种图像令牌缩减方法，尽管这两种图像标记近似方法取得了一定成效，但仍存在**局限性**：（1）上述两种图像令牌缩减方法**未考虑与文本提示的交互**，极易过滤掉与文本 “语义” 相关的令牌；（2）目前**图像令牌与文本令牌之间的交互机制尚不明确**，LLM 如何利用图像令牌进行作答仍有待进一步探索。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508271507337.png)
图 2：我们发现，信息流会在与提示选项相关的区域汇聚，例如蘑菇（mushroom）和桡足类动物（copepod）所在区域。

本文通过信息流来探究图像令牌与文本提示之间的交互关系。我们将**信息流**定义为：**在图像令牌的注意力机制处理过程中，图像令牌逐步向与文本相关的语义汇聚的过程**。具体而言，我们可视化了 LLM 解码器中图像令牌的注意力分数，并将其叠加到图像块上，发现与文本相关的图像块会出现信息流汇聚现象。如图 2 所示，我们可观察到，与文本选项（如蘑菇、越橘）相关的图像块在网络中会出现信息流汇聚，且这些图像块获得了更高的注意力分数。在其他场景下，若回答问题无需参考图像，网络则不会对图像给予过多关注，信息流也不会出现汇聚。

因此，我们设计了**名为 Simignore 的图像 - 文本令牌过滤算法，以避免无关令牌造成干扰**。具体而言，如图 1 所示，我们将图像令牌与提示令牌的嵌入映射到同一相似度度量空间（该空间中所有令牌均表现为二维平面上的点）；随后，利用相似度算法计算图像令牌与文本令牌之间的相似度值，选择相似度最高的 K 个图像令牌并记录其下标；最后，保留所选图像令牌，对于未选中的令牌，将其对应的注意力掩码设为 0 以实现忽略。

综上，本文的贡献如下：

- 通过信息流分析发现，在 LLM 解码器中，与文本标记存在语义关联的图像更易出现信息流汇聚现象。
- 提出一种新方法 Simignore，该方法旨在保留与文本存在交互的图像令牌，忽略无关且不重要的图像令牌，从而提升 LVLMs 在复杂推理任务中的能力。
- 通过大量实验证明，在不同 LVLMs 的视觉推理任务中，本文方法对复杂推理具有有效性。

## 相关工作

### 多模态大型语言模型

多模态大型语言模型能够处理和理解文本、图像等多种不同模态的数据，因此在各类任务中展现出卓越性能。CLIP（Radford 等人，2021）与 BLIP（Li 等人，2022）采用预训练方法，将图像与文本映射到同一特征空间，使模型能够通过比较特征向量建立图像与文本之间的关联。LLaVA（Liu 等人，2023）、MiniGPT-4（Zhu 等人，2023）、Qwen-VL（Bai 等人，2023）、CogVLM（Wang 等人，2023b）及其他相关方法（Sun 等人，2024、2023；Zhou 等人，2024；Sun 等人，2025；Zhao 等人，2024a、b；Wang 等人，2025；Tang 等人，2024b、a；Wang 等人，2022；Xu 等人，2024；Hu 等人，2025、2024）则利用预训练视觉 Transformer（ViT）处理图像信息。

EAH（Zhang 等人，2024b；Wei 与 Zhang，2024；Yuan 等人，2024；Zhang 等人，2024a）首先从信息流角度对视觉 - 语言模型（VLM）这一 “黑箱” 进行解释，随后发现信息流与幻觉现象呈负相关，进而提出注意力头增强方法以缓解幻觉问题。近期研究表明，多模态学习在文档修复（Li 等人，2024；Wang 等人，2024、2023a）、医学图像转换（Chen、Pun 与 Wang，2024）、缺失模态预测（Huo 等人，2024）、情感识别（Dong 等人，2025）及多模态生成（Shen 与 Tang，2024；Shen 等人，2024a；Shen 等人，2024b）等应用中具有巨大潜力。

### 图像 - 文本多模态相似度

图像 - 文本相似度研究是多模态学习领域的关键方向，主要用于评估图像与文本之间的一致性。近年来，多种方法显著提升了跨模态检索的准确率，这些方法包括联合嵌入技术（Frome 等人，2013；Kiros、Salakhutdinov 与 Zemel，2014；Faghri 等人，2017）、Diao 等人提出的 SGRAF 网络（Diao 等人，2021）、Zeng 等人提出的实例比较嵌入方法（Zeng 等人，2024）、Yang 等人提出的统一比较学习方法（Yang 等人，2022），以及 Liu 等人提出的余弦度量空间中的图像特征与类别语义嵌入方法（Liu 等人，2021）。其中，余弦相似度因其在减小类内方差、提升识别能力方面的优势，常被用于识别与文本信息关联性强的图像令牌。

## 方法

为实现信息流可视化，我们采用注意力分数技术，以全面理解信息流特征。注意力分数能够反映模型的前向处理过程，展示不同输入元素对最终输出的贡献。

### 图像令牌对输出令牌的影响率

针对 ScienceQA 数据集（Lu 等人，2022）等复杂推理任务的输出令牌，在第ra层中，我们定义 $\mathcal{G}$ 为所有令牌的索引集合，且 $\mathcal{G}$ 可分为三部分，分别对应系统令牌、图像令牌与用户令牌的索引集合： $$\mathcal{G}=\mathcal{S}+\mathcal{I}+\mathcal{U} \quad(1)$$其中，$\mathcal{S}=\{1, \dots, N_{sys}\}$ 表示系统令牌的索引，$N_{sys}$ 表示系统令牌的长度；$\mathcal{I}=\{N_{sys}+1, \dots, N_{sys}+N_{img}\}$ 表示图像令牌的索引，$N_{img}$ 表示图像令牌的长度；$\mathcal{U}=\{N_{sys}+N_{img}+1, \dots, N_{sys}+N_{img}+N_{user}\}$ 表示用户令牌的索引，$N_{user}$ 表示用户令牌的长度。 

定义 $A_{i,j}$ 为输出令牌对不同类型令牌的注意力总分数。对于第 i 个查询令牌，其从系统令牌、图像令牌与用户令牌获得的注意力之和为 1： $$\sum_{j \in \mathcal{S}} A_{i,j}+\sum_{j \in \mathcal{I}} A_{i,j}+\sum_{j \in \mathcal{U}} A_{i,j}=1 \quad(2)$$为确保每个令牌的注意力分数之和为 1，需对上述求和结果进行归一化，以计算图像令牌的注意力总分数： $$\lambda_{img}^{j}=\sum_{j \in \mathcal{I}} A_{i,j} \quad(3)$$在 LLaVA1.5 模型中，存在 576 个图像令牌，注意力掩码矩阵的形状为 $(B, H, N_{img}, N_{img})$ 。首先，我们对注意力矩阵进行无序化处理，将其形状转换为 $(H, N_{img}, N_{img})$（注意力矩阵如图 2 和图 4 所示，横坐标代表查询（Q），纵坐标代表键（K），纵坐标第一行代表系统令牌、图像令牌及用户令牌对输出令牌的影响率）。以索引为 35-611 的图像令牌为例，我们可得到 1×576 维的向量，该向量对应 576 个令牌的注意力分数；随后，将注意力分数重塑为 14×14 的图像块，并叠加到原始图像上，得到影响率热力图。

通过影响率热力图可发现，图像会在提示中出现的选项区域呈现明显的汇聚现象。因此，去除冗余信息、保留有效图像信息的一种方法是计算图像令牌与文本令牌之间的相似度。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508271601818.png)
图 4：图像令牌的注意力分数影响率

### 图像与文本嵌入的相似度计算

本节将详细介绍本文聚焦于文本相关图像信息的方法，具体流程如图 3 左侧所示。首先，需介绍图像嵌入与文本嵌入的概念。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508271531463.png)
图 3：通过图像与文本嵌入间的相似度计算来增强多模态大型语言模型复杂推理能力的整体框架。我们将图像令牌与提示令牌的嵌入映射到同一相似度度量空间，此过程涉及正则化等操作。在此，我们计算它们的相似度值，随后筛选出相似度最高的 K 个图像令牌并将其视为重要令牌；对于未被选中的令牌，我们通过将其注意力掩码设为 0 的方式忽略它们。

在推理过程中，输入图像为 $X \in \mathbb{R}^{C \times H \times W}$ ，其中 C 表示通道数，$H \times W$ 表示图像尺寸。随后，将图像输入视觉 - 语言预训练模型（VLP）进行处理： $$\mathbb{F}: \mathbb{R}^{C \times H \times W} \to \mathbb{R}^{C' \times H' \times W'} \quad(4)$$ 其中，$\mathbb{F}$ 表示卷积操作，$C'$ 表示新的通道数，$H'$ 与 $W'$ 表示新的空间维度。接下来，利用自适应池化（adaptive-pooling）将卷积后的特征映射下采样至特定尺寸 $N_{img} \times I$，自适应池化函数 $\mathbb{AP}$ 定义为： $$\mathbb{AP}: \mathbb{R}^{C' \times H' \times W'} \to \mathbb{R}^{N_{img} \times I} \quad(5)$$整个过程可表示为： $$X'=\mathbb{AP}(\mathbb{F}(X)) \quad(6)$$其中，$X' \in \mathbb{R}^{N_{img} \times I}$ 表示图像的特征。

文本嵌入后的维度为 $\text{TextEmb} \in \mathbb{R}^{N_{user} \times T}$ 。我们从文本中去除系统令牌，仅保留上下文、问题及选项令牌。其中，$N_{user}$ 表示文本令牌的长度，T 表示文本嵌入的维度。为实现图像信息与文本信息的融合，还需将图像与文本嵌入到同一特征空间。因此，通常需要将图像特征 $N_{img} \times I$ 的维度与文本特征 $N_{user} \times T$ 的维度对齐，特征对齐函数 $\mathbb{Fo}$ 定义为： $$\mathbb{Fo}: \mathbb{R}^{N_{img} \times I} \to \mathbb{R}^{N_{img} \times T} \quad(7)$$$$\text{ImgEmb}=\mathbb{Fo}(X') \quad(8)$$其中，$\text{ImgEmb} \in \mathbb{R}^{N_{img} \times T}$ 与文本具有相同的特征维度。此时，我们已获得图像令牌与文本令牌的嵌入，接下来详细介绍本文方法（如图 3 右侧所示）。

为计算图像嵌入与文本嵌入之间的相关性，首先对 ImgEmb 与 TextEmb 进行归一化处理： $$\text{ImgEmb}_{norm}(i,:)=\frac{\text{ImgEmb}(i,:)}{\|\text{ImgEmb}(i,:)\|} \quad(9)$$$$\text{TextEmb}_{norm}(j,:)=\frac{\text{TextEmb}(j,:)}{\|\text{TextEmb}(j,:)\|} \quad(10)$$随后，计算余弦相似度矩阵： $$S(i,j)=\text{ImgEmb}_{norm}(i,:) \cdot \text{TextEmb}_{norm}(j,:)^T \quad(11)$$其中，$S \in \mathbb{R}^{N_{img} \times N_{user}}$，$S(i,j)$ 表示第i个图像令牌与第j个文本令牌之间的相似度。

接下来，筛选出与文本令牌相似度最高的K个图像令牌，并记录其索引。具体而言，首先需将相似度矩阵 S 展开为维度为 $N_{img} \times N_{user}$ 的一维数组 $S_{flat}$ ： $$S_{flat}=\text{flatten}(S) \quad(12)$$其中，$S_{flat}$ 的维度为 $1 \times P$ ，$P=N_{img} \times N_{user}$ 。  

随后，对展开后的一维数组 $S_{flat}$ 进行排序，获取相似度最高的K个下标： $$\text{Indices}=\text{argsort}(S_{flat})[-K:]$$利用这些一维下标，可确定对应图像令牌的下标。由于每个下标 i 对应一个二维坐标 $(\lfloor i / N_{user} \rfloor, i \% N_{user})$ ，因此只需取 $i // N_{user}$ 部分即可表示图像令牌的下标： $$\text{Indimg}_i=\left\lfloor \text{Indices}_i / N_{user} \right\rfloor \quad(14)$$之后，将不属于 $\text{Indimg}$ 的令牌的注意力掩码设为 0： $$M_{i}^{\mathcal{I}}= \begin{cases}1 & \text{if } i \in \text{Indimg} \\ 0 & \text{else} \end{cases}$$最后，将系统令牌、图像令牌与用户令牌的注意力掩码拼接： $$M=M^{\mathcal{S}}+M^{\mathcal{I}}+M^{\mathcal{U}} \quad(16)$$其中，$M^{\mathcal{S}}=\text{ones} \in \mathbb{R}^{1 \times N_{sys}}$ ，$M^{\mathcal{U}}=\text{ones} \in \mathbb{R}^{1 \times N_{user}}$ 。
## 实验

### 数据集与实现细节

ScienceQA 数据集（Lu 等人，2022）是目前唯一可用于复杂推理的数据集，包含 21,208 道来自中小学科学课程的多选项问答题目。一道典型题目包含多模态上下文、正确选项、通用背景知识及详细解析。本文实验在单张 4090D GPU 上进行。
### 注意力汇聚

人类在解决视觉复杂推理问题时，往往会在图像中寻找与文本相关的信息，这种行为能高效获取图像的关键信息。为探究多模态大型模型推理过程中对图像信息的关注程度，如图 2 所示，我们获取了图像令牌的注意力分数，并将其叠加到原始图像上。结果发现，图像会对提示中出现的选项呈现明显的关注倾向，具体而言，LLM 会特别关注图像中与文本信息密切相关的文字或物体。这表明在推理过程中图像与文本存在交互，且 LVLMs 更倾向于关注与文本相关的图像令牌。
### 图像 - 文本相似度

我们探究了图像令牌与文本令牌之间的关系。将嵌入映射到同一余弦度量空间后（如图 5 所示），发现图像令牌与文本令牌在该空间中存在相似度。具体而言，图像令牌与文本令牌的分布特征相似，均集中在 $X<0$ 的度量空间内。在蓝色矩形区域中，图像令牌与文本令牌分布密集，且具有较强的相似度。

基于注意力汇聚现象及图像与文本在余弦度量空间中的相似度，我们设计了一种筛选与文本关联性强的图像令牌的方法，使 LVLMs 能够获取有用的图像令牌，避免无关令牌对其推理过程产生干扰。

我们在 ScienceQA 数据集上使用不同的 LVLMs 进行评估，包括 LLaVA-v1.5 的 7B 和 13B 模型，以及 Mipha-3B 模型。如表 1 所示，我们展示了不同基准模型及应用本文方法后的模型性能。结果显示，应用本文方法后的模型相较于基准模型有显著提升，其中 LLaVA1.5-7B 模型的准确率提升了 2.87%。在以 ScienceQA（图像）为数据集的实验中，无论以 Vicuna-7B 还是 Vicuna-13B 为骨干模型，本文方法在零样本学习（Zero-shot Learning）中均表现最优。

表 1：不同 LVLMs 在 ScienceQA 基准数据集上的性能对比（“Res” 表示输入图像分辨率）

| 方法                              | 学习方式 | LLM 骨干模型    | 分辨率 | ScienceQA（图像）准确率（%） |
| ------------------------------- | ---- | ----------- | --- | ------------------- |
| Instruct-BLIP（Dai 等人，2024）      | 零样本  | Vicuna-7B   | 224 | 60.50               |
| Instruct-BLIP（Dai 等人，2024）      | 零样本  | Vicuna-13B  | 224 | 63.10               |
| BLIP-2（Li 等人，2023a）             | 零样本  | Vicuna-13B  | 224 | 61.03               |
| Shikra（Chen 等人，2023）            | 零样本  | Vicuna-13B  | 224 | 45.80               |
| DDCoT（GPT3.5）（Zheng 等人，2023）    | 零样本  | 175B        | -   | 72.53               |
| DDCoT（MiniGPT-4）（Zheng 等人，2023） | 零样本  | Vicuna-13B  | -   | 56.72               |
| Ying-VLM（Li 等人，2023b）           | 零样本  | -           | -   | 55.70               |
| Otter（Zhao 等人，2023）             | 零样本  | -           | -   | 66.30               |
| MiniGPT-4（Zhu 等人，2023）          | 零样本  | Vicuna-13B  | 336 | 42.34               |
| Qwen-VL-Chat（Bai 等人，2023）       | 零样本  | Qwen-7B     | 448 | 68.21               |
| MobileVLM（Chu 等人，2023b）         | 零样本  | MobileLLaMA | 336 | 61.00               |
| Qwen-VL（Bai 等人，2023）            | 零样本  | Qwen-7B     | 448 | 67.12               |
| Mipha-3B（Liu 等人，2023）           | 零样本  | Phi-2-2.7B  | 384 | 70.40               |
| Mipha-3B + 本文方法                 | 零样本  | Phi-2-2.7B  | 384 | 70.85               |
| LLaVA1.5（Liu 等人，2023）           | 零样本  | Vicuna-7B   | 336 | 65.15               |
| LLaVA1.5 + 本文方法                 | 零样本  | Vicuna-7B   | 336 | 68.02               |
| LLaVA1.5（Liu 等人，2023）           | 零样本  | Vicuna-13B  | 336 | 72.09               |
| LLaVA1.5 + 本文方法                 | 零样本  | Vicuna-13B  | 336 | 73.23               |

### 消融实验

#### 忽略不同数量的图像令牌

为探究忽略不同数量的图像令牌对 LLM 复杂推理任务的影响，我们设置了十组对比实验。如表 2 所示，我们获取了 LLM 在 ScienceQA（图像）数据集上忽略不同数量图像令牌时的准确率及运行时间（注：本文使用的 LLM 为 LLaVA1.5-7B）。结果发现，忽略部分图像令牌时，LLM 的运行时间也会相应减少，且忽略的图像令牌数量越多，消耗的时间越少。这表明，当我们忽略部分不重要的图像令牌后，原本密集的注意力矩阵会变得稀疏，从而大幅降低计算量；随着忽略令牌数量的增加，注意力矩阵的稀疏性进一步提升，计算量也随之减少。

有趣的是，当忽略所有图像令牌时，LLM 的准确率仍能达到 53.35%。但由于并非所有数据都具有相同的图像特征，因此自适应选择合适的图像令牌忽略数量至关重要，这也将是我们未来的研究方向。

表 2：忽略不同数量图像令牌时 LLM 的准确率与运行时间（基准模型：LLaVA1.5-7B）

| 忽略数量    | 准确率（%） | 运行时间（/ 秒） |
| ------- | ------ | --------- |
| 72/576  | 67.23  | 292       |
| 124/576 | 68.02  | 279       |
| 144/576 | 67.67  | 277       |
| 216/576 | 67.28  | 265       |
| 288/576 | 66.48  | 260       |
| 360/576 | 66.04  | 255       |
| 432/576 | 65.54  | 252       |
| 504/576 | 63.96  | 246       |
| 576/576 | 53.35  | 244       |
| 基准模型    | 65.15  | 303       |

本节还探究了忽略不同数量图像令牌、忽略不同重要程度的图像令牌及使用不同相似度算法对实验结果的影响，此外，还开展了忽略文本标记的相关实验。

#### 不同重要程度的图像令牌对推理能力的影响

为探究图像令牌在提升模型复杂推理能力中的重要性，我们进行了消融实验。如表 3 所示，我们开展了四组实验，分别为：忽略不重要的图像令牌（实验 1）、忽略中等重要程度的图像令牌（实验 2）、忽略重要的图像令牌（实验 3）及随机忽略图像令牌（实验 4）。其中，图像令牌的重要程度根据其与文本的相似度计算得出。在实验 2 中，我们进行了十组实验，并取平均值作为最终结果。

结果显示，实验 1 取得了最佳效果，实验 3 效果较差，这表明重要的图像令牌对 LLM 的复杂推理任务具有积极作用，而不重要的图像令牌则具有消极作用；实验 2 的结果介于实验 1 与实验 3 之间，表明 LLM 复杂推理任务的准确率与图像令牌的重要程度呈正相关；实验 4 的结果既有高于基准模型的情况，也有低于基准模型的情况。我们认为，当随机忽略部分重要图像令牌时，准确率会降低；而当随机忽略部分不重要图像令牌时，准确率会提高。实验 4 的详细结果已在补充材料中提供。

表 3：不同重要程度的图像令牌对复杂推理任务的影响

|忽略类型|模型|忽略数量|准确率（%）|
|---|---|---|---|
|不重要令牌|LLaVA1.5-7B|124/576|68.02|
|中等重要令牌|LLaVA1.5-7B|124/576|64.55|
|重要令牌|LLaVA1.5-7B|124/576|61.73|
|随机令牌|LLaVA1.5-7B|124/576|65.12|
|基准模型|LLaVA1.5-7B|0/576|65.15|
#### 不同相似度算法的影响

为探究不同相似度算法对实验结果的影响，我们开展了三组不同实验，分别采用余弦相似度、欧氏距离相似度和曼哈顿距离相似度，并获取了 LLM 在不同方案下的准确率。如表 4 所示，结果发现采用余弦相似度算法时提升效果最佳，另外两种算法也实现了小幅提升。这是因为在嵌入相似度计算任务中，余弦相似度通过测量向量之间的夹角而非绝对距离来实现计算，能够忽略尺度差异，更易捕捉图像与文本嵌入之间的语义关联，因此表现更优。在高维空间中，向量之间的距离往往趋于一致，而欧氏距离和曼哈顿距离会受向量大小与尺度的影响，无法同样有效地衡量这种相似度。

表 4：不同相似度计算算法对 LLM 复杂推理准确率的影响

| 模型          | 算法    | 忽略数量 | 准确率（%） |
| ----------- | ----- | ---- | ------ |
| LLaVA1.5-7B | 余弦相似度 | 124  | 68.02  |
| LLaVA1.5-7B | 欧氏距离  | 124  | 66.73  |
| LLaVA1.5-7B | 曼哈顿距离 | 124  | 66.88  |
| 基准模型        | -     | 0    | 65.15  |

#### 对图像令牌嵌入的聚类分析

为进一步探究本文方法如何提升 LLM 的作答准确率，我们对图像数据进行了相关分析。以 ScienceQA 数据集中 ID 为 1879 的问题为例（图 6 展示了该问题的提示与图像），在基准方法中，LLM 给出了错误答案；而应用本文提出的方法后，LLM 成功给出了正确答案。

为探究忽略部分图像令牌后 LLM 能够正确推理答案的原因，我们对图像嵌入进行 K 均值聚类，将每个图像令牌的嵌入张量映射为二维平面上的点。图 7 展示了 576 个图像令牌的嵌入经聚类后在二维平面上的映射点，其中红点表示我们忽略的图像令牌映射点。我们注意到，本文方法忽略的图像令牌集中在聚类 0（cluster0）中，因此我们忽略了聚类 0 中的所有 258 个图像令牌，仍得到了正确答案。此外，我们还开展了对比实验，结果如表 5 所示。

通过对上述实验的分析，我们提出以下推测：聚类 0 中存在一些 “干扰” 令牌，这些令牌会影响 LLM 对图像的理解，导致作答错误；聚类 2（cluster2）和聚类 4（cluster4）包含一些重要令牌，若忽略这些令牌，也会影响 LLM 对图像的理解。

表 5：忽略不同聚类中的图像令牌后 LLM 的作答结果（“√” 表示忽略，“×” 表示不忽略；“T” 表示答案正确，“F” 表示答案错误）

| 聚类 0 | 聚类 1 | 聚类 2 | 聚类 3 | 聚类 4 | 结果  |
| ---- | ---- | ---- | ---- | ---- | --- |
| ×    | ×    | ×    | ×    | ×    | F   |
| √    | ×    | ×    | ×    | ×    | T   |
| √    | √    | ×    | ×    | ×    | T   |
| √    | ×    | √    | ×    | ×    | T   |
| √    | ×    | ×    | √    | ×    | T   |
| ×    | √    | ×    | ×    | ×    | F   |
| ×    | ×    | √    | ×    | ×    | F   |
| ×    | ×    | ×    | √    | ×    | F   |
| ×    | ×    | ×    | ×    | √    | F   |
| √    | √    | √    | √    | √    | T   |

为验证上述推测，我们发现了一个临界值：86。当忽略的令牌数量少于 86 时，LVLMs 作答错误；当忽略的令牌数量不小于 86 时，LVLMs 作答正确。且这 86 个令牌均位于聚类 0 中，这表明聚类 0 中的这 86 个令牌不仅无法提升 LVLMs 对图像的理解，反而会对 LVLMs 的作答准确率产生消极影响。我们对更多数据进行了分析，得出了类似结论。

## 讨论与局限性

尽管 Simignore 在提升 LVLMs 推理能力方面取得了显著成效，但仍存在一些局限性。未来的研究将探索自适应选择图像令牌忽略数量的策略，并深入研究模型的内部机制，以更全面地理解图像与文本信息如何协同作用，从而促进复杂推理。

## 结论

本研究提出了一种创新方法，通过计算图像与文本嵌入之间的相似度，提升 LVLMs 在复杂推理任务中的性能。我们发现，在 LLM 解码器中，与文本存在语义关联的图像令牌更易出现信息流汇聚现象。基于这一发现，我们设计了 Simignore 算法，该算法通过计算图像与文本嵌入的相似度，过滤掉无关的图像信息。实验结果表明，Simignore 能够提升不同 LVLMs 的复杂推理能力。
