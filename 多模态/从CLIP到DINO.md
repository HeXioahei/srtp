了解到关注**浅层特征**的重要性，评估了**不同模型的不同深度的特征**在任务上的表现。研究了多种多级**特征融合策略**，得出最佳融合模式。我们可以学习借鉴它的评估方法对不同层的特征进行评估以及用其融合策略来**决定该如何融合多级特征**。多级特征融合**有助于提高细粒度水平**。

![732e79646facaf8a0df01b1b86dfc656.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202504252343715.png)
图1: 通过计算从CLIP和DINOv2的浅层和深层提取的不同视觉符元的余弦相似度来进行特征对应可视化。

但是由于找不到源码，可能无法复现，所以实现的具体方法和细节我就没有去看了。主要记录了一下摘要和结论。
# 摘要

多模态大语言模型 (MLLMs) 通过整合视觉感知接口，在扩展大语言模型 (LLMs) 的能力方面取得了显著进展。 尽管出现了令人兴奋的应用和各种指令调优数据，但现有方法通常依赖于CLIP或其变体作为视觉分支，并且仅仅提取深层特征。 然而，这些方法缺乏对MLLMs中视觉编码器的全面分析。 在本文中，我们对MLLMs中不同视觉编码器的有效性进行了广泛的研究。 我们的研究结果表明，**CLIP的浅层特征对于细粒度任务（例如接地和区域理解）具有特别的优势**。 令人惊讶的是，**未经文本图像对齐预训练的纯视觉模型DINO**，作为MLLMs中的视觉分支表现出了令人鼓舞的性能。 **仅仅通过为其配备一个用于对齐的多层感知器 (MLP) 层，DINO 在细粒度相关感知任务上超越了 CLIP。** 基于这些观察结果，我们提出了一种简单而有效的**特征融合策略，命名为COMM**，它将CLIP 和 DINO 与Multi-level 特征Merging 集成，以**增强大型多模态语言模型 (MLLMs) 的视觉能力**。 我们通过在广泛的基准测试中进行综合实验来评估COMM，这些基准测试包括图像字幕、视觉问答、视觉定位和物体幻觉。 实验结果证明了COMM与现有方法相比具有优越的性能，展示了其在MLLMs中增强的视觉能力。

# 结论

本文对在多模态大语言模型 (MLLMs) 中使用不同视觉模型作为视觉分支的有效性进行了广泛的研究。 通过系统分析，我们强调了**浅层特征的重要性**，这些特征捕捉到低级细节，这对于接地和定位任务非常有益。 此外，我们认识到**仅视觉模型 DINOv2 的潜力**，它利用其固有的细粒度像素级信息，在与 MLP 层结合用于对齐目的时，可以增强 MLLMs 中的细粒度感知。 受我们分析的启发，我们引入了一种**融合方法**来**结合从 CLIP 和 DINOv2 获得的视觉特征**，从而进一步增强 MLLMs 的视觉能力和性能。 通过定性分析和大量的定量实验，我们证明了我们提出的方法的有效性，其性能超越了现有 MLLM 模型在各种基准数据集上的性能。 展望未来，我们鼓励未来的研究探索集成更强大的视觉模型以增强 MLLMs 中视觉分支的能力。 我们相信，这条研究途径是释放下一代 MLLMs 潜力的关键。
