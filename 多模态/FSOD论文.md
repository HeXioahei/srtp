# 基于 CLIP 语义原型学习的遥感图像少样本目标检测

刘天颖　周水庚（IEEE 高级会员）　李文根（IEEE 会员）　张一超（IEEE 会员）　关吉虹

摘要：少样本目标检测（FSOD）旨在解决训练数据不足的问题，近年来已引起遥感领域的广泛关注。主流 FSOD 方法通过有限样本生成类别原型，辅助构建分类决策边界，但在少样本场景下，这些原型可能与真实类别中心存在较大偏差。近年来，视觉 - 语言模型（VLM）展现出强大的视觉特征与文本特征对齐能力，仅需文本输入即可在多种下游计算机视觉任务中实现优异的零样本性能。因此，本文提出利用预训练 VLM（CLIP）从文本描述中构建类别原型，而非依赖有限视觉样本。具体而言，通过将类别名称输入 CLIP 文本编码器生成原型，并促使每个正样本候选区域特征向对应原型靠拢；为加速对齐过程，将 CLIP 视觉编码器作为教师模型实现视觉知识蒸馏；此外，采用提示调优（prompt tuning）使 CLIP 适配遥感场景。在 DIOR 和 NWPU VHR-10.v2 两个公开 FSOD 数据集上的大量实验表明，该方法有效性显著，性能与现有方法相比具有竞争力。

关键词：少样本目标检测（FSOD）；遥感图像（RSIs）；视觉 - 语言模型（VLM）

## 一、引言

近年来，遥感图像（RSIs）目标检测受到广泛关注，其目标是识别并定位图像中的感兴趣目标。作为航空与卫星图像分析的核心基础任务，目标检测已应用于城市规划、地质灾害检测、环境监测、土地利用 / 土地覆盖（LULC）制图等多个领域 [1,2]。得益于深度卷积神经网络（DCNN）[3,4] 和 Transformer [5,6] 强大的鲁棒图像特征提取能力，适用于自然场景的先进目标检测器 [7-14] 已取得显著进展。针对卫星图像中目标任意朝向、密集分布等特有问题，遥感领域也提出了多种目标检测算法 [15-24]。

尽管现有最优目标检测器性能优异，但训练过程需要海量标注数据 [25]。然而，标注足量训练数据耗时费力且成本高昂，部分稀有目标类别更难以收集足够样本 [26]。因此，如何使检测器在数据稀缺场景下保持鲁棒性成为关键问题。

为降低对训练数据的依赖，少样本学习（FSL）[27] 被提出以模拟人类快速理解新事物的能力。通常，FSL 旨在充分利用有限数据中的知识且避免过拟合，同时提升对新类别的泛化能力。少样本目标检测（FSOD）则试图将从标注充足的基础类别中学习到的知识迁移到新类别检测中，且每个新类别仅有少量样本。近年来，自然场景下已提出多种 FSOD 方法 [28-35]，但将通用 FSOD 方法直接应用于遥感场景效果欠佳。与自然场景目标相比，航空图像中的目标更易分布在复杂背景中，且类间相似度更高、类内相似度更低。解决这一问题的关键是设计鲁棒分类器以减轻误分类。

部分先驱工作 [36-39] 采用基于原型的框架 [40]，从有限数据中生成类别原型，并将其与目标特征融合以简化模型分类过程。该框架的核心思路是：同一类别目标应具有相似特征，因此会更接近对应类别原型。然而，由于类内多样性较高，从有限样本中生成的原型可能与真实类别中心存在巨大偏差，且远离原型的样本可能被误分为其他类别。

与不同图像中多变的视觉信息相比，文本中每个类别的语义信息相对稳定且不受数据可用性影响，在少样本场景下构建类别原型时更具鲁棒性。同时，VLM 通过大规模预训练具备强大的跨模态对齐能力，可在同一特征空间中更易比较视觉特征与文本特征。因此，本文提出一种适用于遥感图像的 FSOD 新方法，通过迁移预训练 VLM 的知识生成更可靠的类别原型。

具体而言，基于通用两阶段微调 FSOD 架构，本文采用对比语言 - 图像预训练模型（CLIP）[41] 作为辅助原型生成器。CLIP 通过自监督方式在 4 亿图像 - 文本对上预训练，已在多种计算机视觉任务中展现出优异泛化能力。将类别名称与提示模板结合输入 CLIP 文本编码器，即可得到每个类别的原型；随后通过交叉熵损失优化目标检测模型提取的候选区域特征，此过程可视为语言知识蒸馏。由于候选区域特征与自然语言描述的类别原型处于不同特征空间，本文将冻结的 CLIP 视觉编码器作为教师模型，实现特征对齐。

鉴于 CLIP 预训练目标是提取图像级特征以匹配输入文本，并不适用于目标检测任务，本文根据真值框并扩展尺度，将图像裁剪为多个 patches，再将其输入 CLIP 视觉编码器以获取目标实例级特征。此外，由于 CLIP 主要在自然图像 - 文本对上预训练，本文采用提示调优 [42] 实现 CLIP 的领域适配，使其更适用于遥感场景。

综上，本文主要贡献如下：

1. 利用预训练视觉 - 语言模型（VLM）CLIP 从文本描述中生成更可靠的类别原型，并通过原型对齐优化候选区域特征；
2. 将 CLIP 视觉编码器作为教师模型，实现视觉知识蒸馏；
3. 进一步采用提示调优，缓解预训练 CLIP 与遥感场景间的领域差距；
4. 在两个主流遥感 FSOD 数据集上的大量实验表明，所提方法与现有 FSOD 方法相比具有竞争力，尤其在低样本设置下表现更优。

需注意的是，本文提出的特征优化模块仅在训练阶段使用，测试阶段与原始基准模型完全一致，因此不会增加额外推理开销。

本文其余部分结构如下：第二节简要综述相关工作，包括自然场景与遥感场景下的目标检测及 FSOD；第三节介绍 FSOD 任务设定及所用基础框架；第四节详细阐述所提模型；第五节报告在两个主流遥感 FSOD 数据集上的实验结果；最后第六节总结全文并展望未来工作。

## 二、相关工作

本节从通用目标检测和少样本目标检测两方面简要综述相关工作。

### 2.1 目标检测

目标检测是计算机视觉的核心问题之一，要求模型不仅能识别目标，还能定位目标在图像中的位置。在深度学习时代，目标检测任务被解耦为分类与回归 [43]，方法可分为两阶段、一阶段和端到端检测器：前两者基于 DCNN [3]，后者基于 Transformer [5]。

- **两阶段检测器**：首先生成区域提议（RoIs），然后在第二阶段对提议进行微调，如 R-CNN 系列 [7,10,43,44]；
- **一阶段检测器**：如 YOLO 系列 [11,45,46] 和 RetinaNet [9]，直接在图像特征图上密集预测类别概率和坐标；
- **端到端检测器**：两阶段和一阶段检测器均需非极大值抑制（NMS）等后处理以去除冗余框，而 DETR [13] 基于 Transformer 编解码器架构 [5] 提出全新端到端目标检测范式，从特征图生成目标查询集，并为每个查询预测类别和坐标。此后，多种基于 Transformer 的检测器 [14,47] 被提出，检测性能进一步提升。

与自然场景目标相比，遥感图像中的目标通常分布分散、尺度变化大，且复杂背景与较小目标尺寸更增加了检测难度。得益于现代目标检测器的发展，遥感领域目标检测也取得显著进展。例如，Deng 等人 [15] 设计多尺度目标提议网络；Zheng 等人 [48] 提出超尺度块与细粒度多尺度结构以解决尺度变化问题；Pang 等人 [17] 设计统一自增强网络，提升复杂背景下小目标检测性能。

由于遥感图像中目标存在任意朝向，定向目标检测成为研究热点：Ding 等人 [16] 对定向框监督学习得到的 RoI 进行空间变换，缓解分类与回归的错位问题；Yang 等人 [18] 设计定位损失以解决旋转边界框的边界问题；Han 等人 [19] 显式编码旋转等变性与旋转不变性，实现更精准的航空目标检测；Wang 等人 [21] 用新的旋转变尺度窗口注意力替代 Transformer [5] 块中的原始全注意力，同时解决尺度变化与任意朝向问题；ACFG [49] 采用知识蒸馏策略减少模型参数。

然而，上述检测器均需大规模数据集才能获得满意性能，在小规模数据集上易过拟合且性能显著下降。

### 2.2 少样本目标检测

FSOD 旨在解决样本稀缺场景下的目标检测问题，通常可分为两类：基于元学习的方法和基于微调（又称迁移学习）的方法。

- **基于元学习的方法**：在从基础类别中采样的 “episode” 上训练元学习器，以显式构建少样本场景。例如，FSRW [29] 以 YOLO-v2 [45] 为元特征提取器，设计特征重加权模块，借助标注数据优化查询特征；RepMet [30] 提出用于 FSOD 的新型度量学习框架；Zhang 等人 [34] 为支持特征与查询特征设计互引导机制。
- **基于微调的方法**：先在基础集上训练模型，再在新集上微调参数。TFA [31] 提出简单高效的框架，无需修改通用目标检测框架结构，仅在微调阶段冻结特定参数；FSCE [32] 融入监督对比学习 [50] 以减轻误分类；DeFRCN [33] 解耦类别相关与类别无关参数，并利用独立原型生成模块提升分类精度；Yang 等人 [35] 提出高效预训练 - 迁移框架，设计知识继承机制，从基础类别权重初始化新类别权重。

随着通用 FSOD 方法的发展，也出现了多种针对遥感图像的 FSOD 算法：FSODM [51] 基于 FSRW [29] 构建元学习框架；DH-FSDet [52] 扩展 TFA [31]，提出新的标注采样策略与数据预处理方法，并采用双头预测器结构；PAMS-Det [53] 将内卷操作（involution）融入骨干网络，将路径聚合模块融入 TFA 以提升定位精度；G-FSDet [39] 提出新型损失函数以学习更具区分性的分类器，并设计表示补偿模块抑制基础类别的灾难性遗忘；P-CNN [36] 设计原型学习网络引导提议生成；MM-RCNN [37] 用元记忆模块替代原型，存储每个类别的学习知识；MINN [54] 基于元学习，设计查询图像与支持图像间的新型特征交互策略，缓解特征错位与注意力偏差。

与上述方法不同，本文采用预训练 VLM（CLIP [41]）作为原型生成器，优化 RoI 特征提取以提升分类精度；此外，进一步引入监督对比学习 [50] 以获取更具代表性的类别专属特征。

## 三、预备知识

### 3.1 问题设定

遵循 FSRW [29] 和 TFA [31] 中的 FSOD 设定：存在基础集\(D_{\text{base}}\)，包含基础类别\(C_{\text{base}}\)，每个类别有充足标注图像；新集\(D_{\text{novel}}\)包含新类别\(C_{\text{novel}}\)，每个类别仅有少量标注样本。需注意，基础类别与新类别互不重叠，即\(C_{\text{base}} \cap C_{\text{novel}} = \emptyset\)。

通常考虑**N-way K-shot**问题：N 表示新类别数量，K 表示每个新类别的标注样本数。即\(D_{\text{novel}} = \bigcup_{j=1}^N \{(x_i, c_i, b_i)_{i=1}^K \mid c_i \in C_{\text{novel}}\}\)，其中\(x_i\)表示目标实例，\(c_i\)为对应类别标签，\(b_i\)表示\(x_i\)在图像中的边界框标注。测试阶段的目标是在提升新类别检测性能的同时，保持基础类别的性能。

### 3.2 两阶段微调框架

该框架的训练过程分为两个阶段：基础训练阶段与微调阶段。

- **基础训练阶段**：在标注充足的基础集\(D_{\text{base}}\)上训练模型，以学习基础类别的知识；
- **微调阶段**：模型参数继承自基础训练阶段（边界框预测器参数除外），在基础集与新集的混合数据上微调，且部分参数冻结。需注意，两阶段的参数无需完全匹配。为平衡基础类别与新类别的样本数量，微调阶段从每个基础类别中采样 K 个样本。

这种简单的两阶段框架可将基础集的知识迁移到新集，在 FSOD 任务中展现出良好潜力。

### 3.3 CLIP

CLIP [41] 是一种预训练 VLM，在 4 亿图像 - 文本对上预训练，可学习鲁棒特征以用于动作识别、地理定位等下游计算机视觉任务。视觉 - 语言预训练（VLP）的核心优势在于零样本能力与模块化：预训练完成后，模型可作为基础模型，通过微调或直接用于其他通用视觉 - 语言任务。

CLIP 在多种计算机视觉任务中展现出强大的零样本能力：给定一段文本（又称 “提示”），CLIP 可判断输入图像是否与该文本匹配。因此，本文通过知识蒸馏，将 CLIP 作为教师模型引导检测模型的目标特征学习。由于遥感图像背景复杂，难以用特定提示描述，本文将 CLIP 视为仅优化前景目标特征的辅助模块，而非直接作为基础模型使用。

## 四、所提方法

所提模型的整体架构如图 1 所示，以 TFA [31] 为基准（基于 Faster R-CNN [7] 构建），包含以下组件：

- **骨干网络**：由 ResNet [3] 和 FPN [8] 组成，从输入图像中提取视觉特征；
- **区域提议网络（RPN）**：生成候选区域；
- **RoI Align 模块**：将候选区域投影到图像特征图上；
- **预测头**：实现目标分类与回归。

训练过程遵循 TFA [31] 的两阶段微调策略，在此基础上，本文提出在微调阶段利用预训练 VLM（CLIP [41]）的文本原型引导 RoI 特征优化，具体而言，在原始预测头旁添加 “CLIP 原型头” 辅助模块，以学习更鲁棒的实例特征。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 1 所提方法 TPG-FSOD 的架构。训练过程遵循两阶段微调框架，为简洁起见未展示基础训练阶段。微调阶段仅冻结骨干网络中的 ResNet [3] 参数，其余组件均参与微调。核心创新在于：利用预训练 VLM（CLIP [41]）提取的文本原型优化候选区域特征；由于 CLIP 主要在自然图像 - 文本对上预训练，引入提示学习使其更好适配遥感图像；此外，将 CLIP 视觉编码器作为教师模型，加速文本原型与候选区域特征的对齐过程。

### 4.1 训练框架

训练框架分为两个阶段：基础训练阶段与微调阶段。

#### 4.1.1 基础训练阶段

第一阶段在标注充足的基础集上训练模型，与传统目标检测模型完全一致。联合损失函数如下：\(\mathcal{L}_{\text{det}} = \mathcal{L}_{\text{rpn}} + \mathcal{L}_{\text{cls}} + \mathcal{L}_{\text{reg}} \tag{1}\)其中：

- \(\mathcal{L}_{\text{rpn}}\)：RPN [7] 的损失，用于区分目标与背景，并粗回归候选区域；
- \(\mathcal{L}_{\text{cls}}\)：分类器损失，采用传统目标检测中的交叉熵损失；
- \(\mathcal{L}_{\text{reg}}\)：回归损失，采用 Faster R-CNN [7] 提出的平滑 L1 损失。

#### 4.1.2 改进的微调阶段

第二阶段继承第一阶段的模型参数，在基础类别与新类别的混合数据上微调。为平衡两类别的样本数量，从每个基础类别中随机采样 K 个样本作为训练数据。

TFA [31] 在微调阶段冻结除预测头外的所有参数，但其过度依赖从基础类别学习的通用知识，限制了新类别的性能。通过对比不同参数微调策略，本文发现：仅冻结骨干网络中 ResNet [3] 的参数，可在基础类别与新类别性能间取得最佳平衡（后续消融实验将详细分析）。

此外，TFA [31] 将原始基于全连接层的分类器替换为基于余弦相似度的分类器，通过实例级特征归一化减少类内方差，同时保持基础类别的性能。输入\(x_i\)与分类器权重向量w的余弦相似度计算如下：\(s_{i,j} = \alpha \cdot \frac{x_i \cdot w_j^\top}{\|x_i\| \cdot \|w_j\|} \tag{2}\)其中，j表示权重向量的类别索引，\(\alpha\)为缩放因子（实验中经验性设为 20）。基于余弦相似度得到预测类别得分：\(p_{i,c} = \frac{e^{s_{i,c}}}{\sum_{j=0}^C e^{s_{i,j}}} \tag{3}\)其中，C为总类别数，\(j=0\)表示背景类别。

### 4.2 基于 CLIP 的文本原型监督

原型网络 [40] 的思想已被广泛应用于 FSOD [36,37,55,56]，其核心假设是：同一类别目标的特征相似，因此与对应类别原型的距离会小于与其他原型的距离。通过基于余弦相似度的注意力机制将 RoI 特征与原型融合，可有效减轻误分类问题。然而，稀缺样本难以代表整个类别的数据分布，由此生成的原型可靠性不足，限制了模型性能。

近年来，预训练 VLM 在多种任务中展现出强大的零样本能力，可在跨模态特征空间中生成视觉与文本特征。因此，无需额外训练数据，仅通过类别名称从 VLM 中提取文本特征，即可将 VLM 蕴含的知识迁移到 FSOD 任务中。

具体而言，本文采用代表性 VLM（CLIP [41]）生成文本类别原型：给定第c个类别的名称\(\text{cls}\)，设计 CLIP 专用的简单提示模板\(T_c\)——“a centered satellite photo of [cls]”（“一张以 [类别名] 为中心的卫星照片”）；将提示\(T_c\)输入 CLIP 文本编码器，即可生成类别原型\(z_c^t\)。

首先，引入与原始边界框预测器并行的非线性嵌入头（由两个线性层及中间的 ReLU 激活函数组成），将每个 RoI 特征\(x_i\)投影为与 CLIP 输出特征维度一致的向量\(z_i\)；随后，通过计算\(z_i\)与类别原型的余弦相似度，估计匹配得分：\(s_{i,c} = \alpha \cdot \frac{z_i \cdot z_c^t}{\|z_i\| \cdot \|z_c^t\|} \tag{4}\)\(s_i = [s_{i,1}, s_{i,2}, ..., s_{i,C}] \tag{5}\)其中，\(\alpha\)为缩放因子（实验中设为 20），C为类别数。需注意，由于背景难以用单一特征向量描述，仅将分配给前景真值框的候选区域特征输入 CLIP 原型头。

最终目标函数定义为：\(\mathcal{L}_{\text{lkd}} = \frac{1}{N_{\text{pos}}} \sum_{i=1}^N \mathbb{1}\{y_i \neq C_{\text{bg}}\} \cdot \mathcal{H}\left(\text{softmax}(s_i), y_i\right) \tag{6}\)其中，N为每张图像中的候选区域数量，\(\mathbb{1}\{\cdot\}\)为指示函数，\(y_i\)为\(z_i\)的类别标签，\(\mathcal{H}\)为交叉熵损失。

### 4.3 基于 CLIP 的视觉知识蒸馏

检测模型提取的 RoI 特征与 CLIP 视觉编码器输出的视觉特征处于不同特征空间，可能阻碍原型匹配。受 ViLD [57] 启发，本文将 CLIP 视觉编码器作为教师模型，引导 RoI 特征优化。

CLIP 视觉编码器的预训练目标是提取图像级特征，因此本文根据真值框标注b，从原始图像I中裁剪 patches，并输入编码器以生成目标实例级特征。考虑到遥感图像中的目标通常尺寸较小且部分目标纹理简单，本文对b进行尺度扩展以获取更多上下文信息，从而提升视觉特征质量。最终目标视觉特征生成过程如下：\(z_i^v = \xi_v\left(\text{Crop}\left(I, \text{Scale}(b_i)\right)\right) \tag{7}\)其中，CLIP 视觉编码器\(\xi_v\)的参数在训练过程中冻结。

视觉知识蒸馏采用 L1 损失，对齐每个嵌入后的真值特征\(z_i\)与其目标视觉特征\(z_i^v\)：\(\mathcal{L}_{\text{vkd}} = \frac{1}{M} \sum_{i=1}^M \|z_i - z_i^v\|_1 \tag{8}\)其中，M为图像I中的真值框数量。

通过 CLIP 文本编码器与视觉编码器的双重引导优化，检测模型可学习到每个类别的鲁棒特征空间，从而提升分类性能。

### 4.4 基于提示学习的领域适配

CLIP 在互联网收集的图像 - 文本对上预训练，且大部分为自然图像。由于自然图像与遥感图像的分布差异显著，直接将 CLIP 用于遥感图像实例分类会存在领域差距，导致文本编码器与图像编码器的输出无法正确匹配。

微调 CLIP 可简单弥合领域差距，但该方法效率低且易过拟合。研究表明，输入 VLM 的提示对下游任务性能至关重要，因此提示学习 [42,58,59] 被提出，通过动态适配预训练模型以适应不同任务。本文借鉴 CoOp [42] 的思路，用可学习向量替换固定提示，并冻结 CLIP 所有参数，以缓解领域差距问题。

第c个类别的提示定义为：\(T_c = [V]_1[V]_2...[V]_M[\text{cls}] \tag{9}\)其中，每个\([V]_m\)为可学习向量，\([\text{cls}]\)表示 CLIP 中第c个类别名称的令牌嵌入，M为控制提示向量数量的超参数（所有类别共享提示向量）。冻结 CLIP 文本编码器输出的提示表示仍取自 [EOS] 令牌位置。

本文通过对比学习，将类别提示特征与冻结 CLIP 图像编码器输出的对应支持视觉特征对齐。给定支持实例视觉特征\(z_i^v\)，匹配第c个类别提示的概率计算如下：\(p_{i,c} = \frac{\exp\left(z_i^v \cdot \xi_t(T_c)^\top / \tau\right)}{\sum_k^c \exp\left(z_i^v \cdot \xi_t(T_k)^\top / \tau\right)} \tag{10}\)其中，\(\xi_t\)表示 CLIP 文本编码器，\(\tau\)为控制概率分布形状的温度系数。目标函数定义为：\(\mathcal{L}_{\text{pl}} = \frac{1}{N_{\text{support}}} \sum_i^{N_{\text{support}}} \mathcal{H}(p_i, y_i) \tag{11}\)其中，\(N_{\text{support}}\)为批次中的真值框数量。

### 4.5 损失函数

本文提出的三个模块均基于与预测头并行的嵌入头，微调阶段的总损失函数为：\(\mathcal{L} = \mathcal{L}_{\text{det}} + \beta \cdot \mathcal{L}_{\text{lkd}} + \gamma \cdot \mathcal{L}_{\text{vkd}} + \eta \cdot \mathcal{L}_{\text{pl}} \tag{12}\)其中，\(\beta\)、\(\gamma\)、\(\eta\)为超参数（实验中设\(\beta = \eta = 1.0\)，\(\gamma = 0.1\)以平衡损失值）。

## 五、实验

本节在 DIOR [60] 和 NWPU VHR-10.v2 [62] 两个主流遥感少样本目标检测基准数据集上进行大量实验，通过与现有方法对比验证所提方法的有效性。首先介绍数据集与评估指标，随后详述实现细节；接着将所提方法与现有最优 FSOD 方法对比，并通过消融实验验证模型各模块的有效性。

### 5.1 数据集与评估指标

#### 5.1.1 数据集

- **DIOR[60]**：大规模光学遥感图像目标检测基准，包含 23463 幅遥感图像、20 个类别及 192472 个标注实例（取自谷歌地球）。目标类别包括飞机、机场、棒球场、篮球场、桥梁、烟囱、水坝、高速公路服务区、高速公路收费站、港口、高尔夫球场、跑道、立交桥、船舶、体育馆、储罐、网球场、火车站、车辆、风车。数据集分为训练集（5862 幅）、验证集（5863 幅）、测试集（11738 幅），图像尺寸均为 800×800 像素，空间分辨率范围为 0.5-30 米。

遵循 FSODM [51] 构建少样本检测场景：设计 4 种不同的基础 / 新类别分割方式，每种分割包含 15 个基础类别与 5 个新类别（具体类别见表 1）。样本数K设为 3、5、10、20、30，将训练集与验证集合并用于训练，测试集用于评估基础类别与新类别的性能。

- **NWPU VHR-10.v2[62]**：另一主流基准数据集，包含 1172 幅图像、10 个类别。目标类别包括飞机、棒球场、篮球场、桥梁、跑道、港口、船舶、储罐、网球场、车辆，图像尺寸均为 800×800 像素。

遵循 SCoDANet [66] 将所有类别分为 2 种基础 / 新类别分割方式（见表 1），每种分割包含 3 个新类别与 7 个基础类别。样本数K设为 3、5、10、20。

**表 1 DIOR [60] 与 NWPU VHR-10.v2 [61] 的不同基础 / 新类别分割**

|数据集|分割方式|新类别|基础类别|
|---|---|---|---|
|DIOR[60]|1|棒球场、篮球场、桥梁、烟囱、船舶|其余类别|
||2|飞机、机场、高速公路收费站、港口、跑道|其余类别|
||3|水坝、高尔夫球场、储罐、网球场、车辆|其余类别|
||4|高速公路服务区、立交桥、体育馆、火车站、风车|其余类别|
|NWPU VHR-10.v2[61]|1|飞机、棒球场、网球场|其余类别|
||2|篮球场、跑道、车辆|其余类别|

#### 5.1.2 评估指标

与现有 FSOD 方法一致，实验采用交并比（IoU）为 0.5 的平均精度（mAP）作为评估指标。为同时评估基础类别与新类别的性能，分别报告两类别的 mAP（记为 bAP 和 nAP）。

### 5.2 实现细节

以 TFA [31] 为基准，采用 ResNet101 [3]（结合 FPN [8]）作为骨干网络。

- **基础训练阶段**：在基础类别上训练 36k 迭代，预热后初始学习率设为 0.01，在 24k 和 32k 迭代时分别除以 10；
- **微调阶段**：所有样本设置下的训练迭代数均为 8k，预热后学习率设为 0.001。

两阶段均采用 SGD 优化器（动量 0.9，权重衰减 0.0001），在 2 块 NVIDIA RTX 4090 GPU 上训练，批大小设为 4。为公平对比，未采用多尺度训练策略。为保证结果稳定性，每个实验重复 3 次，取平均值作为最终性能。

### 5.3 与现有最优 FSOD 方法的对比

将所提方法与多种现有最优 FSOD 方法（包括通用 FSOD 方法与遥感专用 FSOD 方法）在两个数据集上对比，同时纳入通用目标检测方法 Faster R-CNN [7] 作为参考。

#### 5.3.1 对比方法分类

1. **通用 FSOD 方法**：
    
    - FSRW [29]：引入特征重加权策略，基于元学习聚焦新目标检测的相关特征；
    - Meta RCNN [63]：同样采用元学习进行特征优化；
    - TFA [31]：基于迁移学习的简单高效策略；
    - DeFRCN [33]：扩展 TFA [31]，融入解耦微调策略；
    - FSDetView [67]：结合特征表示学习与视角一致性机制。
2. **遥感专用 FSOD 方法**：
    
    - P-CNN [36]：利用原型学习提升类别表示能力；
    - MM-RCNN [37]：提出记忆策略，存储每个类别的学习知识；
    - SAGS&TFS [64]：利用自适应全局相似度与双向前景激励器提升性能；
    - G-FSDet [39]：融入全局特征对齐与自适应策略，性能提升显著；
    - SAE-FSDet [65]：采用自适应集成机制。

更多方法细节见第二节相关工作。

#### 5.3.2 DIOR 数据集上的结果

DIOR [60] 上的检测结果如表 2 所示。首先将所提方法与基准模型 TFA [31] 对比：TFA [31] 在微调阶段冻结除检测头外的所有模块，虽能保留基础类别的大部分知识（基础类别性能最优），但微调策略严重限制了新类别的学习过程，导致新类别性能低下。

本文在消融实验中探索了不同参数微调策略（类似 G-FSDet [39]，但实验更全面）。结果表明，在所有少样本设置与类别分割方式下，所提模型在新类别上的性能均显著优于 TFA [31]，且基础类别性能下降在可接受范围内。性能提升主要源于改进的微调阶段（在保留基础类别性能的同时提升新类别性能），更重要的是：CLIP [41] 的文本原型提供了更鲁棒的监督信号，帮助检测模型学习更优特征空间；视觉知识蒸馏进一步促进跨模态特征学习；提示学习则缓解了 CLIP [41] 与遥感场景的领域差距。

结果显示，在 4 种基础 / 新类别分割的所有少样本设置下，所提方法的 mAP 平均提升分别为 10.4%、13.7%、17.7%、20.5%、22.2%，提升幅度显著。

此外，与其他现有最优 FSOD 方法相比，所提方法在多数设置下表现更优。尽管框架更简洁，但所提方法在基础类别与新类别性能上均优于元学习方法 MM-RCNN [37]；与迁移学习方法 G-FSDet [39]（现有最优）相比，所提方法在新类别上表现更优，但基础类别性能略低（因 G-FSDet [39] 专为保留基础类别知识设计）。

基准模型与所提方法在 10-shot 设置下的检测结果可视化如图 2 所示，可见所提方法在缓解漏检与误检问题上比基准模型更有效。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 2 DIOR 数据集第一分割方式下 10-shot 设置的新类别检测示例。第一行为真值标注，第二行为 TFA [31] 的检测结果，第三行为所提方法的检测结果。可见 TFA 因参数冻结策略，易忽略新类别实例。

#### 5.3.3 NWPU VHR-10.v2 数据集上的结果

NWPU VHR-10.v2 [62] 上的结果如表 3 所示。与 DIOR 相比，该数据集类内差异较小，因此预训练 CLIP [41] 的文本原型在基础类别与新类别上均相对鲁棒，基础类别的灾难性遗忘更少（如表 3 所示）。

具体而言，在第一组基础 / 新类别分割的 3-shot 至 20-shot 设置下，所提方法在新类别上的 AP 分别比现有最优模型 G-FSDet [39] 高 7%、10.3%、3.8%、0.4%，基础类别 AP 分别高 0.3%、3.8%、0.8%。实验结果再次验证了所提方法的有效性。10-shot 设置下的检测结果可视化如图 3 所示，直观展现了所提方法的优势。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 3 NWPU VHR-10.v2 数据集两种分割方式下 10-shot 设置的新类别检测示例（虚线两侧分别为两种分割）。第一行为真值标注，第二行为 TFA [31] 的检测结果，第三行为所提方法的检测结果。

### 5.4 消融实验

#### 5.4.1 不同参数微调策略的影响

通常，两阶段锚点检测器可分为四个部分：骨干网络、RPN、RoI 特征提取器、检测头。TFA [31] 已证明仅微调检测头对 FSOD 有效，但多个方法 [33,35,39] 表明，微调阶段需考虑每个组件是否参与微调。本文针对除检测头外的其他组件，设计不同参数微调策略，结果如表 4 所示。

结果表明，微调任一组件均能显著提升新类别的检测性能：若仅微调单个组件，样本数较少时微调 RPN 效果最优，样本数较多时微调骨干网络效果最优。推测原因是：冻结的 RPN 对基础类别存在偏见，易将新类别目标误判为背景，因此样本稀少时新类别检测性能差；而微调骨干网络会导致基础类别出现严重的灾难性遗忘（AP 下降 9.1%-10.9%），表明骨干网络对图像特征鲁棒性的影响最大。

如表 4 所示，若将骨干网络与 FPN 视为整体，冻结骨干网络、解冻其他组件时新类别性能最优；最后一行实验进一步表明，仅冻结骨干网络（不冻结 FPN）可使新类别性能进一步提升。

#### 5.4.2 所提模块的影响

在 DIOR [60] 第一分割方式的新类别上进行大量消融实验，结果如表 5 所示。第一行为基准模型 TFA [31] 的性能，后续行依次采用参数微调策略，并结合训练专用的三个模块。

首先，通过 CLIP [41] 文本编码器生成类别原型，提示模板固定为 “a centered satellite photograph of [类别名]”。如表 5 所示，此时性能提升微弱（3-shot 和 5-shot 设置下 AP 仅提升 0.5%），推测原因是检测头的视觉语义空间与 CLIP 的语言语义空间存在差距 ——CLIP 在与目标检测无关的图像 - 文本对上预训练。

因此，本文将 CLIP 视觉编码器作为教师模型，直接引导检测头嵌入特征的优化，促进检测模型特征与 CLIP 特征的对齐，在所有设置下均实现稳定性能提升。

另一影响检测性能的因素是：CLIP 预训练数据中遥感图像占比极低，自然图像与遥感图像存在领域差距。因此，添加提示调优以适配遥感场景后，检测性能进一步提升，且在样本数较少时提升更显著。

综上，表 5 验证了所提模型各模块的有效性。此外，图 4 展示了目标特征的 t-SNE 可视化结果：尽管 CLIP 原型与真实类别中心并非完全对齐，但在 CLIP 原型引导下学习的特征空间，仍优于从有限视觉样本中学习的特征空间。

**表 4 不同参数微调策略影响的消融实验（DIOR 第一分割方式）**（注：RoI Feat. 表示共享 RoI 特征提取器）

|方法|微调组件||||3-shot||5-shot||10-shot||20-shot||30-shot||
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|
||骨干网络（ResNet）|FPN|RPN|RoI Feat.|nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|
|TFA|×|×|×|×|0.138|0.706|0.130|0.707|0.150|0.712|0.158|0.710|0.168|0.709|
|微调策略 1|√|×|×|×|0.242|0.615|0.286|0.607|0.354|0.616|0.382|0.620|0.417|0.630|
|微调策略 2|×|√|×|×|0.291|0.710|0.310|0.702|0.335|0.695|0.356|0.703|0.363|0.703|
|微调策略 3|×|×|√|×|0.170|0.663|0.180|0.665|0.205|0.673|0.214|0.686|0.226|0.684|
|微调策略 4|×|×|×|√|0.240|0.611|0.281|0.608|0.355|0.616|0.385|0.619|0.422|0.629|
|微调策略 5|×|×|√|√|0.260|0.601|0.296|0.599|0.358|0.607|0.394|0.615|0.432|0.625|
|微调策略 6|×|√|√|√|0.301|0.661|0.320|0.663|0.371|0.671|0.405|0.687|0.430|0.679|
|微调策略 7|√|√|√|√|0.258|0.597|0.297|0.599|0.360|0.609|0.393|0.614|0.423|0.626|
|微调策略 8（最优）|×|√|√|√|0.314|0.646|0.331|0.638|0.388|0.651|0.417|0.672|0.442|0.674|

**表 5 所提模块影响的消融实验（DIOR 第一分割方式新类别，报告 nAP）**（注：TPS = 文本原型监督，VKD = 视觉知识蒸馏，PL = 提示调优）

|方法|TPS|VKD|PL|3-shot|5-shot|10-shot|20-shot|30-shot|
|---|---|---|---|---|---|---|---|---|
|TFA[31]|×|×|×|0.138|0.130|0.150|0.158|0.168|
|仅微调|×|×|×|0.314|0.331|0.388|0.417|0.442|
|微调 + TPS|√|×|×|0.319|0.336|0.394|0.421|0.451|
|微调 + TPS+VKD|√|√|×|0.330|0.347|0.395|0.426|0.454|
|微调 + TPS+VKD+PL（所提方法）|√|√|√|0.342|0.356|0.402|0.430|0.452|

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 4 基准模型 TFA [31] 与所提方法学习的新类别目标特征 t-SNE 可视化（DIOR 第一分割方式 10-shot 设置）。从测试集中每个新类别随机选取 500 个目标。

表 6 详细展示了 3-shot 设置下 DIOR [60] 第一分割方式中每个新类别的性能。所提方法显著提升了大部分新类别的 FSOD 性能，但 “桥梁” 类别除外 —— 推测原因是 “桥梁” 的训练样本相似，模型发生过拟合。此外，“船舶” 类别性能较差（类内差异大），需更鲁棒的文本表示；而 “棒球场”“篮球场” 等类内差异小的新类别，模型性能更优。综上，所提方法可有效提升 FSOD 性能。

**表 6 各新类别上所提模块影响的消融实验（DIOR 第一分割方式 3-shot 设置，报告 nAP）**（注：TPS = 文本原型监督，VKD = 视觉知识蒸馏，PL = 提示调优）

|方法|TPS|VKD|PL|棒球场|篮球场|桥梁|烟囱|船舶|
|---|---|---|---|---|---|---|---|---|
|TFA[31]|×|×|×|0.000|0.093|0.214|0.303|0.081|
|仅微调|×|×|×|0.346|0.425|0.142|0.567|0.091|
|微调 + TPS|√|×|×|0.350|0.447|0.131|0.575|0.091|
|微调 + TPS+VKD|√|√|×|0.360|0.458|0.148|0.591|0.091|
|微调 + TPS+VKD+PL（所提方法）|√|√|√|0.413|0.451|0.144|0.609|0.091|

#### 5.4.3 \(\mathcal{L}_{\text{vkd}}\)权重的选择

\(\mathcal{L}_{\text{vkd}}\)的权重控制文本原型监督与视觉知识蒸馏的平衡。在 DIOR [60] 第一分割方式上进行消融实验，选择最优权重，结果如表 7 所示。

结果表明：样本数较少时（3-shot、5-shot），较大的\(\mathcal{L}_{\text{vkd}}\)权重可使新类别性能更高；但随着样本数增加，较大权重的新类别性能反而低于较小权重 —— 推测原因是样本数较多时，多变的视觉特征增加了视觉知识蒸馏的难度。此外，基础类别性能随权重增加而下降，因 CLIP 的视觉特征不如从大量训练样本中学习的特征鲁棒。

**表 7 \(\mathcal{L}_{\text{vkd}}\)权重影响的消融实验（DIOR 第一分割方式，报告新类别 nAP）**

|\(\mathcal{L}_{\text{vkd}}\)权重|3-shot||5-shot||10-shot||20-shot||30-shot||
|---|---|---|---|---|---|---|---|---|---|---|
||nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|
|0.1|0.342|0.627|0.356|0.627|0.402|0.644|0.430|0.655|0.452|0.664|
|0.5|0.342|0.618|0.362|0.615|0.391|0.629|0.423|0.649|0.446|0.655|
|1.0|0.342|0.613|0.361|0.604|0.386|0.621|0.419|0.639|0.443|0.649|

#### 5.4.4 提示学习中超参数的筛选

提示学习中提示向量数量M影响每个文本提示的表示能力。在 DIOR [60] 第一分割方式上筛选不同M值，结果如表 8 所示。

结果表明：随着M增加，性能总体略有提升，说明提示向量数量越多，对不同类别的表示能力越强；但\(M=16\)与\(M=24\)时性能波动，表明低样本场景下过多向量难以学习。综合性能与成本，最终设\(M=16\)。

**表 8 提示学习中超参数M影响的消融实验（DIOR 第一分割方式，报告新类别 nAP）**

|M|3-shot||5-shot||10-shot||20-shot||30-shot||
|---|---|---|---|---|---|---|---|---|---|---|
||nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|
|4|0.338|0.621|0.349|0.623|0.398|0.636|0.429|0.659|0.454|0.657|
|8|0.338|0.639|0.355|0.624|0.402|0.640|0.426|0.659|0.454|0.663|
|16|0.342|0.627|0.356|0.627|0.402|0.644|0.430|0.655|0.452|0.664|
|24|0.333|0.627|0.354|0.628|0.403|0.639|0.431|0.657|0.456|0.655|

#### 5.4.5 不同预训练 VLM 的对比

引入其他预训练 VLM 与 CLIP [41] 对比，所选 VLM 需具备独立文本与视觉编码器，以在不同步骤中提取文本原型与真值视觉特征。具体选择 BLIP [68] 和 FLAVA [69] 两种知名 VLM，所有 VLM 均采用相同简单提示模板 “a centered satellite photo of {class name}”，实验结果如表 9 所示。

结果表明，相同提示下不同 VLM 的性能差距较小 —— 因它们均具备强大的跨模态特征提取能力，而简单模板难以区分其性能差异。但该结果仍验证了所提方法对不同预训练 VLM 的鲁棒性。

**表 9 不同预训练 VLM 的对比实验（DIOR 第一分割方式，固定提示）**

|VLM|3-shot||5-shot||10-shot||20-shot||30-shot||
|---|---|---|---|---|---|---|---|---|---|---|
||nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|nAP|bAP|
|CLIP[41]|0.330|0.629|0.347|0.627|0.395|0.638|0.426|0.656|0.454|0.656|
|BLIP[68]|0.340|0.634|0.354|0.624|0.391|0.630|0.422|0.645|0.445|0.660|
|FLAVA[69]|0.332|0.634|0.357|0.624|0.395|0.636|0.420|0.646|0.446|0.659|

## 六、结论

本文提出利用预训练 VLM，通过自然语言生成类别原型（而非仅依赖有限视觉样本），以提升 FSOD 性能。具体而言，以预训练 VLM（CLIP [41]）为教师模型，同时采用语言知识蒸馏与视觉知识蒸馏；此外，由于 CLIP [41] 主要在自然图像 - 文本对上预训练，引入提示调优作为辅助优化方法，实现领域适配。在两个主流遥感 FSOD 数据集上的大量实验验证了所提方法的有效性。

当前多数 FSOD 方法试图从有限视觉样本中挖掘尽可能多的信息，但受限于多样性不足，检测性能仍远低于充分监督训练。本文尝试将预训练 VLM 的通用知识引入 FSOD，仍处于初步探索阶段。未来计划探索更高效的 VLM 利用方式，如先进领域适配、视觉提示调优；此外，还可利用生成模型合成更多训练样本。

## 参考文献

（注：参考文献格式与原文一致，此处省略具体条目）

## 作者简介

刘天颖：2018 年获同济大学计算机科学学士学位，现于同济大学计算机科学与技术系攻读博士学位。研究方向为目标检测、少样本学习、半监督学习。

周水庚（IEEE 高级会员）：1988 年获华中科技大学学士学位，1991 年获电子科技大学硕士学位，2000 年获复旦大学计算机科学博士学位。1991-1997 年任职于上海航天技术研究院，历任工程师、高级工程师（1995 年 8 月起）；2000-2002 年为武汉大学软件工程国家重点实验室博士后；现任复旦大学计算机科学学院教授。研究方向为人工智能、大数据管理与分析、生物信息学。

李文根（IEEE 会员）：2011 年、2017 年分别获同济大学计算机科学学士、博士学位，2018 年获香港理工大学计算机科学双博士学位。现任同济大学计算机科学与技术系助理教授。研究方向为多模态数据分析、时空数据分析（面向城市计算与海洋计算）。

张一超（IEEE 会员）：2012 年获同济大学计算机科学与技术博士学位。现任同济大学计算机科学与技术系副教授。研究方向为信息传播、链路预测、加权网络建模、加权网络随机扩散、网络演化博弈。

关吉虹：1991 年获华中师范大学学士学位，1998 年获武汉测绘科技大学（2000 年 8 月并入武汉大学）硕士学位，2002 年获武汉大学博士学位。现任同济大学计算机科学与技术系教授。1991-1997 年任职于武汉测绘科技大学计算机系，历任助教、副教授（2000 年 8 月起）；2000 年 8 月 - 2003 年 10 月任武汉大学计算机学院副教授，2003 年 11 月起任教授。研究方向为空间数据库、人工智能、生物信息学。

（注：原文中图表及公式编号与译文保持一致，因排版限制未展示图表，实际应用中需结合原文图表查看；基金项目信息已整合至摘要及作者单位部分，符合中文科技论文表达习惯。）