## 摘要

对比语言 - 图像预训练（CLIP）的普及推动了其在多种下游视觉任务中的应用。为了提升其在下游任务中的性能，少样本学习已成为一种广泛采用的技术。然而，现有方法要么性能有限，要么存在过多的可学习参数。在本文中，我们提出了 APE（Adaptive Prior rEfinement，自适应先验优化）方法，用于优化 CLIP 的预训练知识，该方法能以较高的计算效率实现卓越的准确率。通过先验优化模块，我们分析下游数据中的类间差异，并从 CLIP 提取的缓存模型中分离出领域特定知识。在此基础上，我们提出了两个模型变体：无需训练的 APE 和需要训练的 APE-T。我们探究了测试图像、先验缓存模型和文本表征之间的三边亲和力，仅训练一个轻量级的类别残差模块。在 11 个基准测试的平均准确率上，APE 和 APE-T 均达到了最先进水平，在 16-shot 设置下分别比第二好的方法高出 1.59% 和 1.99%，且可学习参数减少了 30 倍。代码可在[https://github.com/yangyangyang127/APE](https://github.com/yangyangyang127/APE)获取。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231101542.png)

## 1. 引言

对比视觉 - 语言预训练的出现为多模态学习提供了新范式 [46, 48, 57]。其普及已在多种下游视觉任务中显现，包括 2D 或 3D 分类 [38, 88, 92]、分割 [65, 83, 101] 和检测 [69, 87, 98]。CLIP [64] 是最受认可的对比视觉 - 语言模型之一，因其简洁性和优越性而受到广泛关注。通过互联网上的海量图像 - 文本对进行预训练，CLIP 在对齐视觉 - 语言表征方面表现出显著能力，在下游任务中具有良好的零样本性能。为了在低数据场景下进一步增强 CLIP，许多研究在冻结的 CLIP 基础上添加额外的可学习模块，提出了少样本学习技术。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231057313.png)

如图 2（a）和（b）所示，基于 CLIP 的少样本方法可根据是否通过 CLIP 的先验知识显式构建可学习模块分为两类：1）非先验方法（Non-prior Methods）在不借助 CLIP 先验的情况下随机初始化可学习模块，并在少样本训练中对其进行优化。例如，CoOp 系列 [99, 100] 在 CLIP 的文本编码器前采用可学习提示，而 CLIP-Adapter [24] 则在 CLIP 后学习两个残差风格的适配器。这类网络仅引入少量可学习参数，但少样本准确率有限，因为额外模块未显式利用预训练的先验知识。2）先验方法（Prior-based Methods）通过从少样本数据中提取 CLIP 特征构建键值缓存模型，能够以无需训练的方式进行识别，包括 Tip-Adapter [91] 和 Tip-X [76]。然后，它们可以进一步将缓存模型视为性能良好的初始化，并微调缓存键以获得更好的分类准确率。这些基于先验的方法将先验知识显式注入训练过程，但由于缓存模型规模大、可学习参数多而较为繁琐。我们由此提出问题：能否融合两者的优点，实现两全其美 —— 既配备高效的可学习模块，又能受益于 CLIP 的先验知识？

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231102325.png)

为此，我们提出了自适应先验优化（Adaptive Prior rEfinement，APE）方法，通过优化 CLIP 在视觉表征中的预训练知识，高效地将 CLIP 适配于少样本分类。如图 1 所示，**APE 不仅能借助 CLIP 的先验实现卓越性能，还比非先验方法消耗更少的计算资源**。我们观察到，**并非 CLIP 的所有先验（即缓存模型或测试图像的提取视觉特征）在通道维度上对下游任务都很重要**。在图 3 中，我们将 CLIP 提取的视觉表征的特征通道分为两组，并分别可视化它们与 ImageNet [16] 中文本表征的相似度图。第一组（a）的特征比第二组（b）的特征表现出更好的视觉 - 语言对齐。受此启发，**我们提出先验优化模块，通过类间相似度和方差两个标准自适应选择最重要的特征通道。通过最大化少样本训练数据中的类间差异，优化后的特征通道可以丢弃冗余信息，减小缓存规模，降低内存成本。**

在此基础上，我们提出了该方法的两个变体，分别记为 APE 和 APE-T。第一个是无需训练的模型，直接利用优化后的缓存模型进行推理。APE 创新性地探究测试图像、优化后的缓存模型和文本表征之间的三边亲和力，实现稳健的无训练识别。第二个变体 APE-T（图 2（c））仅在顶部训练轻量级的类别残差，而非对整个缓存模型进行昂贵的微调。这些类别残差进一步更新优化后的缓存模型，并在模态间共享，以确保视觉 - 语言的对应关系。我们的 APE 和 APE-T 在 11 个少样本基准测试上分别比现有的无训练和需训练方法表现更优，在 16-shot 平均准确率上分别比第二好的方法高出 1.59% 和 1.99%。

**本文的贡献总结如下：**  

- **我们提出了自适应先验优化（APE）方法，这是一种 CLIP 的适配方法，能显式利用其先验知识同时保持计算效率。**
- **先验优化后，我们探究 CLIP 提取的视觉 - 语言表征之间的三边亲和力，以实现有效的少样本学习。**
- **我们无需训练的 APE 和需要训练的 APE-T 在 11 个少样本基准测试上表现出最先进的性能，证明了我们方法的优越性。**

## 2. 相关工作

**零样本 CLIP**。对于 C 类数据集的测试图像，CLIP [64] 利用其编码器提取 D 维视觉和文本表征，分别记为$f∈R^D$和$W∈R^{C×D}$。然后，零样本分类的 logits 通过它们的相似度计算得出：  
$$R_{fW}​=fW^Τ∈R^{1×C}\tag{1}$$
基于这种零样本范式，最近的研究将 CLIP 的预训练能力扩展到许多其他视觉任务，如少样本图像分类 [63, 90, 93, 99, 100]、视频识别 [52, 80]、3D 理解 [92, 97, 101] 和自监督学习 [25, 95]。其中，现有的少样本图像分类适配方法分为两类。

**非先验方法**在 CLIP 顶部附加额外的可学习模块，并在不显式利用 CLIP 先验的情况下随机初始化它们。这类方法包括 CoOp [100]、CoCoOp [99]、TPT [71] 和 CLIP-Adapter [24]。这些方法仅引入少量可学习参数（如提示或适配器），但由于缺乏 CLIP 的先验知识，下游任务的准确率有限。

**先验方法**通过显式利用 CLIP 先验和缓存模型可以实现更高的分类准确率，包括 Tip-Adapter [91]、Causal-FS [51] 和 Tip-X [76]。对于每个类有 K 个样本的 C 类数据集，会在顶部构建键值缓存模型。缓存键和值分别用 CLIP 提取的训练集特征$F∈R^{CK×D}$和它们的独热标签$L∈R^{CK×C}$初始化。然后，测试图像和训练图像之间的相似度$R_{fF}$​计算为：
$$R_{fF}​=exp(−β(1−fF^⊤))∈R^{1×CK}\tag{2}$$
其中 β 是平滑标量。然后，将关系$R_{fF}$​视为整合缓存值（即独热标签 L）的权重，并与零样本预测融合作为少样本 logits：
$$logits=R_{fW}​+αR_{fF}​L\tag{3}$$
其中 α 表示平衡因子。通过这种方式，基于先验的方法可以利用$R_{fW}$​和$R_{fF}$​的双边关系实现无训练识别。在此基础上，它们可以进一步使缓存模型可学习，并在训练期间优化训练集特征 F。尽管可学习模块的初始化显式融入了 CLIP 的先验知识，但这些方法因缓存模型带来了过多参数。  

与上述所有方法不同，我们的 APE 和 APE-T 不仅能通过 CLIP 的先验知识实现有竞争力的性能，还能通过自适应先验优化模块引入轻量级参数和计算资源。

## 3. 方法

在 3.1 节中，我们首先通过两个类间指标说明 APE 中的先验优化模块。然后在 3.2 节和 3.3 节中，分别详细介绍基于优化后表征的无需训练和需要训练的变体 APE 和 APE-T。

### 3.1. CLIP 的先验优化

对于下游数据集，CLIP 提取的视觉表征在通道维度上可能同时包含领域特定信息和冗余信息。前者在下游图像分类中更具判别性，后者则代表更通用的视觉语义。因此，我们提出**类间相似度和方差两个标准**，为不同下游场景自适应选择最重要的特征通道。

#### 3.1.1 类间相似度

该标准旨在提取最小化类间相似度的特征通道，即对分类最具判别性的通道。对于下游图像，我们将其 CLIP 提取的特征表示为$x∈R^D$，其中 D 表示总通道数，我们希望从 D 中优化出 Q 个特征通道。然后我们设置一个二进制标志$B∈\{0,1\}^D$，其中$B_k​=1（k=1,...,D）$表示第 k 个元素$x_k$​被选中，且$BB^⊤$=Q。现在，我们的目标是找到最优的 B，为下游数据产生最大的类间差异。  

对于 C 类下游数据集，我们计算所有训练样本的类别间平均相似度 S。我们采用余弦相似度δ(⋅,⋅)作为度量：  
$$S=\sum_{i=1}^C​P^i\sum_{j=1,j≠i}^CP^j\frac{1}{M^i}\frac{1}{​M^j}\sum_{m=1}^{M^i}​\sum_{n=1}^{M^j}​δ(x^{i,m},x^{j,n})\tag{4}$$
其中 $i,j ∈ {1,...,C}$ 表示两个不同的类别。$P^i$、$P^j$表示这两个类别的先验概率，$M^i$、$M^j$表示它们的训练样本总数。  

然而，即使对于少样本，计算整个数据集的 S 也很耗时。考虑到 CLIP 的对比预训练中，视觉 - 语言表征已经很好地对齐，下游类别的文本特征可以被视为一组视觉原型 [17, 37, 72]。这些原型可以近似不同类别视觉特征在嵌入空间中的聚类中心 [27, 82]。为了获得文本特征，我们简单地使用模板` “a photo of a [CLASS]”`，并将所有类别名称代入 `[CLASS]` 作为 CLIP 的输入。然后我们将下游类别的文本特征记为$x^i∈R^D$，其中 $i ∈ {1,...,C}$。

因此，我们采用这些文本特征代替每个类别的图像特征，这使得$M^1=...=M^C=1$。在开放世界设置中，我们还可以假设$P^1=...=P^C=C^1$​。然后，我们定义最小化类间相似度的优化问题：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231602084.png)

其中⊙表示元素乘法，x⊙B仅选择领域特定的特征通道。我们进一步假设文本特征已经过 L2 归一化，因此可以将余弦相似度简化为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231603446.png)

其中$k={d_1​,d_2​,...,d_Q​}$表示$B_k​=1$的选中特征通道索引，$S_k$​ 表示第 k 个通道的平均类间相似度。从式（14）可以看出，求解式（5）中的优化问题相当于选择 Q 个具有最小平均相似度的元素。也就是说，我们按平均相似度对所有 D 个元素进行排序，并选择前 Q 个最小的元素。通过这种方式，我们可以得到二进制标志 B，并获得对下游分类最具判别性的特征通道。  

#### 3.1.2 类间方差

除了类间相似度，我们引入另一个标准来消除类别间几乎不变的特征通道，这些通道没有类间差异，对分类影响很小。为了提高效率，我们还采用类别文本特征$x^i∈R^D(i ∈ {1,...,C})$作为下游数据集的视觉原型。对于第 k 个特征通道，我们将其类间方差表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231618832.png)

其中![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231619354.png)​表示第 k 个通道在类别上的平均方差。与式（14）类似，方差标准也可以视为一个排序问题，选择具有最高方差的前 Q 个通道。通过这种方式，我们可以有效过滤 CLIP 先验知识中对下游数据集冗余和信息量较少的通道。  

最后，我们用平衡因子 λ 将相似度和方差标准融合作为最终度量。对于第 k 个特征通道，我们将其表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231620862.png)
 
其中 k=1,...,D。选择Jk​最小的前 Q 个作为最终优化的特征通道，这表明它们具有最大的类间差异和判别性。  

#### 3.1.3 有效性

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231549501.png)

图 4 显示了我们的自适应优化模块带来的好处。我们通过文本特征在 ImageNet [16] 验证集上进行优化，并可视化统计结果，其中类别数 C 等于 1000。我们使用 ResNet-50 [32] 作为 CLIP 的视觉编码器，从整个 D=1024 个通道中优化出 Q=512 个特征通道。我们参考 [76] 比较了三种指标。如图所示，对于优化后的 512 个特征通道，图像之间的类间相似度（“Inter-class Image-Image”）大幅降低，表明具有很强的类别判别性。同时，我们的优化更好地对齐了配对的图像 - 文本特征（“Matched Image-Text”），并推开了未配对的特征（“Unmatched Image-Text”），这增强了 CLIP 在下游识别中的多模态对应性。

在优化后的 CLIP 提取特征之上，我们提出了两种 CLIP 的少样本适配方法：无需训练的 APE 和需要训练的 APE-T。

### 3.2. 无需训练的 APE

本质上，CLIP 是一种基于相似度的零样本分类器，它依赖于测试图像和类别文本表征在嵌入空间中的距离。

考虑到这一点，我们的 APE 基于优化后的 CLIP 先验，探究测试图像、下游类别文本和缓存模型中的训练图像之间的三边嵌入距离，如图 5 所示。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231626470.png)

对于每个类别有 K 个训练样本的 C-way-K-shot 下游数据集，我们采用 CLIP 提取测试图像、类别文本和训练图像的 L2 归一化特征，分别记为$f∈R^D$、$W∈R^{C×D}$和$F∈R^{CK×D}$。然后我们执行自适应先验优化模块，为这三个特征获得最具信息量的 Q 个通道，表示为$f′∈R^Q$、$W′∈R^{C×Q}$和$F′∈R^{CK×Q}$。这不仅丢弃了预训练 CLIP 中的冗余信号，还减小了缓存模型，降低了推理时的计算成本。

对于三边关系，我们首先将 f 和 W 之间的关系记为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231642669.png)
 
它表示测试图像和类别文本之间的余弦相似度，即 2 节中描述的 CLIP 零样本预测的原始分类 logits。然后，我们将f′和F′之间的亲和力表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231642514.png)
 
参考基于先验的方法 [76, 91]，它表示来自缓存模型的图像 - 图像相似度，带有调制标量 β。此外，我们考虑F′和W′之间的关系，并将它们的余弦相似度表示为$F′W′^⊤$，这表示 CLIP 对少样本训练数据的零样本预测。为了评估 CLIP 的这种下游识别能力，我们计算 CLIP 预测与其独热标签 L 之间的 KL 散度$D_{KL}​(⋅∣⋅)$。我们将其表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231714030.png)

其中 γ 作为平滑因子。$R_{F′W′}$​可以被视为缓存模型中每个训练特征的分数，表明 CLIP 提取的其表征准确性以及它对最终预测的贡献程度。  

最后，整合所有三边关系，我们得到 APE 的整体分类 logits：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231715903.png)

其中 α 作为平衡因子，diag () 表示对角化。第一项表示 CLIP 的零样本预测，包含其预训练先验知识。第二项表示来自缓存模型的少样本预测，它基于优化后的特征通道和$R_{F′W′}$​的重新加权。因此，通过自适应先验优化和三边关系分析，我们的 APE 可以高效且有效地增强少样本 CLIP。  

### 3.3. 需要训练的 APE-T

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508231725550.png)

为了进一步提高 APE 的少样本性能，我们在图 6 中引入了需要训练的框架 APE-T。现有的基于先验的方法 [51, 91] 直接微调缓存模型中的所有训练特征，这导致大规模的可学习参数和计算成本。相比之下，APE-T 冻结缓存模型，仅训练一组额外的轻量级类别残差$Res∈R^{C×Q}$，以及缓存分数$R_{F′W′}​∈R^{1×CK}$。

具体来说，类别残差 Res 由一组 C 个可学习嵌入实现。每个嵌入对应一个下游类别，旨在少样本训练期间为不同类别优化 Q 个特征通道。为了保持嵌入空间中的视觉 - 语言对应性，我们将 Res 应用于文本特征 W 和训练集特征 F。

对于式（9），我们首先通过将冗余通道索引填充为零，将 Q 通道的 Res 填充到如 W 的 D 通道。然后，我们将填充后的 Res 与 W 进行元素相加，通过优化后的文本特征更新 CLIP 的零样本预测，表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508232051863.png)

对于式（10），我们首先通过在每个类别内重复残差，将 C 个嵌入 Res 广播到如 F′ 的 CK。然后，我们将扩展后的 Res 与F′进行元素相加，通过优化训练集特征改进缓存模型的少样本预测，表示为：  

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508232052024.png)

对于式（11），我们在训练期间直接使$R_{F′W′}$​可学习，无需手动计算。通过这种方式，APE-T 可以自适应地学习不同训练集特征的最优缓存分数，并确定哪个特征对预测的贡献更大。

最后，我们也利用式（12）获得 APE-T 的最终分类 logits。通过仅训练这种小规模参数，APE-T 避免了缓存模型的昂贵微调，并通过更新两种模态的优化特征实现了卓越性能。

## 4. 实验

在 4.1 节中，我们首先介绍 APE 和 APE-T 的详细设置。然后在 4.2 节中，我们在 11 个广泛采用的基准测试上评估我们的方法。

### 4.1. 实验设置

数据集。我们采用 11 个图像分类基准测试进行综合评估：ImageNet [16]、Caltech101 [22]、DTD [14]、EuroSAT [33]、FGVCAircraft [56]、Flowers102 [58]、Food101 [10]、OxfordPets [60]、StanfordCars [41]、SUN397 [85] 和 UCF101 [73]。此外，采用 ImageNet-Sketch [79] 和 ImageNet-V2 [66] 来测试泛化能力。给定每个数据集的少样本训练数据，我们在官方验证集上调整模型，并在完整测试集上评估结果。

实验设置。对于 APE 和 APE-T，我们默认采用 ResNet-50 [32] 作为 CLIP 的视觉编码器，输出 1024 通道的视觉 - 语言特征。我们遵循现有工作 [24,91,100] 进行 1/2/4/8/16-shot 学习，并使用 Tip-X [76] 和 CuPL [62] 中的文本提示。对于先验优化模块，我们在式（8）中为 APE 设置 λ=0.7，为 APE-T 设置 λ=0.2。为了训练 APE-T，我们采用批量大小 256 和 AdamW [55] 优化器，配合余弦退火调度器 [54]。我们对 ImageNet 和 Food101 使用 0.0001 的学习率，对其余数据集使用 0.001 的学习率。

### 4.2. 性能分析

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202508232114892.png)

APE 结果。在无需训练的设置下，我们在图 7 中将我们的 APE 与 Tip-Adapter [91] 和 Tip-X [76] 进行比较。它们都是基于先验的方法，并且也无需训练，带有缓存模型。如图所示，在 11 个数据集的平均结果中，APE 在 1 到 16-shot 上始终优于其他方法，表明我们强大的少样本适配能力。尽管我们在 OxfordPets 上落后于 Tip-X，但在 DTD 和 EuroSAT 数据集上观察到显著提升，即在 16-shot 设置下比 Tip-Adapter 分别高出 7.03% 和 7.53%。这证明了优化领域特定知识和利用不同下游场景的三边关系的有效性。

APE-T 结果。在图 8 中，我们将 APE-T 与其他三种需要训练的方法 CoOp [100]、CLIP-Adapter [24] 和 Tip-Adapter-F [91] 进行比较。我们的 APE-T 在每个基准测试上都优于现有方法，并在所有少样本设置下取得了最先进的结果。平均而言，APE-T 在 16-shot 上的准确率为 77.28%，比 Tip-Adapter-F 高出 1.59%。特别是，我们观察到 APE-T 在 DTD 和 FGVCAircraft 上的分类准确率分别比 Tip-Adapter-F 显著提高了 3.05% 和 4.50%。这些优异的结果充分验证了通过我们的可学习类别残差更新优化特征通道的重要性。

计算效率。我们还在表 1 中比较了我们的方法与现有方法的计算开销。我们通过 NVIDIA RTX A6000 GPU 进行测试，并报告在 16-shot ImageNet 上的性能。如表所示，CoOp 涉及最少的可学习参数，但需要大量的训练时间和 GFLOPs 来在整个文本编码器上反向传播梯度。Tip-Adapter-F 减少了训练时间，但通过微调完整的缓存模型带来了大规模的可学习参数，同时梯度计算的 GFLOPs 也不小。相比之下，我们的 APE-T 不仅达到了最高的准确率，还实现了优越的计算效率：GFLOPs 比 CoOp 少 5000 倍，参数比 Tip-Adapter-F 少 30 倍。

泛化能力。在表 2 中，我们通过域内 ImageNet 训练模型，并在分布外数据集上测试它们的泛化能力。凭借最佳的域内性能，我们的 APE 和 APE-T 在 ImageNet-V2 上都实现了显著的分布外性能。对于分布偏移更大的 ImageNet-Sketch，我们无需训练的 APE 优于包括需要训练的方法在内的所有现有方法。然而，由于我们在域内 ImageNet 上训练类别残差，APE-T 在 ImageNet-Sketch 上的测试性能比 APE 差。

## 5. 消融研究

在本节中，我们进行了大量的消融实验，分别研究我们方法中先验优化模块、无需训练的 APE 和需要训练的 APE-T 的贡献。

先验优化模块。在图 9（a）中，我们评估了两个优化标准（类间相似度和方差）的影响，并采用我们无需训练的 APE 与 ResNet-50 [32] 作为基线。如图所示，缺少任何一个相似度或方差标准都会损害性能。此外，我们观察到相似度标准比方差更重要，它能更好地从 CLIP 提取的表征中选择最具判别性的通道。然后在图 9（b）中，我们探究优化通道数 Q 的影响。对于所有 shot，通道数在 [500,900] 范围内时性能更好。这表明我们优化的特征通道比其他冗余通道更重要。

无需训练的 APE。在图 10（a）中，我们分解了所提出的三边关系，并分别揭示了它们的作用。对于 0-shot 结果，“Only RfW​” 表示零样本 CLIP 的性能，准确率为 61.64%。通过配备 “RfW​+Rf′F′​”，带有先验优化的缓存模型有助于在少样本设置下获得更高的性能。最后，考虑所有三个关系（“APE”）构建了性能最佳的框架，这证明了我们三边分析的有效提升。

需要训练的 APE-T。在图 10（b）中，我们比较了 APE-T 中不同可学习模块的影响，包括视觉F′和文本 W 的类别残差 Res，以及缓存分数RF′W′​。从结果来看，每个可学习组件对于充分发挥 APE-T 的潜力都是必要的。我们观察到调整 W 中的优化特征通道比F′更重要。这表明文本零样本预测的作用比缓存模型更关键，因为 CLIP 的原始预训练目标在于视觉 - 语言对比。

## 6. 结论

在本文中，我们提出了一种自适应先验优化（APE）方法，使 CLIP 适配下游数据集。我们的 APE 通过两个标准提取信息丰富的领域特定特征通道，并深入挖掘三个 CLIP 提取表征之间的三边关系。在此基础上，我们提出了 APE 的两个模型变体，分别用于无需训练和需要训练的少样本学习。大量实验表明，我们的方法不仅能取得领先的少样本结果，还能获得优越的效率。我们未来的方向将专注于将 APE 扩展到除分类之外更广泛的基于 CLIP 的下游任务，例如开放世界目标检测、分割和 3D 点云识别，并进一步提高 APE-T 的训练效率，甚至实现无参数增强 [30,94,96]。
