# ClearCLIP：分解 CLIP 表征以实现密集型视觉 - 语言推理

蓝孟程 ¹、陈超锋 ¹、柯一平 ²、王新疆 ³、冯立童 ³⋆、张伟 ³¹ 新加坡南洋理工大学 S 实验室 ² 新加坡南洋理工大学复杂系统与数据科学中心 ³ 商汤科技研究院邮箱：lanm0002@e.ntu.edu.sg；{chaofeng.chen, ypke}@ntu.edu.sg；{wangxinjiang, fenglitong, wayne.zhang}@[sensetime.com](https://sensetime.com/)项目链接：[https://github.com/mc-lan/ClearCLIP](https://github.com/mc-lan/ClearCLIP)

## 摘要

尽管以 CLIP 为代表的大规模预训练视觉 - 语言模型（Vision-Language Models, VLMs）在各类开放词汇任务中取得了成功，但其在语义分割任务中的应用仍面临挑战 —— 生成的分割图常含噪声且存在误分割区域。本文重新深入研究 CLIP 的架构，发现**残差连接（residual connection）** 是导致分割质量下降的主要噪声来源。通过对比分析不同预训练模型中残差连接与注意力输出的统计特性，我们进一步发现：CLIP 的图文对比训练范式以==牺牲局部区分度==为代价==强调全局特征==，最终==导致含噪声的分割结果==。为此，我们提出一种名为**ClearCLIP**的新方法，通过==分解 CLIP 的表征==来==提升开放词汇语义分割性能==。我们对 CLIP 的==最后一层==进行三项简单修改：==移除残差连接、采用自 - 自注意力（self-self attention）、丢弃前馈网络（Feed-Forward Network, FFN）==。在多个基准数据集上的实验表明，ClearCLIP 能持续生成更清晰、更准确的分割图，且性能优于现有方法，充分验证了我们的研究发现的有效性。

**关键词**：语义分割；视觉 - 语言模型；开放词汇

## 1 引言

以对比语言 - 图像预训练（Contrastive Language-Image Pre-training, ==CLIP）系列模型== [8,35,43] 为代表的大规模视觉 - 语言预训练模型（VLMs），在各类下游任务中展现出==卓越的泛化性与鲁棒性==，例如零样本图像分类 [19,35]、视觉问答 [2,21,51] 和图文检索 [9,24,33]。目前，利用 CLIP 解决开放词汇与零样本问题的研究关注度日益提升。然而，正如已有研究 [26,56] 所指出的，==CLIP 在密集预测任务（尤其是语义分割）中难以保持其零样本能力==。这一局限性主要源于 VLMs 的训练数据 —— 此类==数据多为图像级标签，缺乏对视觉定位的敏感性==。例如，如图 1（左上角图像）所示，基于视觉与文本特征间的 patch 级余弦相似度生成的 CLIP 分割图，存在大量误分类 patch 和明显噪声，充分暴露了 CLIP 在密集视觉定位中的不足。

近期研究 [3,26,40,56] 通常将 CLIP 中的噪声表征归因于自注意力层，并通过改进该模块取得了显著进展。例如，==MaskCLIP [56] 将最后一个模块的查询 - 键（query-key）注意力图设为单位矩阵==，使 COCOStuff [4] 数据集上的平均交并比（mIoU）从 4.4 提升至 16.4；==CLIPSurgery [26] 提出值 - 值（value-value）注意力图更清晰==，进一步将 mIoU 提升至 21.9；最新研究 ==SCLIP [40] 结合查询 - 查询（query-query）与值 - 值注意力==，取得了更优性能。尽管这些注意力机制的改进能让模型优先关注相关上下文并提升性能，但其分割结果仍存在噪声；且当使用更大的骨干网络时，噪声会更显著，导致性能下降。这引发了一个核心问题：这些噪声究竟源于何处？又如何在 CLIP 模型中显现？

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510012336491.png)

图 1：左图：开放词汇语义分割示例。CLIP [35] 无法对目标进行定位；MaskCLIP [56] 能区分前景与背景，但仍存在明显噪声；本文提出的 ClearCLIP 可生成高质量分割图。核心发现：原始 CLIP 的分割图可分解为两部分 —— 残差连接对应的杂乱图，以及最后一层 Transformer 注意力输出对应的更清晰、平滑的图。右图：开放词汇语义分割性能对比。

为回答 “噪声来源” 这一问题，我们对 CLIP 的架构进行了深入研究，意外发现：由 ResNet 提出且广泛应用于 Transformer 架构的**残差连接**，对 CLIP 适配开放词汇语义分割任务具有显著影响。为阐明这一点，我们将 CLIP 视觉编码器的输出分解为两个部分 —— 残差连接（$X_{res}$）与注意力输出（$X_{attn}$），具体通过在视觉编码器最后一层直接分离二者实现。如图 1（上方三幅图像）所示，基于残差连接生成的分割结果存在明显噪声，而基于注意力输出特征生成的分割结果则更清晰，且定位性能更优。据此，我们提出：==分割图中的噪声主要源于残差连接==。

[^1]为探究 “噪声如何产生”，我们首先对比了 CLIP 中残差连接与注意力输出的统计特性。值得注意的是，二者的==归一化熵存在显著差异==：CLIP 中残差连接的熵沿网络层逐渐趋近于 0，而注意力输出的熵始终保持在 1 左右。这一发现与我们的观察一致 —— 残差连接在各层中均存在更大的最大值。因此，CLIP 的最终输出（即残差连接与注意力输出之和）表现出与残差连接相似的特性。我们的发现也与文献 [12] 的结论一致，该文献指出大规模预训练模型的最终特征图中存在许多高范数伪影。进一步观察残差连接图可发现，这些峰值集中在少数几个通道中。换言之，残差连接图中的大多数特征向量共享相同的峰值维度（即 latent 空间中的相似方向）。这种特性使得通过余弦相似度区分不同空间特征向量变得困难，从而导致噪声产生。相反，注意力分支中的自注意力机制经过学习可分离不同的空间特征，从而缓解此类问题。

随后，我们研究了同样采用 Transformer 架构但通过自监督方式预训练的 DINO [5] 模型，发现其两种特征图（残差连接与注意力输出）的熵并无上述差异。因此，我们提出：CLIP 中的高层监督会强调残差 latent 空间中的全局特征方向，导致局部特征向量的区分度降低，进而使残差特征中产生噪声。

基于上述发现，我们重新审视了近期方法 [26,40,56]，发现这些方法的性能提升部分源于 “增强注意力输出时降低了残差连接的影响”。由此可推导出：CLIP 适配密集视觉 - 语言推理任务的两个关键因素为 ——**降低残差连接的影响**与**通过自 - 自注意力重组空间信息**。基于这些见解，我们提出 ClearCLIP 方法，对 CLIP 的最后一层进行三项简单修改：移除残差连接、采用自 - 自注意力、丢弃前馈网络（FFN）。这些修改旨在增强注意力输出，从而为开放词汇语义分割任务提供更清晰的表征（如图 1 所示）。在 8 个基准数据集上的大量实验验证了 ClearCLIP 的有效性。

## 2 相关工作

### 2.1 视觉 - 语言预训练

近年来，视觉 - 语言模型（VLMs）取得了显著进展。其中一类重要的视觉 - 语言模型基于对比学习 [1,8,19,25,32,35,43,50,53]，例如 CLIP [35] 在含 4 亿图文对的私有数据集 WIT-400M 上训练，在图文检索、基于文本提示的图像分类等下游任务中展现出优异的零样本能力；ALIGN [19] 采用与 CLIP 相同的双编码器架构，但在含 10 亿以上噪声图文对的私有数据集上训练；OpenCLIP [8] 则通过在公开数据集 LAION [37]（含多达 20 亿图文对）上训练，探索了 CLIP 的缩放规律。另一类研究 [7,22,24] 聚焦于视觉与语言模态间的共享或混合架构，使其具备额外的零样本能力，如视觉问答 [21,22] 和图像描述生成 [24]。本文的研究重点是 CLIP 系列模型 [8,35] 在下游密集预测任务中的适配问题。

### 2.2 开放词汇语义分割

开放词汇语义分割（又称零样本语义分割）旨在对文本描述的任意类别图像进行分割。现有工作多基于大规模视觉 - 语言模型 [8,35,43] 展开，大致可分为三类：

1. **无训练方法**[3,26,27,39,56]：通过最少修改挖掘 CLIP 固有的定位能力。例如，MaskCLIP [56] 提出提取 CLIP 视觉编码器最后一个自注意力模块的值嵌入（value embedding）用于密集预测任务；后续研究 [3,26,40] 将查询 - 键注意力推广为自 - 自注意力机制，如 CLIPSurgery [26] 中的值 - 值注意力、SCLIP [40] 中的查询 - 查询与键 - 键注意力、GEM [3] 中的通用自 - 自注意力组合。这些修改使模型更关注相关上下文，从而显著提升性能。
    
2. **无监督 / 弱监督方法**：主要通过设计更复杂的架构，利用仅图像或图文训练样本显式分组语义内容。例如，GroupViT [44] 和 SegCLIP [30] 在视觉编码器中引入分组模块，其分组令牌（group token）作为语义分割的类别中心；OVSegmentor [46] 通过槽注意力（slot-attention）引入一组可学习的分组令牌，并通过掩码实体补全和跨图像掩码一致性代理任务进行模型训练；PGseg [54] 提出结合分组令牌与原型令牌（prototype token）进行图像分割；TCL [6] 和 CLIP-\(S^4\)[18] 则直接在单幅图像内生成掩码 / 分割提议。
    
3. **全监督方法**：通常涉及域内微调（例如在含完整密集标注的 COCOStuff [4] 训练集上训练），因此性能通常优于无训练和弱监督方法。此类方法可大致分为基于 CLIP 的方法 [17,20,29,48,49,52] 和基于 Stable Diffusion 的方法 [28,45,47]。
    

本文方法属于**无训练开放词汇语义分割**，旨在从特征分解的角度探索 CLIP 固有的定位特性。

## 3 方法

本节首先在 3.1 节概述 CLIP 模型 [35]，并介绍开放词汇密集推理的基准方法；随后在 3.2 节说明 CLIP 基准方法为何无法取得理想结果，从而引出本文研究动机；最后在 3.3 节详细阐述用于开放词汇语义分割的 ClearCLIP 方法。

### 3.1 CLIP 模型基础

#### 3.1.1 ViT 架构

基于 ViT（Vision Transformer）的 CLIP 模型 [35] 由一系列残差注意力模块组成。每个模块的输入为视觉令牌集合\(X=[x_{cls}, x_1, ..., x_{h \times w}]^T\)，其中\(x_{cls}\)表示全局类别令牌，\(\{x_i | i=1,2,...,h \times w\}\)表示局部 patch 令牌。为简洁起见，我们省略层编号，将残差注意力模块表示为：

\(q=Proj_q(LN(X)),\ k=Proj_k(LN(X)),\ v=Proj_v(LN(X)) \quad (1)\)

\(X_{sum}=X_{res}+X_{attn}=X+Proj(Attn_{qk} \cdot v) \quad (2)\)

\(X=X_{sum}+FFN(LN(X_{sum})) \quad (3)\)

其中，LN表示层归一化（Layer Normalization），Proj表示投影层，FFN表示前馈网络；\(X_{res}\)和\(X_{attn}\)分别表示残差连接和注意力输出；\(Attn_{qk}=softmax\left(\frac{qk^T}{\sqrt{d_k}}\right)\)表示查询 - 键（q-k）注意力，\(d_k\)为k的维度。

#### 3.1.2 对比预训练

CLIP 采用基于 Transformer 的视觉编码器v和文本编码器T，分别为每个图文对生成视觉表征\(X_{cls}^{visual}\)和文本表征\(X^{text}\)。CLIP 的预训练基于对比损失：给定一批图文对，模型需最大化视觉表征\(X_{cls}^{visual}\)与其对应文本表征\(X^{text}\)的余弦相似度，同时最小化不同图文对间表征的相似度。

#### 3.1.3 开放词汇密集推理

为将 CLIP 适配于开放词汇语义分割，基准方法采用密集 patch 级分类策略：对于一幅图像，首先通过视觉编码器v提取其视觉表征\(X^{visual}=[x_{cls}^{visual}, X_{dense}^{visual}]^T\)，其中\(X_{dense}^{visual} \in \mathbb{R}^{hw \times d}\)表示d维 latent 空间中的局部 patch 表征；对于文本特征，首先将含C个类别的目标标签嵌入到提示模板 “a photo of a {label}.” 中生成文本描述，再将这些文本描述输入 CLIP 的文本编码器，生成C个类别的文本表征\(X^{text} \in \mathbb{R}^{C \times d}\)。最终分割图\(M \in \mathbb{R}^{hw \times 1}\)的计算方式如下：

\(\mathcal{M}=\underset{c}{arg\ max}\ cos\left(X_{dense}^{visual}, X^{text}\right) \quad (4)\)

### 3.2 研究动机

公式（4）所示的基准方法往往无法取得理想结果 [56]，这可能是因为 CLIP 通过图文间的图像级对比损失训练，导致局部图像区域与文本表征的对齐效果较差 [41]。已有研究 [3,26,40,56] 尝试通过最少修改 CLIP（不重新训练）解决这一问题，其核心思路是将最后一个自注意力层中的原始\(Attn_{qk}\)修改为单位注意力 [56] 或自 - 自注意力 [3,26,40]（即\(Attn_{qq}\)、\(Attn_{kk}\)或\(Attn_{vv}\)），以重组空间信息。如图 2 所示，这些方法确实改进了基准性能：在采用 ViT-B/16 架构的 CLIP（CLIP-B/16）上，COCOStuff 数据集的 mIoU 从 4.4 提升至近 20.0。然而，仍存在以下关键挑战：

1. 现有方法生成的分割结果仍非最优，且存在噪声；
2. 当使用更大模型（如 ViT-L/14）时，这些方法无法取得合理结果。例如，图 2 中 ViT-L/14 架构下，\(Attn_{qq}\)和\(Attn_{kk}\)的性能甚至低于原始\(Attn_{qk}\)，且分割图噪声更多。

这种反直觉现象表明，现有工作在将 CLIP 适配于密集预测任务时，可能忽略了某些重要问题。本文旨在探究：分割结果中的噪声究竟源于何处？又如何显现？

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 2：CLIP-B/16（左图）与 CLIP-L/14（右图）中不同注意力机制的范数与 mIoU 对比。\(X_{attn}\)的范数曲线与 mIoU 曲线呈正相关；CLIP-L/14 中\(X_{res}\)的范数更大，阻碍了通过修改注意力机制提升性能的效果。

### 3.3 ClearCLIP 方法

如 3.1 节所述，基于 ViT 的 CLIP 模块包含三个组件：残差连接、自注意力层和前馈网络（FFN）。我们通过深入分析这些组件对开放词汇语义分割任务的影响，提出 ClearCLIP 方法，以生成更清晰、更准确的分割图。

#### 3.3.1 残差连接的影响

我们首先在 COCOStuff 数据集上，对比了 CLIP-B/16 和 CLIP-L/14 模型最后一个模块中 “残差连接（\(X_{res}\)）” 与 “不同注意力输出（\(X_{attn}\)）” 的弗罗贝尼乌斯范数。如图 2 所示，两幅子图存在共性与差异：

- **共性**：mIoU 曲线与\(X_{attn}\)的范数曲线呈一定正相关；
- **差异**：1）CLIP-B/16 中\(X_{res}\)的范数远小于 CLIP-L/14；2）CLIP-B/16 中修改注意力机制能持续优于\(q-k\)基准，而 CLIP-L/14 中则不能。

据此，我们提出假设：仅当\(X_{res}\)的影响（或范数）较小时，修改注意力机制才有效。换言之，\(X_{res}\)严重损害了 CLIP 系列模型在密集推理任务中的性能。

为验证这一假设，我们基于 CLIP-B/16，分别使用\(X_{sum}\)、\(X_{res}\)和\(X_{attn}\)进行开放词汇语义分割实验，结果如图 3 所示。令人惊讶的是：\(X_{res}\)的 mIoU 接近 0，表明残差连接对图像分割几乎无帮助；相反，仅使用\(X_{attn}\)的 mIoU 远高于\(X_{sum}\)。图 3 的可视化结果进一步表明，CLIP 的含噪声分割图可分解为两部分 ——\(X_{res}\)对应的杂乱图，以及\(X_{attn}\)对应的更清晰图。基于这些实验结果，我们可初步得出结论：分割图中的噪声主要源于残差连接。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 3：CLIP-B/16 模型在 COCOStuff 数据集上使用不同特征图的开放词汇语义分割结果。左图为可视化示例，右图为定量结果。

|特征|mIoU|
|---|---|
|\(X_{sum}\)|4.4|
|\(X_{res}\)|0.01|
|\(X_{attn}\)|11.6|

为更深入理解噪声在语义分割任务中的产生机制，我们对比分析了 CLIP-B/16 与 DINO-B/16 的特征统计特性（DINO 在学习可迁移、语义一致的密集特征方面表现出较强能力 [16,23,31]）。首先，我们计算了各层的归一化熵 [15]，公式如下：

\(H(X^L)=-\frac{1}{\log(hw \times d)} \sum_{i,j} p(X_{i,j}^L) \log p(X_{i,j}^L),\ p(X_{i,j}^L)=\frac{e^{X_{i,j}^L}}{\sum_{m,n} e^{X_{m,n}^L}} \quad (5)\)

其中，\(X^L\)表示 ViT 网络第L层的特征图（即\(X_{sum}\)、\(X_{res}\)和\(X_{attn}\)）。如图 4 (a) 所示，DINO-B/16 中各层\(X^L\)的熵变化不大；而 CLIP-B/16 中，仅\(X_{attn}\)的熵在各层保持稳定，\(X_{sum}\)和\(X_{res}\)的熵则急剧下降至接近 0。根据公式（5），低熵意味着\(X^L\)中存在少量峰值。因此，我们在图 4 (b) 中分析了\(max_{i,j} X_{i,j}^L\)的平均最大值：DINO-B/16 中各类特征图的最大值在各层保持稳定（通常低于 10），因此各层熵值一致；而 CLIP-B/16 中，\(X_{res}\)和\(X_{sum}\)的最大值随层数增加而显著上升，在最后一层达到的峰值约为早期层的 90 倍，导致\(X_{res}\)和\(X_{sum}\)的熵从 ViT 中间层开始急剧下降至接近 0。

通过对多个特征图的可视化（见补充材料），我们发现这些峰值集中在少数通道中。为验证这一观察，我们对\(X_{res}\)中各通道的平均归一化均值按升序排序后计算其平均值，并在图 4 (c) 中可视化：结果显示\(X_{res}\)中的少数通道主导了峰值，与特征图可视化结果一致。

直观而言，这些通道级统计特性反映了\(X^L\)的全局特征（因其与局部模式无关）。若\(X_{res}\)和\(X_{sum}\)的熵低且主要受少数通道影响，则局部信息很可能被破坏。如公式（4）所示，若两个特征向量共享相同的主导通道，则难以通过余弦相似度区分它们。这种特性对优先考虑全局信息的图像识别任务无害，但会导致 CLIP 在强调局部信息的密集预测任务中性能不佳。理论上，这种现象在层数更深的大型视觉 Transformer 模型中会更显著 —— 这也解释了为何现有自注意力修改方法在 CLIP-L/14 模型上无法取得理想结果。

为进一步验证\(X_{res}\)对 CLIP 性能的影响，我们引入缩放因子\(\alpha^3\)，将\(X_{sum}\)表示为\(X_{sum}=X_{res}+\alpha X_{attn}\)，以控制\(X_{attn}\)相对于\(X_{res}\)的影响。图 6 的实验结果表明，更大的\(\alpha\)能显著提升性能 —— 这清晰地证明了\(X_{res}\)对性能的负面影响。最终，我们提出：为在密集视觉 - 语言推理任务中取得最佳性能，应直接丢弃残差连接。

> 注 3：SCLIP [40] 可大致视为\(\alpha=2\)的特例，即\(Proj((Attn_{qq}+Attn_{kk}) \cdot v) \approx Proj(2 Attn_{qk} \cdot v) \approx 2 X_{attn}\)。

#### 3.3.2 前馈网络（FFN）的影响

Transformer 架构中的前馈网络（FFN）在建模数据内的关系与模式方面发挥重要作用。然而，近期研究 [14] 表明，FFN 在推理过程中对图像表征的影响微乎其微；CLIPSurgery [26] 发现，最后一个注意力模块的 FFN 特征与最终分类特征的余弦角显著更大，因此提出在密集预测任务中丢弃 FFN。

本文实验发现：对原始 CLIP 模型而言，单独移除 FFN 对开放词汇语义分割任务的影响极小；但如图 5 所示，当与 “移除残差连接” 结合时，丢弃 FFN 能进一步提升性能（尤其在模型规模较大时）。其原因在于：移除残差连接会显著改变 FFN 的输入，进而影响其输出；因此，移除 FFN 输出可能会减轻其对性能的负面影响。

#### 3.3.3 ClearCLIP 的最终设计

基于上述分析，我们提出一种简洁的 CLIP 适配方案，用于开放词汇语义分割：直接使用最后一个自注意力层的注意力输出作为视觉表征，公式如下：

\(X^{visual}=X_{attn}=Proj(Attn_{(\cdot)(\cdot)} \cdot v) \quad (6)\)

其中，\(Attn_{(\cdot)(\cdot)}\)表示注意力机制中的查询 - 键组合。受已有工作启发，我们尝试了多种组合，最终发现\(Attn_{qq}\)在大多数情况下性能更优，因此将其设为默认选择。

> 注 4：为简洁起见，此处省略了最终投影层。

## 4 实验

### 4.1 实验设置

#### 4.1.1 数据集与评价指标

我们在 8 个广泛用于开放词汇语义分割的基准数据集上评估 ClearCLIP。参考 [6]，这些数据集分为两类：

1. **含背景类**：PASCAL VOC[13]（VOC21）、PASCAL Context[34]（Context60）、COCO Object[4]（Object）；
2. **不含背景类**：PASCAL VOC20[13]（VOC20）、PASCAL Context59[34]（Context59）、COCOStuff[4]（Stuff）、Cityscapes[11]、ADE20K[55]。

实验基于 MMSegmentation [10] 实现，采用滑动窗口策略，将输入图像短边调整为 448 像素。为遵循常规实验设置，我们未对类别名称进行文本扩展，仅使用标准 ImageNet 提示 [35]。为保证对比公平性，所有方法均未采用后处理。由于 ClearCLIP 无需重新训练或微调，可直接在所有数据集的验证集上评估性能。语义分割任务的评价指标采用**平均交并比（mIoU）**。

#### 4.1.2 对比基准方法

我们将 ClearCLIP 与两类开放词汇语义分割方法对比：

1. **无训练方法**：CLIP[35]、MaskCLIP[56]、ReCo[38]、CLIPSurgery[26]、GEM[3]、SCLIP[40]；
2. **弱监督方法**：GroupViT[44]、SegCLIP[30]、OVSegmentor[46]、PGSeg[54]、ViewCo[36]、CoCu[42]、TCL[6]。

除非特别说明，所有基准方法的结果均直接引自其原始论文；此外，我们还基于自身实现补充了 CLIP-L/14 架构下部分基准方法的结果，以实现更全面的对比。

### 4.2 分析与讨论

本节通过大量实验验证 ClearCLIP 的有效性，重点关注 5 个不含背景类的数据集。

#### 4.2.1 消融实验

我们基于 CLIP-B/16 模型进行消融实验，评估各修改组件的有效性，结果如表 1 所示。关键发现：

- 移除残差连接（RC）后，平均 mIoU 从 27.3 显著提升至 33.7，验证了我们的观点 —— 残差特征含有的局部信息较少，会影响密集 patch 预测；
- 单独移除 FFN 无法提升性能；但同时移除残差连接与 FFN 时，模型取得最佳性能（mIoU=37.5）。这一现象的原因在于：移除残差连接改变了 FFN 的输入，进而影响其输出，此时移除 FFN 可减轻其对性能的负面影响。

表 1：基于 CLIP-B/16 架构在 5 个不含背景类数据集上的消融实验结果。RC 表示残差连接（Residual Connection）。

|注意力机制|RC|FFN|VOC20|Context59|Stuff|Cityscapes|ADE20k|平均值|
|---|---|---|---|---|---|---|---|---|
|q-q|✓|✓|68.4|24.9|14.7|20.8|7.6|27.3|
|q-q|✓|✗|62.8|25.5|14.6|19.5|6.9|25.9|
|q-q|✗|✓|77.6|31.8|21.0|23.4|14.7|33.7|
|q-q|✗|✗|80.9|35.9|23.9|30.0|16.7|37.5|

#### 4.2.2 不同架构的适配性

ClearCLIP 的设计简洁，可无缝适配不同架构。我们在 CLIP [35] 和 OpenCLIP [8] 上分别采用 ViT-B/16 和 ViT-L/14 架构进行实验，5 个数据集的平均 mIoU 结果如图 5 所示。关键发现：

1. **跨架构有效性**：在所有架构中，移除最后一个 Transformer 模块的残差连接与 FFN 后，模型的语义分割性能均显著提升 —— 这验证了我们的方法在适配视觉 - 语言预训练模型于下游任务时的有效性；
2. **自 - 自注意力的优势**：在 ClearCLIP 中，自 - 自注意力始终优于原始 q-k 注意力。例如，CLIP-B/16（无 RC 和 FFN）架构下，q-q 注意力的平均 mIoU 为 37.5，高于 q-k 注意力的 27.6；
3. **大型模型的改进**：CLIP-L/14 和 OpenCLIP-L/14 架构下，自 - 自注意力的性能甚至低于原始 q-k 注意力，表明现有仅修改注意力机制的方法未解决 CLIP 适配开放词汇语义分割的核心问题；而 ClearCLIP 通过使用注意力输出，实现了显著性能提升；
4. **ViT-B/16 的特殊情况**：在 ViT-B/16 架构中，单位注意力（I）和 v-v 注意力下，ClearCLIP 的性能提升幅度小于其他注意力类型。这是因为 I 和 v-v 注意力会增强注意力输出的锐度，从而提高其范数（如图 2 所示）—— 我们认为，CLIP-B/16 和 OpenCLIP-B/16 在 I 和 v-v 注意力下的性能提升，主要源于 “隐式消除了残差连接的负面影响”，因此 “显式移除残差连接与 FFN” 的额外提升有限；而在 ViT-L/14 架构中，残差连接的范数显著更大，因此移除残差连接与 FFN 能带来更显著的性能提升。

#### 4.2.3 放大注意力输出范数的影响

为进一步探索开放词汇语义分割任务中 “残差连接” 与 “注意力输出” 的关系，我们设置\(\alpha=\{0.1,1,2,10,100\}\)，将\(X_{attn}\)的 F - 范数显式放大至\(\alpha^2\)倍。如图 6 所示，结果呈现明确趋势：随着\(\alpha\)增大，所有注意力类型的模型性能均显著提升；而当\(\alpha\)从 1 降至 0.5 时，性能急剧下降。这些发现表明：增大注意力输出的范数对减轻残差连接的负面影响至关重要，最终可显著提升性能 —— 这也验证了 ClearCLIP “移除残差连接” 这一设计的简洁有效性。此外，这些见解也解释了 SCLIP [40]（采用 q-q+kk 注意力）性能优异的原因：该注意力机制大致将原始注意力输出翻倍。

### 4.3 与现有最优方法的对比

#### 4.3.1 定量结果

表 2 总结了各类开放词汇语义分割模型在不含背景类数据集上的性能。关键发现：

- ClearCLIP 在 5 个数据集中的 4 个上取得最佳结果，且在所有数据集上均显著优于 TCL，平均 mIoU 提升 4.4；
- SCLIP 的性能也优于多数方法，这是因为其通过 q-q+kk 注意力将注意力输出大致翻倍，从而隐式削弱了残差连接的影响；但 ClearCLIP 通过显式移除残差连接与 FFN，平均 mIoU 比 SCLIP 高出 3.3；
- CLIP-L/14 架构下，MaskCLIP 和 SCLIP 均无法取得理想结果，而 ClearCLIP 的平均 mIoU 达到 34.5，远高于 SCLIP 的 23.6—— 尽管这一结果仍低于 ViT-B/16 架构下的性能，但充分证明了 ClearCLIP 在不同骨干网络上的泛化性。

表 2：不含背景类数据集上的开放词汇语义分割定量对比。†表示结果直接引自 TCL [6]；SCLIP∗表示我们在 “无类别重命名技巧” 的标准设置下复现的结果。

|方法|编码器|VOC20|Context59|Stuff|Cityscapes|ADE20k|平均值|
|---|---|---|---|---|---|---|---|
|GroupViT†[44]|ViT-S/16|79.7|23.4|15.3|11.1|9.2|27.7|
|CoCu[42]|ViT-S/16|-|-|13.6|15.0|11.1|-|
|TCL[6]|ViT-B/16|77.5|30.3|19.6|23.1|14.9|33.1|
|CLIP[35]|ViT-B/16|41.8|9.2|4.4|5.5|2.1|12.6|
|MaskCLIP†[56]|ViT-B/16|74.9|26.4|16.4|12.6|9.8|28.0|
|ReCo†[38]|ViT-B/16|57.7|22.3|14.8|21.1|11.2|25.4|
|CLIPSurgery[26]|ViT-B/16|-|-|21.9|31.4|-|-|
|SCLIP[40]|ViT-B/16|80.4|34.2|22.4|32.2|16.1|37.1|
|SCLIP∗[40]|ViT-B/16|78.2|33.0|21.1|29.1|14.6|35.2|
|ClearCLIP|ViT-B/16|80.9|35.9|23.9|30.0|16.7|37.5|
|CLIP[35]|ViT-L/14|15.8|4.5|2.4|2.9|1.2|5.4|
|MaskCLIP[56]|ViT-L/14|30.1|12.6|8.9|10.1|6.9|13.7|
|SCLIP[40]|ViT-L/14|60.3|20.5|13.1|17.0|7.1|23.6|
|ClearCLIP|ViT-L/14|80.0|29.6|19.9|27.9|15.0|34.5|

表 3 展示了含背景类数据集上的实验结果。关键发现：

- ClearCLIP 的性能显著优于所有弱监督最优方法，平均 mIoU 比 TCL 高出 3.8；
- ClearCLIP 在 VOC21、Context60、COCO Object 数据集上分别比 SCLIP 高出 0.4、2.1、3.0 mIoU—— 这些结果充分验证了 “通过分解 CLIP 特征提升开放词汇语义分割性能” 这一方法的有效性。

表 3：含背景类数据集上的开放词汇语义分割定量对比。†表示结果直接引自 TCL [6]；SCLIP∗表示我们在 “无类别重命名技巧” 的标准设置下复现的结果。

|方法|编码器|VOC21|Context60|Object|平均值|
|---|---|---|---|---|---|
|GroupViT†[44]|ViT-S/16|50.4|18.7|27.5|32.2|
|SegCLIP[30]|ViT-S/16|52.6|24.7|26.5|34.6|
|OVSegmentor[46]|ViT-B/16|53.8|20.4|25.1|33.1|
|PGSeg[54]|ViT-S/16|53.2|23.8|28.7|35.2|
|ViewCo[36]|ViT-S/16|52.4|23.0|23.5|33.0|
|CoCu[42]|ViT-S/16|40.9|21.2|20.3|27.5|
|TCL[6]|ViT-B/16|51.2|24.3|30.4|35.3|
|CLIP[35]|ViT-B/16|16.2|7.7|5.5|9.8|
|MaskCLIP†[56]|ViT-B/16|38.8|23.6|20.6|27.7|
|ReCo†[38]|ViT-B/16|25.1|19.9|15.7|20.2|
|CLIPSurgery[26]|ViT-B/16|-|29.3|-|-|
|GEM[3]|ViT-B/16|46.2|32.6|-|-|
|SCLIP[40]|ViT-B/16|59.1|30.4|30.5|40.0|
|SCLIP∗[40]|ViT-B/16|51.4|30.5|30.0|37.3|
|ClearCLIP|ViT-B/16|51.8|32.6|33.0|39.1|

#### 4.3.2 定性结果

图 7 展示了 ClearCLIP 与三种无训练方法（CLIP、MaskCLIP、SCLIP）的定性对比。关键观察：

1. 与 CLIP 相比，MaskCLIP 的定位能力更好，但分割图仍存在明显噪声和许多不连贯区域（如第 3 列和第 8 列中狗、猫、鸭子的分割结果）；
2. 与 MaskCLIP 相比，SCLIP 能检测更细致的语义特征，且噪声更少；
3. ClearCLIP 始终能生成比其他方法更清晰、更准确的细粒度分割图。

这些观察验证了我们的核心思路 —— 通过分解 CLIP 的表征，可有效提升开放词汇语义分割性能。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 7：开放词汇分割方法的定性对比。

## 5 结论

本文探究了 CLIP 系列模型在开放词汇语义分割任务中产生含噪声分割结果的根源与机制。我们重新审视 CLIP 的架构，对比分析了残差连接与注意力输出的特征统计特性；通过研究不同规模 CLIP 骨干网络的范数差异，发现**残差连接是分割噪声的主要来源**。此外，通过对比 CLIP 与 DINO，我们提出：残差特征缺乏局部信息的原因在于 “高层监督优先关注全局方向”。最后，我们提出 ClearCLIP 方法，通过三项简单修改（移除残差连接、采用自 - 自注意力、丢弃 FFN）显著提升了 CLIP 的语义分割性能。实验表明，ClearCLIP 在 CLIP 系列模型中具有优异的性能与泛化性。

## 致谢

本研究得到新加坡研究、创新与企业 2020 计划（RIE2020）产业协同基金 —— 产业合作项目（IAF-ICP）的资助，同时获得产业合作伙伴的资金与实物支持。

## 参考文献（节选，完整参考文献见原文）

[1] Alayrac, J.B., et al.: Self-supervised multimodal versatile networks. Advances in Neural Information Processing Systems 33, 25–37 (2020)[2] Antol, S., et al.: Vqa: Visual question answering. In: Proceedings of the IEEE International Conference on Computer Vision. pp. 2425–2433 (2015)[3] Bousselham, W., et al.: Grounding everything: Emerging localization properties in vision-language transformers. arXiv preprint arXiv:2312.00878 (2023)[4] Caesar, H., et al.: Coco-stuff: Thing and stuff classes in context. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 1209–1218 (2018)[5] Caron, M., et al.: Emerging properties in self-supervised vision transformers. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9650–9660 (2021)[6] Cha, J., et al.: Learning to generate text-grounded mask for open-world semantic segmentation from only image-text pairs. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 11165–11174 (2023)[7] Chen, X., et al.: Pali: A jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794 (2022)[8] Cherti, M., et al.: Reproducible scaling laws for contrastive language-image learning. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 2818–2829 (2023)[9] Cho, J., et al.: Unifying vision-and-language tasks via text generation. In: International Conference on Machine Learning. pp. 1931–1942. PMLR (2021)[10] Contributors, M.: Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark (2020)[11] Cordts, M., et al.: The cityscapes dataset for semantic urban scene understanding. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 3213–3223 (2016)[12] Darcet, T., et al.: Vision transformers need registers. arXiv preprint arXiv:2309.16588 (2023)[13] Everingham, M., et al.: The pascal visual object classes challenge 2012 (voc2012) development kit. Pattern Anal. Stat. Model. Comput. Learn., Tech. Rep 2007(1-45), 5 (2012)[14] Gandelsman, Y., et al.: Interpreting clip’s image representation via text-based decomposition. arXiv preprint arXiv:2310.05916 (2023)[15] Gray, R.M.: Entropy and information theory. Springer Science & Business Media (2011)[35] Radford, A., et al.: Learning transferable visual models from natural language supervision. In: International Conference on Machine Learning. pp. 8748–8763. PMLR (2021)[56] Zhou, C., et al.: Extract free dense labels from clip. In: European Conference on Computer Vision. pp. 696–712. Springer (2022)

## 附录（节选）

### A 不同骨干网络与数据集的消融实验

图 8 展示了不同 CLIP 模型在各数据集上的消融实验结果。可见，“移除残差连接与 FFN” 的方法在所有数据集上均显著提升了 CLIP 的开放词汇语义分割能力，且在残差连接范数更大的 ViT-L/14 架构上提升更为明显 —— 这些结果充分验证了本文方法的有效性。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

图 8：不同架构与注意力机制下各数据集的消融实验结果。⃝：原始 CLIP；△：移除残差连接的 CLIP；✩：移除残差连接与 FFN 的 CLIP。

### B 通道级残差特征的影响

本节探究低强度残差特征的影响：通过选择性重新引入残差特征中 “平均值较低的通道” 进行实验，并在表 4 中报告 “移除前 β 个高值通道” 与 “归一化” 的效果。结果显示，当 β≥70% 时性能最佳；此外，对\(X_{res}\)进行归一化可显著降低其规模，性能接近 β≥70% 的情况。这些发现支持我们的假设 ——CLIP 中的高层监督会强调残差 latent 空间中的全局特征方向，从而在残差特征中引入噪声。为简洁起见，我们选择移除\(X_{res}\)的所有通道。

表 4：8 个数据集上的平均性能（mIoU）。

|β（%）|0|5|10|30|50|70|100|归一化|
|---|---|---|---|---|---|---|---|---|
|平均值|22.1|30.2|33.5|37.4|38.0|38.1|38.1|38.1|

### C 跨模型集成

ClearCLIP 的修改仅需 2-3 行代码，可作为 “免费改进” 适配于多种架构与分割模型：

- 对 MaskCLIP 和 SCLIP：移除最后一个自注意力层的残差连接与 FFN；
- 对 GEM：使用最后一层的注意力输出作为最终表征；
- 对基准模型（CLIP、BLIP、OpenCLIP、MetaCLIP）：采用完整的 ClearCLIP 修改方案。

表 5 总结了不同模型在 5 个数据集上的性能，结果表明：ClearCLIP 能持续提升现有模型在开放词汇语义分割任务中的性能，展现出优异的泛化性。

### D 特征图可视化

为直观展示残差连接对性能的影响，图 9 可视化了两个随机样本的\(X_{res}\)、\(X_{attn}\)和\(X_{sum}\)特征图。可见：

- 残差连接对应的\(X_{res}\)特征图中，少数通道存在显著高于其他通道的峰值（红色框标注）；
- \(X_{sum}\)与\(X_{res}\)特征图相似，表明\(X_{res}\)对最终特征影响显著；
- 相反，\(X_{attn}\)特征图的通道分布更均匀。

由于分割图基于各空间位置特征向量的余弦相似度生成，\(X_{sum}\)和\(X_{res}\)的特征区分度低于\(X_{attn}\)，从而导致噪声产生 —— 这进一步验证了我们的观点：CLIP 中的高层监督会强调残差 latent 空间中的全局特征方向，降低局部特征向量的区分度，进而在残差特征中引入噪声。

### E 额外定性示例

图 10 和图 11 分别展示了 COCOStuff、ADE20K 和 Pascal Context59 数据集上的额外定性对比结果。与正文发现一致，ClearCLIP 的分割结果噪声显著少于其他方法，进一步证明了其优越性。

[^1]: 要理解这段话，我们需要先拆解**核心概念**（归一化熵、残差连接、注意力输出），再结合 CLIP 的训练目标和特征特性，一步步解释 “熵差异” 如何导致噪声产生。
	
	### 第一步：先搞懂 “归一化熵” 的含义（关键前提）
	
	熵（Entropy）在信息论中是衡量**特征数值分布 “混乱度” 或 “不确定性”** 的指标，归一化后取值范围是「0~1」，含义非常直观：
	
	- **熵≈0**：特征数值分布极不均匀，少数值特别大（“峰值” 突出），大部分值接近 0。
	    
	    就像一张照片里只有 1 个像素特别亮，其他全是黑色 —— 这种特征的 “信息量少”，因为大部分区域的差异被峰值掩盖，无法区分不同局部。
	- **熵≈1**：特征数值分布相对均匀，没有极端突出的峰值，不同位置 / 通道的数值差异能保留。
	    
	    就像一张照片的亮度分布均匀，每个区域的细节都能看清 —— 这种特征的 “信息量足”，能区分不同局部（比如不同物体的 patch）。
	
	### 第二步：CLIP 的两层核心输出是什么？
	
	CLIP 的视觉编码器（比如 ViT 架构）每一层有两个关键输出，最终的层输出是两者相加：
	
	1. **残差连接（X_res）**：为了缓解深层网络的梯度消失，直接把 “上一层的原始输出” 传递到当前层（相当于 “保留原始全局特征”）。
	2. **注意力输出（X_attn）**：通过自注意力机制计算的特征，专门捕捉局部区域的关联（比如 “猫的耳朵” 和 “猫的眼睛” 属于同一物体），是区分局部细节的关键。
	3. **最终输出（X_sum）**：X_sum = X_res + X_attn（残差连接和注意力输出相加）。
	
	### 第三步：为什么两者的熵差异会导致噪声？
	
	结合 “熵的含义” 和 CLIP 的输出逻辑，我们可以拆解用户问题中的关键结论：
	
	#### 1. 残差连接（X_res）的熵 “沿网络层逐渐趋近于 0”
	
	- 含义：随着网络层数加深（从浅层到深层），残差连接的特征越来越 “极端”—— 少数通道 / 位置的数值特别大（峰值），其他数值都很小。
	- 原因：CLIP 的训练目标是 “全局图文对齐”（让整个图像的特征和文本匹配，比如 “猫” 的文本对应整个猫的图像），所以残差连接会优先保留**全局主导特征**（比如整个图像里 “猫” 的全局轮廓），而忽略局部细节（比如猫的爪子、耳朵的细微差异）。
	- 问题：这种 “峰值主导” 的特征无法区分局部区域 —— 比如图像里 “猫的爪子” 和 “背景的地毯”，它们的残差特征都被 “猫的全局峰值” 掩盖，导致两者的特征差异消失，后续分割时就会误判（噪声来源之一）。
	
	#### 2. 注意力输出（X_attn）的熵 “始终保持在 1 左右”
	
	- 含义：注意力输出的特征数值分布均匀，没有极端峰值，能保留局部细节的差异。
	- 原因：自注意力机制的核心是 “关注局部关联”，比如通过计算不同 patch 的相似度，把 “猫的耳朵”“猫的眼睛” 归为同一物体，同时区分 “猫” 和 “地毯”—— 这种计算天然会让特征分布更均匀，避免单一峰值掩盖细节。
	- 优势：这种特征非常适合语义分割（需要区分不同局部物体），因为每个区域的特征都有 “独特性”，能准确匹配文本描述（比如 “猫的爪子” 对应 “a photo of a cat's paw”）。
	
	#### 3. 最终输出（X_sum）“表现出与残差连接相似的特性”
	
	- 关键：残差连接（X_res）的 “峰值数值” 远大于注意力输出（X_attn）的数值。
	    
	    就像 “一滴水（注意力输出）加入一碗浓盐水（残差连接）”，最终混合液的味道还是由盐水主导 —— 即使加上注意力输出，最终输出的特征还是被残差连接的 “极端峰值” 掩盖，熵依然接近 0，局部细节还是无法区分。
	- 后果：CLIP 的最终输出虽然包含注意力的局部信息，但被残差连接的全局峰值 “污染”，导致分割时出现大量误判（比如把 “地毯” 误判为 “猫”），这就是文中说的 “噪声”。
	
	### 总结：一句话讲清逻辑链
	
	CLIP 为了对齐全局图文特征，让残差连接的特征变得 “极端不均（熵≈0）”，而注意力输出的 “均匀特征（熵≈1）” 被残差的峰值掩盖，最终输出的特征无法区分局部细节，导致语义分割时产生噪声。
