# SCLIP：重新思考自注意力机制在密集型视觉 - 语言推理中的应用

冯王、梅洁如、艾伦・尤伊尔约翰・霍普金斯大学

## 摘要

对比性语言 - 图像预训练（CLIP）领域的最新进展表明，通过在图像层面对齐视觉特征与文本特征，该模型在零样本分类任务中展现出强大能力。然而，在密集预测任务中，CLIP 往往难以对图像内的视觉特征进行定位，无法在像素级分割任务中取得理想结果。本研究深入探究 CLIP 的空间推理机制，发现其在密集预测任务中表现不佳的原因在于==自注意力过程中存在位置错位问题==。基于这一发现，我们提出一种无需训练的 CLIP 语义分割适配方法：仅对 CLIP 进行简单修改，就能有效解决位置错位问题。具体而言，我们重新设计自注意力机制，==利用查询与查询（query-to-query）、键与键（key-to-key）之间的相似度来确定注意力分数==。值得注意的是，这种对 CLIP 的微小修改显著提升了其密集预测能力 —— 在 8 个语义分割基准测试中，原始 CLIP 的零样本平均交并比（mIoU）从 14.1% 提升至 38.2%，大幅超过现有最优方法（SoTA）33.9% 的成绩。相关代码可在[https://github.com/wangf3014/SCLIP](https://github.com/wangf3014/SCLIP)获取。

**关键词**：CLIP、自注意力机制、语义分割

## 1 引言

在大型基础模型时代，先进行大规模预训练，再针对各类下游任务进行最小化适配，已成为迁移学习的新范式。然而，与自然语言处理领域基础模型取得的显著成功 [4,14,45] 不同，大多数视觉模型在各类下游任务中尚未展现出同等水平的零样本迁移学习能力 [3,34]。通过引入语言监督并在网络规模数据集上进行训练，对比性语言 - 图像预训练（CLIP）模型 [28,44] 能够将视觉表征推广到开放词汇推理任务中，并在零样本分类任务中取得出色成绩，但在语义分割等更复杂任务中，这种能力仍十分有限。

具体来说，CLIP 通过将图像层面的表征与一系列目标文本嵌入进行匹配来实现零样本分类。在配合适当的提示策略 [44] 时，其在 ImageNet [13] 数据集上的测试准确率可超过 70%。然而，将这种推理流程直接迁移到语义分割任务中，却无法获得理想结果。例如，当 CLIP 配备 ViT-Base/16 [17] 编码器且输入图像分辨率为 224×224 时，可生成 14×14 的密集特征图；若简单地将这种补丁级（patch-level）表征与文本嵌入关联，CLIP 在 ADE20k [70] 数据集上的交并比（mIoU）仅为 3.1%，在 COCO-Stuff [5] 数据集上也仅为 5.7%。而有监督模型在这两个基准测试中通常能达到约 40% 的交并比，两者性能差距悬殊。因此，CLIP 在下游密集预测任务中，仍需依赖精细的微调与领域内适配 [39,64,73]。

本研究探究 CLIP 在密集预测任务中的潜力，旨在明确 CLIP 的弱监督能力是否能以最小化下游适配为代价，为各类视觉任务提供帮助。研究首先从定性分析入手，如图 1 所示，我们在 5 张来自 COCO [5] 数据集或真实场景的样本图像上进行简单的开放词汇语义分割实验。结果显示，原始 CLIP 模型的密集预测结果往往存在错误，分割掩码也较为杂乱。但我们发现，尽管 CLIP 的语义分割性能较差，它实际上能够大致识别图像中出现的物体，问题在于定位错误。例如，在第二个示例中，我们设置了 10 个目标类别（包括火烈鸟、水、陆地等），并加入天空、建筑物、人等干扰类别。CLIP 虽能准确识别出水、火烈鸟等正确类别，却出现了定位颠倒的情况 —— 将火烈鸟区域预测为水，将水和陆地区域预测为火烈鸟。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510191050459.png)

这一定性研究表明，CLIP 分割性能不佳的原因在于补丁表征存在空间错位，而非无法提取密集视觉特征。这一发现让我们怀疑问题出在 CLIP 的自注意力模块 —— 该模块负责处理空间信息。如图 2 所示，我们展示了 CLIP 自注意力模式的多个示例，每张图代表图像中特定点（用不同颜色标记）的注意力分数。结果显示，CLIP 的注意力图虽能反映主要物体的形状，但图像中多个不同源点对应的注意力图却极为相似。这表明 CLIP 学习到的是==空间不变性视觉特征==，即局部特征往往不受其在图像中空间位置的影响，模型更关注整体视觉表征。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510191052722.png)

然而，在语义分割等密集预测任务中，我们实际需要的是==空间协变性特征== —— 即局部表征应随其在图像中的空间位置变化而变化。为此，我们重新思考自注意力机制的设计目的，提出了==相关自注意力（Correlative Self-Attention，CSA）==机制。这是一种新型自注意力机制，能够促进协变性视觉特征的生成。具体而言，原始自注意力机制通过两个投影矩阵（查询矩阵和键矩阵）确定注意力分数，而我们的 CSA 模块==仅对输入进行一次投影==，即可找到视觉标记之间的成对相关性，从而促使每个局部标记既关注自身，也关注具有相似信息的位置。

令人意外的是，我们发现进行这一修改后，CSA 机制能有效将 CLIP 适配到密集预测任务中。具体来说，我们提出了新方法 ==SCLIP（适配分割任务的 CLIP 模型==，Segmentation-adapted CLIP model），通过在 CLIP 视觉编码器中用 CSA 模块替换原始自注意力块来实现 ¹。值得注意的是，==CSA 模块对投影权重不敏感，因此我们可直接复用 CLIP 原始自注意力机制的预训练参数，这使得 SCLIP 成为一种无需微调、可独立使用 CLIP 进行语义分割的方法==。

对 SCLIP 模型的实证研究表明，该模型在定性和定量层面均取得了显著成效：==在 PASCAL Context [40]、COCO-Stuff [5] 等 8 个语义分割基准测试中==，其==平均交并比（mIoU）达到 38.2%==，大幅超过 MaskCLIP [71]（30.3%）、GroupViT [61]（30.7%）、TCL [8]（33.9%）等现有支持零样本和开放词汇语义分割的最优方法。如图 1 所示，我们还展示了 SCLIP 在 COCO [5] 数据集图像及真实场景图像上的定性结果 —— 该模型能生成清晰、准确的分割掩码，尤其在高分辨率输入（如 “两只狗坐在船上” 的示例）上表现突出。本研究的**主要贡献**可总结如下：

1. 我们明确了 CLIP 在语义分割任务中表现不佳的原因，并通过引入新型相关自注意力（CSA）机制解决了这一问题，大量实验验证了该机制的显著效果。
2. 在仅使用预训练 CLIP 模型、不进行微调且不引入额外参数的情况下，我们的 SCLIP 方法性能超过了现有方法 [8,49,61,71]，这验证了视觉 - 语言模型在密集预测任务中具有良好的迁移能力。
3. 本研究通过对 CLIP 的微小修改，显著提升了其语义分割性能，这一重要发现表明：结合语言引导的弱监督预训练范式，具有成为支持各类下游任务的视觉基础模型的巨大潜力。

> ¹ 本文聚焦于基于 Transformer 的 CLIP 图像编码器。与 ResNet [25] 编码器相比，视觉 Transformer [17] 更适合零样本迁移至语义分割任务，原因在于：1）具有全局感受野；2）下采样率更低（例如，ViT-Base/16 的下采样率为 16 倍，而 ResNet-50 为 32 倍）。

## 2 相关工作

### 可迁移视觉基础模型

自监督预训练在学习可迁移视觉表征方面展现出良好潜力。通过重构目标（如掩码图像建模 [2,23,58]）或判别目标（如对比学习 [7,9,21,24,26,54]）进行预训练的模型，在下游训练数据充足时，能够很好地适配各类视觉任务。同样，支持高分辨率条件图像生成的去噪扩散模型 [15,27,50]，以及支持语义无关图像分割的 “任意分割” 模型（Segment Anything models [30,76]），也可作为具有可迁移视觉特征的基础模型。

当结合语言引导后，这类基础模型能在下游视觉任务中实现强大的开放词汇和零样本迁移学习 [35,42,44,47]。CLIP [44] 是其中的代表性模型，它通过对比性预训练实现视觉与文本特征的对齐，开创了该领域的研究方向。基于此，一系列后续研究在模型规模 [28,43,66,68]、应用场景 [22,48,61,71] 及下游推理流程 [19,53,72] 等方面对其进行了扩展。

### 开放词汇分割

为充分利用视觉 - 语言模型在零样本和开放词汇视觉推理中的优势，众多后续研究开始探索其在密集预测任务中的应用。例如，GroupViT [61] 在视觉编码器中引入组标记（group tokens），并结合语言引导进行预训练，构建出适用于语义分割任务的开放词汇模型。此外，MaskCLIP [71] 和 CLIP Surgery [33] 通过对视觉 Transformer 进行简单修改，实现了 CLIP 的粗粒度特征定位。目前，语言引导分割领域的研究仍在持续推进 [8,20,32,36,39,41,46,49,59,62-65,69,73]。

### 用于密集视觉特征的自注意力机制

多项相关研究表明，通过采用不同的自注意力机制，可增强视觉 Transformer 提取密集视觉特征的能力。例如，
- **局部注意力**：与 CLIP 及传统视觉 Transformer [17,44] 中使用的原始自注意力机制不同，局部注意力（Local Attention）机制将空间特征聚合限制在局部窗口内，以获取细粒度特征 [37,38,52,57,74]。
- **修改自注意力机制**：修改自注意力机制也可增强局部视觉特征，如 MaskCLIP [71] 在最后一个 Transformer 层中舍弃查询向量和键向量的处理（相当于窗口大小为 1 的局部注意力），MSSA [67] 则将注意力投影简化为单个矩阵。
- **其他**：一些面向分割或检测任务的 Transformer 模型通过交叉注意力（cross attention）将局部视觉特征映射到语义标记 [6,10,11,51,61]；配备轴向注意力（Axial Attention [16,55,56]）或可变形注意力（Deformable Attention [60,75]）的模型，在密集预测任务中也表现出强大能力。

## 3 方法

本方法的核心思想是：通过架构修改，将 CLIP 范式学习到的空间不变性视觉特征转化为协变性表征，使 CLIP 模型能够适配密集预测任务。如第 1 节所述，空间不变性特征意味着模型对图像中不同位置生成相似表征，且倾向于整合整体信息（见图 2），这在图像分类等图像层面任务中具有优势；而空间协变性特征则要求每个局部标记能有效表征对应位置的视觉信息，这对语义分割等像素级密集预测任务至关重要。我们通过引入新型自注意力机制来重新组织空间信息，进而提出了 SCLIP（适配分割任务的 CLIP 模型）方法，具体细节如下。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510191547309.png)

### 3.1 重新审视原始自注意力机制

在传统视觉 Transformer [17] 中，首先将尺寸为 3×w×h 的输入图像划分为若干不重叠的补丁（patch），然后将每个补丁投影为向量特征 $x_{i} \in \mathbb{R}^{d}$ （其中 d 表示模型特征空间的维度）。视觉 Transformer 的每一层接收一组视觉标记 $X=\{x_{cls}, x_{1}, x_{2}, ..., x_{l}\} \in \mathbb{R}^{(l+1) ×d}$ 作为输入，其中 $x_{cls} \in \mathbb{R}^{d}$ 为类别标记，$l=wh/p^2$ 为图像补丁总数（每个补丁尺寸为 p×p），每个局部视觉标记 $x_{i} \in \mathbb{R}^{d}$（i=1,2,...,n）对应输入图像中的一个特定位置。

图 3（左侧）展示了传统自注意力块的流程。形式上，注意力图 $Attn \in \mathbb{R}^{(l+1) ×(l+1)}$ 的计算方式如下：$$Attn=Softmax\left(X W_{q} W_{k}^{T} X^{T} / \sqrt{d}\right)$$其中 $W_{q}$、$W_{k} \in \mathbb{R}^{d ×d}$ 是通过预训练学到的投影参数。为简化描述，此处仅考虑单头自注意力机制。

在 CLIP 中，视觉编码器经过预训练后，会将每个输入图像表示为单个特征向量。这使得自注意力块更倾向于提取整体视觉表征，进而生成空间不变性特征。如前所述，这种不变性特征会阻碍 CLIP 在密集预测任务中的应用，因此需要对其自注意力模块进行修改，以适配语义分割任务。

实现这一目标的直接方法是强制每个视觉标记\(x_{i}\)仅关注自身，即无论输入如何，都将注意力图Attn设置为单位矩阵 $I_{(l+1) ×(l+1)}$ 。这样一来，每个局部视觉标记仅接收对应位置的信息，视觉特征的定位效果会更好。在实际应用中，MaskCLIP [71] 在 CLIP 视觉编码器的最后一层采用这种注意力图，使语义分割性能得到了一定提升 —— 例如，将 CLIP 在 COCO-Stuff [5] 数据集上的交并比（mIoU）从 5.7% 提升至 16.7%。然而，这种方法严格限制了局部标记的感受野，导致模型容易过度关注低层次特征，进而生成杂乱的密集预测结果 [8,71]。

### 3.2 相关自注意力机制

为生成空间协变性特征，我们提出相关自注意力（CSA）机制 —— 通过局部标记间的成对相关性计算注意力分数，其整体流程如图 3 所示。形式上，注意力图的计算方式如下：$$Attn =Softmax\left(X W_{r} W_{r}^{T} X^{T} / \tau\right)$$其中 $X \in \mathbb{R}^{(l+1) ×d}$ 表示输入，$W_{r} \in \mathbb{R}^{d ×d}$ 是新引入的投影矩阵。参考传统自注意力机制，温度系数 τ 默认设置为 $\sqrt{d}$ 。这一修改使得自注意力机制依赖于不同位置特征向量之间的距离，其核心思想是：经过投影后，若标记 $x_{i}$ 和 $x_{j}$ 的余弦相似度较高，则它们相互分配的注意力分数也较高。与传统机制相比，这种相关自注意力机制更适合密集预测任务，原因如下：

首先，在视觉 Transformer 中，特征定位效果可通过注意力图 $Attn \in \mathbb{R}^{(l+1) ×(l+1)}$ 对角线元素的大小直观反映。具体而言，Attn中的每个元素 $a_{i j} \in[0,1]$ 表示标记 $x_{i}$ 对 $x_{j}$ 的注意力分数，对角线元素值越高，说明每个局部标记更关注自身位置，进而能更好地保留各位置的视觉信息。这也解释了 MaskCLIP [71] 有效的原因 —— 它强制设置 $a_{i j}=0$（i≠j）且 $a_{i j}=1$（i=j）。在 CSA 模块中，对角线注意力分数同样会得到增强：假设两个向量均已归一化，那么当 i=j 时，$x_{i} W_{r}$ 与 $x_{j} W_{r}$ 之间的相关性始终最大。

除了出色的特征定位能力，CSA 模块还充分考虑了局部标记间的语义相关性，从而生成稳健、平滑的密集预测结果。直观而言，对于每个局部标记 $x_{i}$ ，CSA 不仅会给 $x_{i}$ 自身分配高注意力分数，还会给具有相似语义内容的标记分配高分数。我们在图 4 中可视化了这一效果：对于每个源点，只有与其语义相似度高的位置会被分配显著的注意力，因此在注意力图中可清晰识别每个源点对应的物体（如椅子和猫）。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510191830646.png)

此外，CSA 中的矩阵\(w_{r}\)起到衡量不同位置特征间距离的作用，因此模型对该投影层的参数不敏感 —— 改变\(w_{r}\)只会改变距离度量的形式。实验表明，无需专门训练该投影矩阵：手动赋值甚至采用多个随机初始化矩阵的集成，都能持续获得极具竞争力的结果（详见第 4.3 节和表 2）。值得注意的是，CSA 对模型参数的不敏感性，使其在给定预训练 CLIP 模型时，具有零样本适配密集预测任务的良好潜力。凭借这一优势，我们可基于 CSA 构建分割模型，且无需引入额外参数或进行下游微调。

### 3.3 适配分割任务的 CLIP 模型（SCLIP）

为构建 SCLIP 方法，我们以配备 ViT-Base/16 [17] 图像编码器的预训练 CLIP 模型作为骨干网络。通常，在不引入额外参数的情况下将 CLIP 适配到下游任务时，我们会将其最后一层或最后几层视为任务特定的解码器头。通过观察不同层的自注意力模式，我们将 CLIP 图像编码器的最后一个 Transformer 块作为适配的解码层，其余组件保持不变。

在该解码层中，我们用 CSA 模块替换原始自注意力块，并将原始自注意力机制的 $W_{q}$ 和 $W_{k}$ 参数复用为 CSA 的投影矩阵。形式上，注意力图的计算方式如下：$$\begin{array} {rlr}{Attn}&{=Softmax\left( XW_{q}W_{q}^{T}X^{T}/\tau \right) }&\\ &{+Softmax\left(XW_{k}W_{k}^{T}X^{T}/\tau \right) ,}&\end{array}$$ 由于矩阵 $W_{q}$ 和 $W_{k}$ 可直接从 CLIP 中加载，因此这种适配无需额外训练。

#### 密集视觉特征的后处理

在密集预测任务中，我们普遍存在一个简单但关键的预设假设 —— 空间连续性：图像中相邻的像素或补丁往往具有相似的视觉特征。在全监督训练中，这一先验知识很容易引入，因为分割掩码的标签本身就满足这一假设。然而，在 CLIP 这类弱监督预训练中，并没有明确的约束来保证密集视觉特征的空间连续性，仅在输入层添加了位置嵌入。因此，现有零样本分割模型通常依赖特定的后处理策略来优化或平滑分割掩码（例如，TCL [8] 使用的 PAMR [1]、ReCo [49] 使用的 DenseCRF [31]）。

但我们认为，这类后处理方法不应作为默认选项，因为确保输出的空间连续性也是语义分割模型推理能力的重要组成部分。实验表明，SCLIP 在这方面具有很强的稳健性 —— 无需依赖任何优化或平滑策略，就能生成良好的分割结果。

## 4 实验

### 4.1 实验设置

#### 数据集

我们在 6 个常用的语义分割基准数据集上对方法进行评估，包括 PASCAL VOC 2012 [18]、PASCAL Context [40]、Cityscapes [12]、ADE20k [70]、COCO-Stuff 和 COCO-Object [5]。考虑到背景类别，我们还额外在 PASCAL VOC 和 PASCAL Context 的两个变体数据集上进行评估：VOC21 和 Context60 为包含背景类别的原始数据集，VOC20 和 Context59 为不含背景类别的变体数据集。

在 GroupViT [61]、TCL [8] 等先前研究中，实验采用的输入图像预处理方式为：将图像短边调整为 448 像素，然后以 448×448 为窗口、224 为步长进行滑动推理。然而，我们的实验发现，使用更小的输入尺寸和更密集的滑动步长，能带来略微更高的性能（例如，在 PASCAL Context 数据集上，交并比（mIoU）提升 0.2%）。具体而言，我们将输入图像短边调整为 336 像素，以 224×224 为窗口、112 为步长进行滑动推理。该流程的计算量与 GroupViT 相近，但更符合 CLIP 原始输入尺寸（如 ViT-Base 的输入尺寸为 224 像素），且支持并行计算。对于 Cityscapes [12] 数据集，由于其原始图像分辨率较高，我们将短边调整为 560 像素。不同图像预处理流程的详细对比见表 4。

#### 基准模型

CLIP [44] 是我们方法的直接基准，用于对比原始自注意力机制与 CSA 机制在密集预测性能上的差异。具体而言，我们首先从 CLIP 的语言编码器中提取目标类别名称的文本嵌入，然后将其与 CLIP 视觉编码器的密集特征直接对齐。此外，我们还将基于 CLIP 或类似视觉 - 语言模型构建的开放词汇语义分割模型作为更强的基准，包括 MaskCLIP [71]、ReCo [49] 和 TCL [8]。对于这些方法，我们会同时参考基于其官方代码库重新实现的结果和现有研究 [8] 中的结果，并选取较高值进行报告。我们还将方法与 SegCLIP [39]、OVSegmentor [63] 等最新基准模型进行对比，相关结果直接取自其原始论文。

参考 TCL [8] 的设置，我们不允许使用计算成本极高的后处理策略（如 Dense CRF [31]），也不考虑除 CLIP 外还依赖其他预训练模型的基准 [29,62]。默认情况下，我们不采用像素自适应掩码优化（Pixel-Adaptive Mask Refinement，PAMR [1]）技术对分割掩码进行后处理，因为该技术计算量大，且可能掩盖分割模型本身的推理能力。

### 4.2 主要结果

表 1 总结了各类零样本语义分割模型的对比结果。我们的 SCLIP 在 8 个评估基准上均取得最佳性能，尤其在 PASCAL Context（34.2%）、Cityscapes（32.2%）和 ADE20k（16.1%）数据集上优势显著。总体而言，SCLIP 的平均性能达到 38.0%，显著高于排名第二的 TCL（33.9%）。这表明 SCLIP 相比现有方法实现了稳健提升，也验证了新提出的相关自注意力（CSA）机制的显著效果。

除竞争基准方法外，我们还报告了在图像编码器中使用原始自注意力机制的原始 CLIP 模型的评估结果。结果显示，这种直接迁移的方式性能远不及其他基准方法，说明原始自注意力机制无法直接适配密集预测任务。

表 1 中还包含额外使用 PAMR 后处理层的结果：几乎所有方法都能从中获益，且提升幅度相近。例如，SCLIP 在 8 个数据集上的平均交并比（mIoU）提升 1.9%，而 GroupViT 和 TCL 等基准模型的提升幅度分别为 1.4% 和 3.3%。与 TCL 论文 [8] 中报告的 “MaskCLIP 使用 PAMR 模块后预测性能下降” 不同，我们发现：通过简单搜索合适的 PAMR 超参数，MaskCLIP 的交并比（mIoU）可比原始版本提升 3.5%。我们建议在开放词汇分割的默认设置中禁用这种优化策略（因其计算密集），转而采用轻量级的预测平滑方法。

### 4.3 消融实验

#### 相关自注意力机制中的投影矩阵

我们探究了在相关自注意力（CSA）块中选择不同类型投影矩阵的影响。如前所述，理论上 CSA 模块可接受任何非零投影矩阵作为\(W_{r}\)，默认情况下，我们采用 CLIP 原始自注意力机制中的 $W_{q}$ 和 $W_{k}$ 进行集成（如公式 3 所示）。此处，我们额外对比了 4 种变体，以验证 CSA 的稳健性：

1. **单位投影（Identity Projection）**：直接通过输入 X 衡量成对相关性，流程简化为 $Attn =Softmax(X X^{T} / \tau)$ 。需注意，这与 MaskCLIP（直接将Attn设为单位矩阵）不同。
2. **随机初始化集成（Ensemble of Random Initializations）**：随机初始化多个投影矩阵作为 $w_{r}$，然后对其对应的注意力分数取平均值。形式上为 $Attn =\frac{1}{n} \sum_{i=1}^{n} softmax (X W_{i} W_{i}^{T} X^{T} / \tau)$ 。
3. **单一 $w_{q}$ 或 $W_{k}$ 投影（Projection with Single $w_{q}$ or $W_{k}$ ）**：分别加载单一的 $w_{q}$ 或 $W_{k}$ 作为 $w_{r}$ ，以分析两者结合使用的效果。
4. **学习投影（Learned Projection）**：为充分挖掘 CSA 的潜力，在每个数据集的训练集上专门学习一个投影矩阵。由于可学习参数较少，即使使用少量训练样本（每个数据集使用 64 个样本），模型也能很好地收敛。

表 2 总结了实验结果。总体而言，在 3 个数据集上，不同模式的性能差异极小，这充分证明了所提 CSA 机制的稳健性。值得注意的是，即使仅使用一个随机初始化矩阵，也能取得不错的结果（例如，在 PASCAL VOC 数据集上的交并比（mIoU）为 57.1%）。此外，尽管 “学习投影” 模式性能最高，但相比默认的无训练结构，提升幅度并不显著。考虑到这种有限的收益，不建议投入大量精力进行领域内训练以获取学习投影矩阵。除 “学习投影” 外，默认方法在所有模式中始终表现最佳，这表明 CSA 与 CLIP 预训练投影参数具有高度兼容性 —— 这种兼容性也证明了 CSA 与 CLIP 预训练投影提供的稳健特征相结合时的有效性。

#### 特征定位的替代方法

还存在其他一些可能使 CLIP 实现视觉特征定位的方法。例如，可通过调整 CLIP 视觉编码器的温度参数来锐化注意力图，防止模型过度关注全局信息，转而将特征集中在少数特定位置。我们将这种方法称为 “注意力锐化（Attention Sharpening）”，并与我们的方法进行对比。类似地，通过采用局部注意力技术（仅在指定窗口内计算注意力分数），也可帮助 CLIP 将视觉特征锚定到对应位置。但这种方法的代价是失去视觉 Transformer 固有的全局感受野，导致模型无法借助窗口外标记进行推理。需注意，当窗口大小设为 1 时，MaskCLIP [71] 算法可视为局部注意力的一种特殊情况。

我们还观察到，视觉 Transformer 的早期层会关注相对较小的局部区域。因此，另一种可能的 CLIP 特征定位方法是：在解码层中直接采用早期层的注意力图，而非原始解码层的注意力图。

表 3 总结了实验结果。结果显示，这三种替代策略在调整特定参数后，虽能相比基准 CLIP 模型带来一定提升，但性能仍远不及我们的方法。具体而言，“注意力锐化” 方法在大多数情况下无法提升性能，仅在 PASCAL VOC 数据集上（τ=2 时）实现了 2.9% 的交并比（mIoU）提升；当窗口大小设为 3 时，局部注意力方法的性能表现良好，在 3 个数据集上的性能几乎与 MaskCLIP 持平；此外，直接采用早期层注意力图的启发式方法效果相对较好 —— 在 PASCAL Context59 数据集上的交并比（mIoU）为 26.8%，在 COCO-Stuff 数据集上为 16.8%，甚至超过了 MaskCLIP。

这一消融实验表明：仅关注局部视觉特征，无法有效将 CLIP 这类弱监督预训练模型适配到语义分割任务中。相比之下，我们的方法结合了相关自注意力机制，既考虑了局部特征间的关系，又兼顾了整体语义上下文，因此更适合处理不同尺度的视觉推理任务。

#### 图像预处理

如第 4.1 节所述，我们采用了新的预处理流程：将输入图像短边固定为 336 像素（而非 448 像素），并以 224×224 为窗口、112 为步长进行滑动推理（相比先前方法 [8,61] 的窗口和步长更小）。为分析该流程的影响，我们在表 4 中详细对比了不同预处理策略的效果。

结果显示，总体而言，更大的图像尺寸结合更小的窗口和步长，能带来更好的性能，但同时也会增加计算成本（表现为浮点运算次数（Flops）增加）。具体而言，图像尺寸过小会导致大量信息丢失，性能显著下降 —— 例如，模式 1（短边 224 像素）的交并比（mIoU）仅为 56.5%；更大的图像尺寸可提升预测精度（如模式 4 所示），但相比默认设置（模式 2），提升幅度并不显著。

与现有研究的默认设置（模式 5）相比，我们提出的流程在计算量相当的情况下取得了更好的结果。这可能源于两个因素：一是 CLIP 在未微调时，对原始输入尺寸（224×224 像素）的适配性更好；二是我们的设置减小了窗口步长，使输出更平滑。此外，即使采用相同的预处理方法（模式 5），我们的 SCLIP 在 PASCAL VOC 数据集上的交并比（mIoU）仍达到 58.9%，超过了 TCL 的 51.2%。

## 5 结论

本研究通过引入新型相关自注意力（CSA）机制，提升了 CLIP 在密集预测任务中的潜力，并将该机制作为语义分割任务的特定解码器头，构建了 SCLIP 方法。这种适配显著提升了 CLIP 在密集型视觉 - 语言推理任务中的性能：在 8 个基准测试中，零样本平均交并比（mIoU）达到 38.2%，大幅超过现有最优模型。

研究表明，对现有 CLIP 模型进行微小修改，就能显著扩展其功能。各类基准测试中零样本交并比（mIoU）的显著提升，验证了我们方法的有效性。值得注意的是，我们的模型在不进行任何微调、不引入额外参数的情况下，性能超过了现有基准方法 —— 这充分证明了 CLIP 这类弱监督预训练范式在构建通用视觉基础模型方面的强大潜力。

## 致谢

本研究得到美国海军研究办公室（ONR）资助，项目编号为 N00014-23-1-2641。

## 附录：额外可视化结果

此处展示了 SCLIP 在 PASCAL VOC [18]（图 5）和 COCO-Object [5]（图 6）数据集上的更多定性结果，并与原始 CLIP [44] 和 MaskCLIP [71] 进行对比。结果显示，SCLIP 在大多数情况下能生成清晰的分割掩码；原始 CLIP 无法正确定位图像中的主要物体；MaskCLIP 的预测结果则常存在明显噪声和不连贯片段。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

具体而言，在 PASCAL VOC 这类类别相对较少的数据集上（见图 5），SCLIP 能够检测到非常细致的语义特征。例如，在第一个示例中，模型准确分割出了羊的腿部（尽管腿部在图像中占比极小）；在第四个示例中，分割掩码清晰呈现了盆栽植物的枝干形状 —— 虽比地面真值略粗糙，但显著优于 MaskCLIP（MaskCLIP 将盆栽周围区域与背景归为同一类别）。这些结果验证了 CSA 模块的显著效果。

对于类别更多的语义分割任务（如 COCO-Object 数据集包含 81 个类别），零样本模型面临更大挑战。如图 6 所示，由于未考虑补丁级视觉标记间的语义相关性，MaskCLIP 的分割结果中出现了大量噪声（如第一个和第三个示例）。值得注意的是，这一问题无法通过简单引入额外优化或阈值策略解决 —— 这类策略可能导致模型将图像分割为单个或极少数类别，进而降低其对精细视觉特征的推理能力。此外，我们还观察到一些有趣现象：例如，在第四个示例中，我们的模型对鸟的分割掩码未包含其站立的围栏，而地面真值包含了围栏；在第六个示例中，左侧的运动型多用途汽车（SUV）在地面真值中被标注为 “公交车”，而我们的模型将其正确归类为 “汽车”。

![](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%27400%27%20height=%27256%27/%3e)![image](data:image/svg+xml;base64,PHN2ZyB3aWR0aD0iNDAwIiBoZWlnaHQ9IjI1NiIgdmVyc2lvbj0iMS4xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciPjwvc3ZnPg==)

（注：原文中参考文献 [1]-[76] 及各表（表 1 - 表 4）的具体数据内容已完整保留在翻译中，图表位置已标注，实际使用时需插入对应图表。）