# 摘要
* 情境学习（In-context learning）允许在测试时给定任务描述的情况下使模型适应新任务。在本文中，我们提出了一种生成模型，它能够从多模态提示中在上下文中学习视觉任务。给出视觉任务的文本描述(例如“左：输入图像，右：前景分割”)、几个输入-输出可视示例，或两者兼而有之，上下文中的模型学习如何为新的测试输入解决该问题。
* 我们在计算机视觉论文及其相关题注的新数据集上训练掩模生成转换器（masked generative transformer），以及带字幕的大规模图文数据集。在推理时，我们用文本和/或图像任务示例来提示模型，并让模型修补相应的输出。
* 我们发现，使用**文本条件化**和**调整数据集大小**来训练我们的模型，可以改善计算机视觉任务的上下文学习，在前景分割方面提高超过10%的AP，在单目标检测方面提高超过5%的AP，在着色方面几乎降低20%的LPIPS。
* 我们的经验结果表明，视觉和语言提示是相辅相成的，同时使用两者有利于获得更好的情景学习成绩。

# 1.Introduction
**情境学习（In-context learning，ICL）**，也被称为**少样本提示（few-shot prompting）**，是机器学习中一种令人兴奋的新范式，它允许模型适应新的下游任务，而不需要微调或改变模型的权重。在自然语言处理(NLP)中，ICL被认为是大型语言模型的一种新兴财富，它最早是在GPT-3的开创性论文中介绍的。少样本的提示通常包括`(输入、输出)对`的例子。大型语言模型的少样本表现已被证明在自然语言处理任务中取得了具有竞争力的结果，有时超过了先前最先进的微调方法。

在计算机视觉中，情境学习(ICL)的全部潜力还远未实现。为了使模型能够在测试期间执行情景学习，需要解决两个关键挑战。
* 首先，模型的体系结构应该设计成能够有效地处理来自各种视觉任务的提示。这意味着它应该能够接收任务指令和/或输入-输出示例作为模型的输入。
* 其次，需要一种不同的方法来训练这些模型。虽然在自然语言处理(NLP)中，ICL的出现是通过利用大规模的非精选数据来促进的，但在计算机视觉中，即使是在数十亿个非精选文本-图像对上训练的生成模型也无法达到类似的结果。[[ICL于CV与NLP的区别]]

实现计算机视觉任务的“测试时少样本提示”的一种可能方法是训练**多任务修复模型（multi-task inpainting model）**。例如，以前的方法采用完全监督的范式来训练一组预先确定的视觉任务的模型。然而，这条研究路线需要少量的手动注释，因此很难很好地扩展和概括到未经预习的视觉任务。Bar等人没有明确设计这些任务，采取了一种不同的无监督方法，提出从非结构化计算机视觉图形数据中学习，其中图像具有隐含的任务监督和网格状结构。然而，使用纯视觉提示(Bar等人，2022)存在模棱两可的问题，并且其描述特定视觉任务的能力受到限制。

为了缓解这些困难，我们建议通过使用由**像素和文本**组成的输入来提示的多模态ICL。直觉上，这两种模式可以协同工作，增强对世界及其复杂性的理解。例如，在交谈中，人们使用语言来交流思想，用视觉来感知面部表情和对话手势。对于提示，以文本为条件的视觉模型可以有效地描述指令，并减少歧义，而不需要多个高质量的视觉示例。

有了这种直觉，我们训练了一个名为 IMProv 的模型，在给定图像其余部分和字幕作为上下文的情况下，修补随机遮盖的区域。我们的训练不需要明确定义任务和每个任务的注释。为了证明更大的数据集可以促进多模态学习，我们从语义学者那里收集了一个新的图文对数据集，这个数据集比现有最大的计算机视觉图形数据集大三倍。我们通过对新构造的数据和LAION 400M(LAI)相结合的随机掩模图像进行修复来训练一个新的模型。在测试时，我们的模型展示了新兴的能力，例如视觉任务的零样本提示，例如，执行前景分割，只使用任务的文本描述，而没有任何图像示例。

在我们的模型中，我们探索了图像和文本提示的互换和组合的结果。我们发现，与过去的纯视觉方法(图1)相比，当使用这两种模式时，我们的模型实现了ICL性能的改善，前景分割的平均精度提高了10%以上，单目标检测的平均精度提高了4%以上，并且缩小了当前ICL方法与使用有监督的基类训练数据的最先进的单镜头训练方法之间40%以上的差距。除了视觉识别，IMProv还可以应用于一般的视觉任务，包括边缘估计、深度估计和条件图像合成，如图1所示。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507162112470.png)

*图1：基于修复的视觉多模态提示(IMProv)。顶部：我们的上下文中的模型学习通过使用可视输入-输出示例(A)、文本任务描述(B)或两者(C)用任务解决方案(以红色方块显示)修复遮罩区域来解决计算机视觉任务。下图：改进预测的例子。*

# 2.Prompting Inpainting Models Via Images And Text
我们首先在第2.1节中介绍我们的 IMProv 模型以及如何训练它，然后在第2.2节中，我们解释了使用视觉和文本提示来提示模型的方法。最后，在第2.3节中，我们描述了我们收集的图像和相关字幕的新数据集。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507180954777.png)

*图2：IMProv架构。在训练期间，输入图像被拼接、掩蔽，并与关联的字幕剪辑嵌入一起馈送到模型。对于每个掩码令牌，解码器在冻结的预先训练的VQGAN码本上输出分布。该模型在训练过程中引入了交叉熵损失。*
## 2.1.IMProv - Text Conditonal Inpainting Model
我们介绍了一种具有基于修复的视觉任务多通道提示能力的模型(IMIV)。它接收文本和掩码输入图像作为上下文，并输出重建图像。给定输入图像$x∈R^{H×W×3}$、二进制掩码$m∈\{0，1\}^{H×W}$和句子$t∈K×V$(其中V是词汇表，句子长度为K)，我们的修复模型 f 的目标是生成新的图像$y∈R^{H×W×3}$，其掩蔽区域根据输入图像上下文和句子填充：

$$y = f(x, m, t)\tag{1}$$

我们的模型 f 具有类似于**MAE-VQGAN**(Bar等人，2022)的编解码器结构，其中编码器和解码器是**Vision Transformers**(Dosovitski等人，2020)。与Bar等人形成对比的是，在每个self-attention层之后，我们在图像令牌和文本令牌之间增加一个cross-attention层，从而有效地让每个图像令牌关注文本令牌：

$$Z_i = \sum^n_{j=1}a_{ij}V_j \qquad a_{ij} =\frac{exp(K^T_j Q_i)}{\sum^n_{m=1} exp(K_m, Q_i)} \tag{2}$$

其中，V是文本令牌值（values）的集合，K是文本令牌密钥（keys）的集合，Q是图像令牌查询（queries）的集合。所得到的输出序列 Z 表示与文本标记最相关的附属图像特征。

**训练**。为了训练模型，输入图像 x 被分割成多个块（patches），并通过丢弃固定百分比的块(在我们的实验中为75%)来随机屏蔽。类似地，输入文本句子被标记化（tokenized），并且每个标记符（token）被映射到其对应的 CLIP 嵌入（embedding）。在给定非掩蔽块的子集和文本标记图像的情况下，然后训练该模型以预测对应于掩蔽块的视觉标记。用 模型预测和每个遮蔽块对应视觉标记之间的交叉熵损失来训练该模型。使用预先训练的VQGAN编码器将输入图像映射到视觉标记索引，获得基本真实视觉标记。形式上，我们的文本条件MAE-VQGAN为分布 $p(z_i|x，m，t)$ 建模，其中$z_i$ 是VQGAN词汇表中对应于第 i 个图像块的可视标记。

## 2.2.Mutimodal Prompt
在推理时，可以通过文本、通过可视提示或通过两者相结合来提示训练的模型。为了通过视觉提示来提示模型，我们采用了Bar等人相同的任务公式。我们形成了由任务输入-输出（例如输入图像和它们的分割掩膜）组成的网格状图像和新的查询图像，并应用该模型来内嵌该查询的相应结果。为了通过文本来提示模型，我们向 f 提供对任务的描述(例如“左：输入图像，右：前景/背景分割结果”)。

形式上，设$S=\{(x_i，y_i)\}^n_{i=1}$为输入输出例集，其中xi为图像，yi为相应的视觉任务输出，t为文本任务描述，xq为查询图像。我们引入一个排列函数g1，它将S和xq排列成一个网格(可视提示)，用$x_{vp}$表示，并为修复函数提供一个掩码m：

$$x_{vp},m = g_1(S, x_q)\tag{3}$$

类似地，我们有一个对应的排列函数g2，它生成一个文本提示，用于指示模型如何在给定属性(如任务名称、位置详细信息和图像类名称)的情况下修复图像：

$$t = g_2(task, loc, class)\tag{4}$$

例如，对于飞机的图像分割任务，输出可以是“左：飞机的输入图像，右：对应的图像分割”。对于更多的例子，参见表1。然后，在给定视觉提示和任务描述的情况下，应用模型 f 来重建掩蔽区域m：

$$y = f(x_{vp}, m, t)$$

当任务描述为空字符串时，不给出文本提示，模型只能从 S 的例子中推断任务。当 S 为空集，并给出文本提示时，模型仅依靠文本指令进行零样本补全。

## 2.3.Image-text Dataset For CV
IMProv绘制的网格状视觉提示图像与自然图像具有不同的分布。与图像配对的视觉任务描述(例如：左：输入，右：分割掩码)并不经常出现在广泛使用的语言和视觉数据集中。因此，由于分布偏移，在这些数据集上训练的模型将难以成功完成修复任务。为了缩小这个领域的差距，我们从计算机视觉论文中提取，收集了一个新的图形数据集，与它们关联的字幕配对。

我们的数据集——语义学者计算机视觉数据集(S2CV)，是从《语义学者》（Semantic
Scholar）网站上发表的计算机视觉论文中收集的。该网站收录了从2010年到2022年的40个会议和期刊的论文。我们从网站上的每篇论文中提取了配对的图形和它们的说明，结果是1,417,398对。然后我们过滤掉不包括图像的图形数据(例如损失曲线的曲线图)。最后，过滤后的S2CV数据集包括268,118个带字幕的图形，并且比现有最大的图形数据集，即计算机视觉图形数据集(CVF；Bar等人)大3倍。(2022年))。参见表2中的比较，数据集数据表中关于S2CV的完整详细信息(Gebru等人，2021年)，并在图7中提供了示例。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507181003429.png)

我们还通过重复其数据收集过程并提取数据集中图形的字幕来扩展现有的CVF。这将产生78,227个图像-文本对。这个数据集CCVF(CaptionedCVF)在我们的实验中用作基线。

# 3.Experiments and Result
我们以VIT-L为骨干，结合我们的CCVF、S2CV数据集和LAION400M(LAI)训练了一个IMProv。在训练过程中，我们通过随机选择一半来自LAION-400M数据集的数据，另一半来自CCVF和S2CV，确保模型从一组不同的类似图形的图像中学习。

我们在各种计算机视觉任务上评估IMProv。默认情况下，我们的视觉提示由一个2×2的网格组成，其中右下角的四分之一被遮蔽，顶行包含输入-输出示例，左下角的图像表示查询。视觉示例和文本提示是根据任务定义的(见第3.2节)。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507181010102.png)
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507181011340.png)

*图3：多模态提示预测实例。提供的示例文本提示与呈现的视觉提示一起出现在它们的下方。对于每个提示，结果都以红色标记。有关更多结果，请参阅补充材料。*

## 3.1.Experiment Details
在培训期间，我们使用图像及其相关字幕。我们跟随He等人调整大小的裁剪和翻转的增加。(2021年)，并培训224×224作物。我们使用AdamW(Loshcheov&Hutter，2017)优化器，学习率为2E−4，权重衰减率为0.05%。我们在一台机器上训练我们的模型，使用8个A100 GPU，批次大小为2048次，迭代15万次。我们的学习率时间表包括2k个线性热身步骤，然后是余弦学习率衰减。我们使用预训练的冷冻剪辑VIT-L/14模型作为文本编码器，并使用预词汇量为1024的训练的VQGAN码本和×16的空间降维。在训练过程中，我们以0.1%的概率丢弃了文本条件（text condition）。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507181029820.png)

*图4：详细的文字提示IMProv。我们尝试使用具有不同细节的文本提示，例如，从没有文本到包含任务、特定位置和对象类名称的说明。请参见表1中的全文提示示例。有关更多结果，请参阅补充材料。*

## 3.2.Downstream CV Tasks
接下来，我们将包括对广泛的计算机视觉任务的Improv的评估结果。当在CCVF/S2CV和LAION 400M(LAI)上进行培训时，与仅使用视觉的ICL方法相比，Improv在广泛的计算机视觉下游任务中显著提高了ICL的性能。

**前景分割**。在本任务中，目标是将查询图像分割为前景和背景两类。输入-输出示例是具有其对应的二进制分割掩码的随机图像(例如，背景为黑色，前景为白色)。我们将文本提示定义为：`“Left - input image, right - Black and white foreground-background segmentation of {class}”`，其中，{class}是前景对象的类，用PASCAL-5I注释。我们遵循Bar等人的评估方案(2022)和在Pascal-5i数据集的四个拆分上的测试改进(Shaban等人，2017年)。结果如表3所示。

**目标检测**。与前景分割的任务类似，我们的目标是对查询图像中存在的对象执行二进制分割。然而，这项任务更具挑战性，因为输入-输出示例包含一个矩形掩模，该掩模派生自边界框，与精细的详细分段掩模相比，边界框的精度较低。我们将文本提示定义为：`“Left - input image, right - Black and white foreground background segmentation of {class} of rectangle shape”`，其中，{class}是前景对象的类。我们使用的是Pascal VOC 2012数据集(Everingham等人，2015)，它由图像及其相关的检测框组成。我们的结果在表3中以并上的平均交集(MIUU)度量的形式报告。

**彩色化**。其目标是将灰度图像映射到彩色图像。该示例对是灰度图像和对应的彩色图像。我们将文本提示定义为：`“Colorization results: Left - input image, Right - Colorized image of class”`，其中{class}是图像中存在的对象的类。我们从ImageNet(Russakovsky et al.，2015)验证集中随机抽取了1000个样本对和图像查询，并将它们转换为灰度，以获得每个图像的灰度和彩色版本。MSE和LPIPS(Zhang等人，2018)的结果如表3所示。

**其他任务**。我们在Wang等人创建的数据集上对我们的模型进行了评估。(2023c)，它包括大约310k个图像-字幕对，这些图像-字幕对通过使用最先进的预先训练的模型自动注释，用于广泛的视觉任务。具体地说，每个图像都用从Midas(Ranftl等人，2022)获得的深度和法线贴图、从Uniform(Li等人，2022)获得的分割贴图以及HED(Xie&Tu，2015)检测到的对象边界贴图进行注释。对于每个视觉任务X，我们在两个任务上对我们的模型进行评估--X到图像和图像到X。由于每个任务都有不同的评估指标，并且我们的模型生成图像输出，因此我们计算LPIPS将生成的图像与任务的渲染注释进行比较(Zhang等人，2018年)，以此来简化评估。我们在表5中报告结果，并在图3中绘制定性结果。

*表3：计算机视觉任务的视觉提示与多模式提示结果的比较。对于前景分割（Foreground Segmentation）和单目标检测（Single Object Detection），我们报告MIU值。对于着色（Colorization），我们报告MSE和LPIPS。训练数据集显示在括号中。*

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202507181020786.png)

# 4.Analysis
**数据集消融**。我们报告了我们的结果，并将它们与表3中的先前工作进行了比较。在LAION-400M和我们的S2CV数据集的组合上训练的∼比仅在CVF数据集上训练的先前工作(Bar等人，2022年)在MIU12点以上。它展示了IMProv可以从对更多数量的未标记图像进行培训中受益。

**文本提示消融**。我们尝试使用与查询图像和任务具有不同相关性的文本和视觉提示。对于视觉提示，我们使用了三种不同的检索策略来选择输入输出样本：(1)随机类别，其中前景对象的类别被随机选择；(2)相同类别，其中，相同类别的随机样本对被随机选择；(3)样本是通过最近邻从具有相同前景对象类别的所有图像中选择的，使用Zhang等人的模型。(2023年)。我们在Pascal5i上评估了我们的即兴模型，有没有包含任务、位置和类名的文本提示。

首先，我们与Bar等人(2022)进行了比较，在表4中的(1)随机班级视觉提示设置和报告结果下。在这种设置中，视觉提示描述了任务(例如，分段)，但不是来自同一类别(表3中的设置)，或者像Zhang等人那样通过最近邻选择。(2023年)。使用非精选的视觉提示是最现实的，因为找到一个完全一致的视觉示例可能和解决原始输入一样困难。结果表明，当使用合理的非刻划视觉提示时，对文本的条件反射使平均MIU值提高了3个点。此外，在LAION和我们的CCVF数据集的组合上进行的IMProv训练进一步将MIU10点提高。

**数据集消融实验**：我们将研究结果与之前的研究成果在表3中进行了对比。在LAION-400M和我们的S2CV数据集上训练的IMProv，与仅在CVF数据集上训练的先前研究（Bar等人，2022）相比，在mIOU（平均交并比）上高出约12个点。这表明IMProv能够从额外的无标记图像训练中获益。

**文本提示消融实验**：我们对与查询图像和任务具有不同相关性的文本提示和视觉提示进行了实验。对于视觉提示，我们采用三种不同的检索策略来选择输入-输出示例：
* （1）随机类别，即随机选择一个前景对象类别为随机类别的示例对；
* （2）相同类别，即随机选择一个前景对象类别相同的示例对；
* （3）使用Zhang等人（2023）提出的模型，从所有具有相同前景对象类别的图像中通过最近邻方法选择示例。
我们在Pascal 5i上评估了IMProv模型在有无包含任务、位置和类别名称的文本提示时的性能。
* 首先，我们在（1）随机类别视觉提示设置下与Bar等人（2022）的研究进行了对比，并在表4中报告了结果。在这种设置下，视觉提示描述了任务（例如分割），但并非来自相同类别（表3中的设置），也不是像Zhang等人（2023）那样通过最近邻方法选择的。使用非精选的视觉提示是最符合实际情况的，因为找到一个完全匹配的视觉示例可能和解决原始输入一样困难。结果表明，在使用合理的非精选视觉提示时，以文本为条件可使平均mIoU（平均交并比）提高3个点。
* 此外，在LAION和我们的CCVF数据集上训练的IMProv进一步将mIoU提高了10个点。 在图5中，我们绘制了不同文本提示下的结果。我们发现文本提示对模型的性能有很大影响（见图5）。为了更深入地了解文本提示的效果，我们在图4中绘制了文本提示和视觉提示之间的关系。结果表明，添加文本提示可以改善任何类型视觉提示的结果，无论是相关性最低的随机类别示例，还是相关性最高的最近邻示例。
* 此外，我们发现通过使用文本，可以在视觉示例质量较低（使用相同类别示例而非最近邻示例（Zhang等人，2023））的情况下达到相近的性能。同样，高质量的视觉示例在所有测试的文本提示下都能改善结果。有趣的是，结果表明这两种模态之间存在一种权衡——高质量的文本提示可以减少对精心选择的视觉提示的需求，反之亦然。 
* 此外，如图6所示，当文本提示与视觉示例不一致时，模型可能会遵循更明确的文本指令。更多关于不同视觉提示和文本提示组合的结果，请参见补充材料。 

**结构化数据是否能改善上下文学习？**：我们的方法的核心见解是在完全无监督的方式下在非结构化数据上训练IMProv，而无需对图像或文本进行解析。在这里，我们进行了在完全监督的方式下在额外的结构化数据上训练IMProv的实验。我们使用了Brooks等人（2022）的数据集，该数据集包含310k个输入-输出图像编辑对及其相应的描述。在训练时，我们将它们作为随机对使用。我们将它们嵌入到一个网格结构中，类似于我们在测试时使用的结构。我们构建的用于训练的网格图像由1×2和2×2的图像组成，通过为数据集中的每个标题随机选择1个或2个输入-输出示例。 我们在一组预留的视觉任务上测试了我们的模型。如表5所示，我们发现仅在结构化监督数据上训练会导致在测试任务上的泛化能力和上下文学习性能较差。在非结构化的S2CV和LAION-400M数据上，再加上结构化数据进行训练，与我们的基础模型相比，在测试任务上的上下文学习结果有所改善。

# 5.Related Work
**NLP中的prompt**。提示语言模型解决特定任务的能力，也称为ICL，是最近发现的经过大量训练的生成语言模型的特性文本语料库(Brown等人，2020年；Touvron等人，2023；Chowdhery等人，2022；Bubeck等人，2023)。Brown等人的研究成果(2020)的研究表明，现有的NLP任务可以用非结构化文本来描述，然后输入一个大型语言模型，以完成缺失的部分，而不需要任何微调(Radford等人，2019年；Brown等人，2020年)。最近出现了不同的提示方法，包括**提示工程**(Brown等人，2020；Lu等人，2021)、**提示整合**(酱等人，2020)、**提示前缀调整**(Li&Leung，2021；Lester等人，2021)和**思想链提示**(魏等人，2022)。Flamingo(Alayrac等人，2022年)模型扩展了仅语言模型，并以图像和文本为条件。我们的方法不同，因为**我们的模型输出像素，而不是文本。因此，它适合于解决各种可以在像素空间中表示的计算机视觉任务**，如语义分割或图像彩色化。

**视觉提示**。最近，多篇论文提出了视觉提示计算机视觉模型的方法(Bahng等人，2022；Jia等人，2022；Bar等人，2022)。Bahng等人(2022)提出将噪声张量添加到输入图像中，以使模型适应不同的任务，而Jia等人(2022)提出将学习的符号附加到视觉transformer(Dosovitski等人，2020)，这从NLP中的前缀调整中获得了动机(Li&Leung，2021)。这两种方法都是针对受监督的数据进行培训的，因此难以扩展和推广到新任务。Bar等人(2022)采取了一种不同的方法，从计算机视觉纸质数字中训练非结构化作物。该方法通过创建一个包含输入输出示例和新图像输入的图像网格，**将视觉提示视为一项图像修复任务**。修复模型的目标是以与输入一致的方式完成输出。我们遵循与(Bar等人，2022)中类似的视觉提示的定义，然而，我们建议也以文本输入为条件来限制模型，这可能有助于解决任务描述中的歧义，并可以更有效地引导视觉模型执行期望的任务。

**图像修复与图像合成**。早期的图像修复方法依赖于输入图像本身进行修复(EFros&Leung，1999；Bertalmio等人，2000；Criminisi等人，2004；Barnes等人，2009)，而最近的作品利用图像数据集来实现这一目的(Hays&EFros，2007；Pathak等人，2016；Yang等人，2017；Liu等人，2018b；a)。最近，扩散模型在图像修复和图像合成方面取得了巨大的成功(Ramesh等人，2022；Rombach等人，2021)，以及其他流行的基于变压器的方法(Chen等人，2020；Yu等人，2021b；Esser等人，2021；Yu等人，2021a；Chang等人，2022)。这些方法中很少依赖于离散的潜在码本，这些码本导致在可能的补全上进行分布(Van Den Oord等人，2017；Ramesh等人，2021；Esser等人，2021；Yu等人，2021A；Chang等人，2022)。例如，Esser等人。(2021)；Yu等人(2021A)提出在码本表示上使用自回归模型来合成图像，而Chang等人(2022)应用了令牌的迭代并行解码。很少有方法还支持具有文本条件反射的图像合成--例如，MUSE(Chang等人，2023)是基于转换器的模型，其将来自图像嵌入(VQGAN(Esser等人，2021))的交叉注意应用于从预训练模型(例如T5(Raffel等人，2020))提取的文本嵌入以对文本进行条件反射。我们的模型在概念上类似于MUSE(Chang等人，2023)，然而，我们专注于修复网格状的视觉提示，需要跨多个子图像和输入文本进行推理。

**少样本学习**。在这种设置中，算法在基类的标记数据集上进行训练，它应该从基类的标签数据集转移到仅给出少数训练样本(如10个或30个)的新类的集合(Nguyen&Todorovic，2019；Kang等人，2019；Liu等人，2020；Wang等人，2020；Yang等人，2020；Tian等人，2020；Zhang等人，2021；Bar等人，2021)。与少数几次尝试的方法不同，在这里，我们不假设可以访问大量的基类训练集，我们的体系结构也不是特定于任务的。我们的方法很少成功，只是因为提示符的视觉部分通常包含一两个任务示例。

# 6.Discussion
提出了一种修复模型的多通道提示方法。要在这类模型中释放情景学习能力，我们必须收集特定的图形数据集以及相关的字幕。为了进一步扩大这一方法的规模，我们认为，应在培训阶段纳入其他非结构化数据来源--视觉、文本和多模式。为了了解这种数据收集工作的可行性，必须能够预测和量化不同数据集大小和类型对基于它们训练的模型的下游上下文中学习能力的影响。我们计划在未来的工作中对其进行研究。

