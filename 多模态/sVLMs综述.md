# 
# 2 小型视觉语言模型的演变
## 2.1 历史概述
### 2.1.1 CLIP

* 原理：用自然语言作为监督信号，通过**对比学习**在共享嵌入空间中最小化不匹配项，来对齐图像和文本
* 优势：实现了显著的**零样本**性能
* 局限：其零样本性能仍然低于最先进的模型。计算要求极高。数据要求极大。在细粒度分类、抽象推理和分布外泛化方面存在困难。
* 框架图：

### 2.1.2 ViLT

* 原理：采用仅transformer的架构，将图像划分为32×32的块，进行线性投影，并与文本嵌入融合以实现跨模态交互。该方法消除了CNNs和区域特定监督，简化了视觉处理。
* 优势：实现了显著的**零样本**性能
* 局限：缺乏细粒度。
* 框架图：

ViLT（视觉与语言Transformer）[45]提出了一种视觉与语言预训练（VLP）方法，该方法消除了卷积神经网络（CNNs）[31]和区域特定监督，简化了视觉处理。它采用仅transformer的架构，将图像划分为32×32的块，进行线性投影，并与文本嵌入融合以实现跨模态交互。在MSCOCO、Visual Genome、SBU Captions和Google Conceptual Captions等数据集上预训练后，ViLT在视觉问答（VQAv2）[29]、自然语言视觉推理（NLVR2）[82]和图像-文本检索等任务上展现出有竞争力的性能。然而，其简化的视觉嵌入缺乏细粒度物体表示，由于缺乏显式的目标检测，导致在VQA中性能有所折衷。对块投影的依赖限制了详细的语义理解，影响了深度视觉推理。通过 Mask 块预测（MPP）[93]尝试增强表示的努力并未奏效，突显了视觉 Mask 中的优化挑战。虽然ViLT效率高，但将其扩展到更大模型需要大量的计算资源和数据。这些局限性强调了移除基于区域的嵌入的权衡，暗示了未来在细粒度视觉表示和增强预训练策略方面的改进。

VirTex [21] 提出了一种预训练方法，该方法使用语义丰富的标题学习视觉表示，为依赖于大型 Token 数据集的传统监督和无监督方法提供了一种数据高效的选择。其架构由一个 ResNet-50 [33] 视觉 Backbone 和一个基于双向 Transformer 的文本 Head 组成，共同在 COCO Captions 数据集 [54] 中的图像-标题对上进行训练以生成标题。预训练后，文本 Head 被丢弃，视觉 Backbone 被迁移到下游任务，如图像分类、目标检测和实例分割。然而，VirTex 存在几个局限性。它依赖于高质量的标题标注，这限制了其在大型、嘈杂的网页数据集上的可扩展性，在这些数据集中此类标注不可用。此外，虽然测试了 Mask 语言模型（MLM）[93]，但发现其样本效率低下且收敛速度慢，导致计算成本高昂。该模型在更大的视觉 Backbone 上也表现出优化挑战，因为 ResNet-101 并没有在 ResNet-50 上持续提高结果。此外，对于下游任务还需要微调，这在资源受限的环境中增加了计算开销。虽然 VirTex 在表示学习方面表现出色，但其生成的标题与最先进的标题模型相比并不理想，突显了在标题任务性能上的权衡。

FLAVA[80]是一种视觉和语言对齐模型，旨在统一Transformer架构中处理视觉、语言和多模态任务。它包括基于ViT-B/16架构的图像编码器，用于处理视觉输入，相同设计的文本编码器用于文本表示，以及一个多模态编码器，用于对齐和融合来自两种模态的信息。该模型在来自公开来源的7000万图像-文本对数据集上进行了预训练，确保了可访问性和可重复性。然而，FLAVA存在一些局限性。尽管努力提高多样性，但使用公共数据集引入了固有的数据集偏差，可能会影响泛化能力。较小的预训练数据集，与模型SimVLM（18亿）[92]相比显著较小，限制了在大规模应用中的可扩展性。此外，由于联合训练单模态和多模态任务，涉及到对比学习、 Mask 多模态建模和图像-文本匹配等复杂目标，因此出现了优化挑战。由于缺乏结构化的课程来指导顺序任务学习，轮询采样进一步复杂化了学习过程。因此，FLAVA在仅视觉的任务中可能表现出潜在的低性能，因为它没有在大规模视觉数据上得到充分的训练。

SimVLM [92] 通过采用单个前缀语言建模（PrefixLM）[22]目标，引入了一种简化的视觉-语言预训练方法，该目标统一了双向上下文表示和自回归文本生成。其架构基于Transformer编码器-解码器框架，原始图像块通过受ResNet [33]启发的卷积阶段进行处理，文本输入使用位置编码进行 Token 化。该模型在大规模弱监督数据集上端到端训练，如ALIGN [84]，不依赖于目标检测模块或辅助损失。然而，其对噪声网络爬取数据的依赖可能导致在需要精细理解的任务中性能不佳。缺乏显式的区域级推理限制了其在捕捉精确空间关系和物体交互方面的有效性。高计算需求，包括使用TPU v3芯片，带来了可扩展性的挑战。此外，对弱监督的依赖限制了模型学习详细视觉-语言关联的能力。在开放式的VQA [40]任务中，SimVLM由于预训练数据的噪声，在没有额外微调的情况下难以生成有意义的响应。虽然展示了强大的零样本能力，但与全监督模型相比，在复杂推理任务中仍有所不足。此外，其对多模态学习的优化导致在语言理解和图像分类等单模态任务中性能权衡。

BLIP（Bootstrapping Language-Image Pre-training）[49]，通过采用一种新颖的预训练策略，有效地整合文本和图像数据，为统一的视觉-语言理解和生成做出了显著的技术贡献。其架构包括一个基于视觉Transformer（ViT）[23]的视觉编码器和一个基于Transformer [91]的文本编码器-解码器，协同工作以对齐和生成多模态表示。BLIP引入了两个训练目标：基于图像的文本生成和图像-文本对比学习，使得在噪声网络数据上能够进行稳健的预训练。该模型在包括图像描述、视觉问答和跨模态检索在内的多种任务上进行了评估，展现了最先进的性能。它利用了如概念描述（Conceptual Captions）[12]和MS COCO数据集[54]等大规模数据集，并加入了网络爬取的图像-文本对进行预训练。BLIP的局限性既来自模型也来自数据方面。在模型方面，基于编码器的方法难以直接应用于文本生成任务，如图像描述，而编码器-解码器模型在理解型任务，如图像-文本检索方面面临挑战。从数据角度来看，BLIP与其他视觉-语言预训练方法一样，严重依赖于噪声网络数据，这对视觉-语言学习来说并不理想。这些噪声网络文本往往无法准确描述图像的视觉内容，尽管数据集规模扩大带来了学习上的收益，但仍然导致学习稳健的视觉-语言对齐效率低下。

Flamingo [4] 是一种视觉语言模型，它将预训练的仅视觉模型和仅语言模型连接起来，以处理交织的视觉和文本数据序列，从而在多模态任务上实现最先进的少样本学习 [55]。其架构集成了冻结的NFNet-F6 [9] 视觉编码器与Perceiver Resampler [41]，将可变长度的视觉特征转换为固定 Token ，这些 Token 随后由一个冻结的大语言模型处理，该模型增加了新的GATED XATTN-DENSE层。这些创新使得跨模态集成高效，同时保留了预训练语言模型的能力。Flamingo在包括M3W数据集 [115]（包含4300万个交织文本和图像的网页）、ALIGN（18亿个图像-文本对） [42] 和LTIP（3.12亿个长文本-图像对） [24] 在内的多种数据集上进行了训练，使用了新颖的多目标损失函数。该模型在16个多模态基准测试中进行了评估，在少样本学习方面取得了新的最先进成果，同时使用的任务特定数据比先前模型显著减少。

尽管Flamingo具有优势，但它像基于预训练语言模型（LM）构建的其他模型一样，继承了其基础的一些固有弱点。例如，虽然LM先验通常有益，但有时会导致幻觉或不切实际的预测。此外，LM在泛化到训练期间观察到的序列长度之外时表现不佳，并且在训练过程中样本效率较低。解决这些限制可以显著提高Flamingo等视觉语言模型（VLM）的有效性和可靠性。Flamingo的另一个局限性在于其分类性能，这低于最先进的对比模型。对比模型在直接优化此类目标（其中分类是子集）的任务上表现出色，如文本-图像检索。

相比之下，Flamingo旨在解决更广泛的开放性任务。开发结合对比模型的优势和Flamingo的灵活性的统一方法，是未来研究的重要方向。Flamingo的关键优势之一，即上下文学习，也带来了挑战。这种方法允许通过最小的超参数调整进行简单的部署和推理，并在低数据场景中证明了其有效性，尤其是在只有几十个示例的情况下。然而，其对演示设置的敏感性、高推理计算成本以及随着示例数量超过低数据范围时的性能下降是明显的缺点。将上下文学习与基于梯度的少样本方法相结合，可以提供更平衡和有效的解决方案。