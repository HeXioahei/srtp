> [美团/浙大等联合提出MobileVLM | 骁龙888就可以实时运行的多模态大模型，边缘端多模态大模型之战打响了！！！ - 知乎](https://zhuanlan.zhihu.com/p/675392936)

# 主要贡献

1. 提出了**MobileVLM**：
	1. 针对**移动场景**的多模态视觉语言模型的全栈式重构。
	2. 详细、可复现且强大的视觉语言模型。
	3. 使用受控和开源数据集。
	4. 一组高性能的基础语言模型和多模态模型。  
    
2. 对**视觉编码器**的设计进行了广泛的调优研究，并系统地评估了VLM在各种训练范式、输入分辨率和模型大小上的性能敏感性。  
    
3. 设计了一个高效的视觉和文本特征之间的**投影器**，更好地对齐多模态特征，同时减少了推理预算。  
    
4. 专门针对**移动、低功耗设备**进行了优化。  
    
5. 尽管作者主要**关注边缘场景**，但在实际中可以应用于许多任务。

# 相关工作

## ViT
当前视觉感知的主流backbone。

## LLM
虽然大的模型可以提高性能，但是目前越来越追求更轻量化更小的模型。

## VLMs
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505242321831.png)

**架构选择**：通常采用ViT，而预训练策略很多样。CLIP式的自然语言监督预训练更受欢迎。

**以数据为中心**：几乎每个新的模型都自带一个新的数据集。构建方式越来越多样。

## Model Compression for LLMs
LLM太大了，需要模型压缩技术来解决资源消耗大和处理速度慢两大问题，同时又不降低性能。这些技术包括但不局限于模型剪枝，量化，知识蒸馏和低秩分解。此外，LLM部署工具也逐渐繁荣。

## VLM Benchmarks
* POPE提供了评估VLMs中幻觉的基准，将评估转换为二分类任务，要求VLMs回答目标是否存在。
* GQA主要关注VLMs在现实世界推理、场景理解和组合性问题回答方面的能力。
* TextVQA包括与图像中的文本相关的问题，评估模型的OCR和推理能力。
* ScienceQA包括覆盖科学主题的多模态多项选择题。
* MME测量VLMs的感知和认知能力，它包括总共14个从粗粒度到细粒度的子任务。MMBench是一个精心构建的多模态数据集，涵盖了20个细粒度的技能，并采用循环评估策略，其中包含Chat-GPT

## Embodied AI 
作者的工作与具身人工智能密切相关。作为人工通用智能的一个核心目标，具身人工智能旨在构建以自我为中心的智能系统，能够通过感知、推理和规划能力与周围环境进行交互。

# MobileVLM
## 总体架构
![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505242357807.png)

## 核心组件

### Visual Encoder
* Pointwise：相当于 1 * 1 * 3 的卷积核。
* Deothwise：

