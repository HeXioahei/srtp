# DeCLIP：面向开放词汇密集感知的解耦学习

王俊杰 ¹ 陈斌 ²,³,* 李玉林 ¹ 康斌 ³ 陈一驰 ³ 田卓韬 ¹,*¹ 哈尔滨工业大学（深圳）计算机科学与技术学院 ² 哈尔滨工业大学（深圳）人工智能国际研究院 ³ 中国科学院大学

## 摘要

密集视觉预测任务受限于对预定义类别的依赖，这使其在视觉概念无界的真实场景中适用性受限。尽管 CLIP 等视觉 - 语言模型（VLMs）在开放词汇任务中展现出潜力，但由于局部特征表示存在局限，将其直接应用于密集预测往往会导致性能欠佳。本研究发现，==CLIP 的图像令牌难以有效聚合空间或语义相关区域的信息==，导致特征==缺乏局部区分性和空间一致性==。为解决该问题，本文提出**DeCLIP**—— 一种新型框架，通过==解耦 CLIP 的自注意力模块==，==分别获取 “内容”（content）和 “上下文”（context）特征==，以增强 CLIP 性能。其中，==“内容” 特征通过与图像裁剪块表示对齐，提升局部区分性==；==“上下文” 特征则在 DINO 等视觉基础模型（VFMs）的引导下，学习保留空间相关性==。大量实验表明，在目标检测、语义分割等多个开放词汇密集预测任务中，DeCLIP 显著优于现有方法。代码已开源至：[https://github.com/xiaomoguhz/DeCLIP](https://github.com/xiaomoguhz/DeCLIP)。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510081515691.png)

## 1. 引言

在深度学习时代，目标检测 [44,55]、图像分割 [12,57] 等密集预测任务已取得快速发展并得到广泛应用。然而，传统方法 [7,40,91] 仅能识别固定的预定义类别，这一限制使其在视觉概念几乎无界的真实场景中难以实际应用。因此，旨在通过文本描述检测和分割任意类别目标的开放词汇方法 [14,67,70,82] 受到越来越多的关注。

依托 CLIP [52] 等在图像 - 文本对上预训练的视觉 - 语言模型（VLMs）[13,41,52,61] 的成功，研究者开始将这类模型用于开放词汇密集预测任务。在相关研究 [8,65,67-69,84] 中，迁移学习方法 [11,30,37,65,68,78] 表现突出：此类方法将 VLM 的图像编码器用作特征提取器，==仅训练轻量级的任务特定组件==。尽管 VLM 因预训练充分，作为特征提取器具有显著优势，但将这些图像级模型直接应用于密集预测，常出现==领域偏移问题== [68,70]。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510081516769.png)

**CLIP 为何在密集感知中表现不佳？** 为评估 VLM 在密集感知中的局限性，我们分析了 CLIP 不同层的注意力图（图 3 (a)）。实验发现，CLIP 的 ==[CLS] 令牌可能会干扰其他图像令牌间的相关性==，导致密集预测任务性能欠佳。具体而言，在较深层（第 9 层之后），[CLS] 令牌会将注意力从图像中的主要目标转移到某些背景令牌上（图 3 (a) 第一行亮点所示）；此外，图像令牌（图 3 (a) 第二、三行）也表现出与 [CLS] 令牌类似的行为 —— 无论自身位置如何，都会对某些背景令牌投入高注意力。

这一观察揭示了 ==CLIP 在密集预测中表现不佳的原因：其图像令牌无法聚合空间或语义相关区域的信息，导致密集特征缺乏局部区分性和空间一致性==。如图 2 (a) 所示，在 COCO 数据集 [43] 上直接使用 CLIP 特征，开放词汇区域分类和语义分割的性能相对较差。

为解决该问题，直观思路是通过微调增强 CLIP 的局部表示，但在统一架构中同时优化局部特征空间相关性和视觉 - 语言语义对齐，成为新的挑战。因此，能否在统一架构中解耦 CLIP 的特征，并对不同特征施加独立的引导约束，以获取多样化特征？

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510081517010.png)

**本文方案**：为应对上述挑战，本文提出 DeCLIP—— 一种通用的==无监督微调方法==，旨在==同时增强 CLIP 局部特征的区分性和空间一致性==。其核心思想是==解耦 CLIP 的自注意力模块，并分别从不同的教师模型中学习==。

具体而言，DeCLIP 将自注意力模块中的特征解耦为 ==“内容” 和 “上下文”== 两个组件：==负责局部区分性的 “内容” 特征，通过将池化后的区域特征与其对应的图像裁剪块 [CLS] 表示对齐进行微调==；==负责空间一致性的 “上下文” 特征，则从视觉基础模型（VFMs）生成的特征相关性中学习==。这种解耦蒸馏设计有效缓解了优化冲突，提升了 CLIP 在下游开放词汇密集预测任务中的泛化能力。如图 2 所示，DeCLIP 在局部区分性和空间一致性上均显著优于 CLIP。

综上，本文的贡献如下：

1. 对 CLIP 的分析表明，其在开放词汇密集预测中的局限性源于图像令牌无法有效聚合空间或语义相关区域的信息；
2. 提出 DeCLIP—— 一种简洁高效的无监督微调框架，通过解耦特征增强策略，提升 CLIP 局部特征的区分性和空间一致性；
3. 大量实验表明，DeCLIP 可有效应用于开放词汇目标检测、语义分割等主流密集预测任务。如图 1 所示，在多个基准数据集上，DeCLIP 均优于现有最优方法（SOTA），在所有评估任务领域均实现了更优的性能指标。

## 2. 背景与动机

本节中，2.1 节简要概述本研究相关的基础概念，2.2 节阐述重要发现 —— 这些发现为本文方法的提出提供了关键思路。

### 2.1. 基础知识

**对比语言 - 图像预训练（CLIP）**[52] 由两个编码器构成：一个用于图像，一个用于文本。CLIP 的视觉编码器可采用 CNN 系列 [27,45] 或 ViT [19] 架构，文本编码器则为 Transformer [62]。本文聚焦于采用 ViT 架构的 CLIP 模型，该模型通过 [CLS] 令牌表示图像的整体特征。CLIP 通过最大化匹配图像 - 文本对的 [CLS] 令牌与文本特征间的余弦相似度、最小化不匹配对的相似度，实现视觉 - 语言对齐学习。

#### CLIP 的密集特征提取

基于 ViT 的 CLIP 由一系列堆叠的注意力块组成（例如，CLIP 的 ViT-B 版本包含 12 个注意力块）。设最后一个注意力块的输入为 $X=\{x_0, x_1, \cdots, x_{h \times w}\}$ ，其中 $x_i \in \mathbb{R}^{1 \times D}$ ，则该注意力块的计算过程可表示为：$$Q=\text{Proj}_q(X),\ K=\text{Proj}_k(X),\ V=\text{Proj}_v(X) \tag{1}$$$$
Y=X+\text{Proj}\left(\text{Attn}_{qk} \cdot V\right) \tag{2}$$$$Z=Y+\text{FFN}(Y) \tag{3}$$其中，Q、K、V 分别表示查询（query）、键（key）、值（value）嵌入；Proj 表示投影层；$\text{Attn}_{qk}=\text{SoftMax}(QK^\top/\sqrt{d})$ 表示自注意力过程（d 为每个注意力头的维度）；FFN 表示前馈网络。为简洁起见，本文省略了归一化操作。

经过最后一个注意力块后，$Z[0]$ 表示全局 [CLS] 令牌，剩余的图像块嵌入 $Z[1:h \times w]$ 可重塑为密集特征表示 $X_{\text{dense}} \in \mathbb{R}^{C \times H \times W}$（注：为简洁起见，此处省略了最终的视觉 - 语言投影层）。

#### CLIP 适配密集预测任务的方法

已有研究尝试通过微调策略缓解 CLIP 应用于密集预测时的领域偏移问题，这些方法主要分为两类：

- **联合微调**：在训练任务特定组件的同时微调 CLIP [14,30,31,39,42,72,77]。例如，CAT-Seg [14] 基于 ViT 架构的 CLIP 提出注意力微调策略，对未见过的类别具有良好泛化性；MAFT [30] 利用注意力偏置微调 CLIP，以实现掩码分类。
- **预微调**：通过低成本技术直接微调 CLIP [49,68-70,85]，与本文方法更接近。如图 4 (a) 所示，CLIM [69] 采用马赛克增强技术将多张图像拼接为一张图像，使每个子图像可作为伪区域用于区域 - 文本对比学习；如图 4 (b) 所示，CLIPSelf [68] 通过最大化区域表示与对应图像裁剪块表示的余弦相似度，提升 CLIP 的区域分类准确率。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510081819491.png)

### 2.2. 关键发现

尽管 2.1 节中的两类微调方法取得了一定效果，但仍存在局限：联合微调方法通常依赖密集预测任务的人工标注，且对任务或模型具有特异性；预微调方法适用性更广，但仅能实现区域级微调，在需要像素级细节的图像分割任务中仍有不足。为解决该问题，本文探索在 CLIP 的预微调中融入像素级细节，使其更适配开放词汇密集预测任务。下文将从分析 CLIP 不同层的注意力图入手展开研究。

#### “代理令牌” 现象

如图 3 (a) 所示，在 CLIP 的浅层（如第 6 层），[CLS] 令牌的注意力权重在图像上分布较广；但在较深层，[CLS] 令牌会将注意力从图像主要目标转移到背景中的特定令牌上（图像背景中的亮点所示）。此外，图像令牌（第二、三行）也表现出与 [CLS] 令牌类似的行为 —— 无论自身位置如何，都会对背景中的某些令牌投入高注意力。

这些背景令牌可能充当 [CLS] 令牌的 “代理”（proxy）：它们聚合其他图像令牌的关键信息，使 [CLS] 令牌通过汇总这些信息形成近似 “全局视图”，从而辅助图像分类。然而，这些 “代理令牌” 会对图像令牌间的特征相关性产生负面影响。如图 3 (a) 所示，当将锚点图像令牌的位置从 “鸟” 转移到 “树枝” 时，新的图像令牌仍对 “代理令牌” 投入高注意力，导致语义相同的图像块间缺乏相关性 —— 这对密集预测任务极为不利。

#### 视觉基础模型（VFMs）具有更优的密集相关性

考虑到 CLIP 在密集感知中存在固有局限，本文观察到：DINO 系列 [5,51]（自监督训练）、SAM 系列 [36,54]（基于大规模分割数据训练）等视觉基础模型（VFMs），能够提取具有强空间一致性的特征（图 3 (b)）。

具体而言，VFMs 的注意力图中未出现 CLIP 的 “代理令牌” 现象；此外，当改变锚点图像令牌的位置时，VFMs 对语义相同的图像令牌表现出更优的相关性。因此，本文考虑将 VFMs 融入 CLIP 的预微调过程，以进一步提升 CLIP 的特征相关性。但这种直接融合的效果并不理想：==同时进行 VFM 蒸馏 ² 和自蒸馏 ³ 会导致区域分类性能下降==（表 1 第 2 行）。本文推测，这一现象源于 “空间特征相关性” 与 “视觉 - 语言对齐” 具有不同的优化重点 —— 在单一模型中同时优化二者会产生性能权衡。

**表 1：不同蒸馏方案的性能对比**

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510081841460.png)


注：²“VFM 蒸馏” 指将 CLIP 的 $X_{\text{dense}}$ 与 VFM 的 $X_{\text{dense}}$ 的特征自相关性对齐；³“自蒸馏” 指将 $X_{\text{dense}}$ 的区域特征与其对应的 [CLS] 表示对齐。

## 3. 方法设计

通过上述分析，本文发现 CLIP 在密集预测中表现不佳的原因是：其图像令牌无法有效聚合语义相关区域的信息。受 VFMs 注意力图的启发，本文尝试将 VFMs 融入 CLIP 的预微调过程；同时，==考虑到 “特征相关性” 与 “视觉 - 语言对齐” 的优化冲突，本文对 CLIP 采用解耦特征增强策略==。

本节将介绍 DeCLIP—— 一种用于 CLIP 适配密集预测任务的无监督微调框架：3.1 节阐述如何将 CLIP 的自注意力机制解耦为 “内容” 和 “上下文” 组件；3.2 节说明这些组件如何通过蒸馏从不同 “教师” 模型中学习。

### 3.1. 解耦注意力

在 $X_{\text{dense}}$ 上同时进行自蒸馏和 VFM 蒸馏的尝试未获成功（表 1 第 2 行），这促使本文探索解耦蒸馏的可行性。下文提出：解耦 CLIP 的自注意力模块，获取 “内容” 和 “上下文” 特征，并分别优化其局部区分性和空间一致性能力（图 4 (c)）。

#### 对自注意力的重新思考

如 2.1 节所述，在 CLIP 的最后一个注意力块中，V 特征在由 Q 和 K 生成的注意力图（ $\text{Attn}_{qk}$ ）引导下进行加权求和 ——==Q 和 K 定义了图像令牌间的空间或语义关系==。研究 [38,59,63,71] 表明，CLIP 的密集特征 $X_{\text{dense}}$ 可通过逐像素分类直接用于语义分割，这意味着 $X_{\text{dense}}$ 的每个像素都包含独立的语义信息。受此启发，本文==将 Q 和 K 视为提升空间一致性的锚点，将 $X_{\text{dense}}$ 视为增强局部区分性的锚点==。

此外，近期无训练开放词汇分割（OVS）研究 [38,63] 进一步推动本文对 CLIP 自注意力进行解耦后蒸馏：这些研究将 CLIP 的注意力块从 $\text{Attn}_{qk}$ 修改为 $\text{Attn}_{qq}$ 并移除残差连接，通过仅关注 Q 简化局部特征一致性的优化。基于对 CLIP 自注意力的重新思考及上述方法的启发，本文提出解耦 CLIP 的最后一个注意力块，以获取用于蒸馏的 “内容” 和 “上下文” 特征，具体过程如下：$$X_{\text{context}}=\text{Proj}_q(X),\ V=\text{Proj}_v(X) \tag{4}$$$$
X_{\text{content}}=\text{Proj}\left(\text{Attn}_{\text{context}} \cdot V\right) \tag{5}$$$$
\text{Attn}_{\text{context}}=\text{SoftMax}\left(X_{\text{context}} X_{\text{context}}^\top/\sqrt{d}\right) \tag{6}$$

具体而言，V 基于 $X_{\text{context}}$ 生成的注意力图（$\text{Attn}_{\text{context}}$）进行聚合；$X_{\text{context}}$ 决定哪些图像令牌在语义或空间上相关；$X_{\text{content}}$ 则承载每个图像令牌在视觉 - 语言空间中的语义信息。==通过这种方式解耦特征，可在统一架构中对 $X_{\text{context}}$ 和 $X_{\text{content}}$ 施加不同的引导约束，获取多样化特征表示且互不干扰==。

如 2.2 节所述，VFMs 对语义相同的图像令牌具有强相关性，因此本文以 VFMs 为引导优化 $X_{\text{context}}$，提升 CLIP 局部特征的空间一致性；同时，以自蒸馏技术为引导优化 $X_{\text{content}}$，增强 CLIP 区域特征的视觉 - 语言对齐。

如表 1 第 3 行所示，这种解耦优化显著提升了 CLIP 特征的局部区分性和空间一致性，使区域分类准确率和语义分割性能同时得到提升。

### 3.2. DeCLIP 框架细节

上一节介绍了从 CLIP 中获取解耦的 “上下文” 和 “内容” 特征的方法。本节将详细阐述解耦特征 $X_{\text{content}}$ 和 $X_{\text{context}}$ 如何从各自的教师模型中学习，以提升 CLIP 在开放词汇密集预测任务中的性能。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082029222.png)

#### 内容特征蒸馏

如图 5 所示，DeCLIP 的第一个教师模型是 CLIP 自身（即自蒸馏 [9,49,50,68]）。本文采用图像分块方法，将学生模型特征图的区域表示与教师模型对应的图像裁剪块表示（即 [CLS] 令牌）对齐。

具体步骤如下：

1. 将输入图像 I 划分为 k 个子区域，从原图中裁剪这些子区域，得到子图像集 $S=\{I_1', I_2', \cdots, I_k'\}$；
2. 学生模型以图像 I 为输入，输出如式 (6) 所述的内容特征 $X_{\text{content}} \in \mathbb{R}^{C \times H \times W}$ 和上下文特征 $X_{\text{context}} \in \mathbb{R}^{D \times H \times W}$（其中，D 为 CLIP 视觉编码器的维度，C 为视觉 - 语言模态的共享维度）；
3. 学生模型基于 S 的裁剪坐标，采用 RoI Align [28] 从 $X_{\text{content}}$ 中池化区域特征，得到区域特征集 $F_s=\{f_1^s, f_2^s, \cdots, f_k^s\}$（$f_i^s \in \mathbb{R}^{1 \times C}$）；
4. 教师模型以子图像集 S 为输入，输出与裁剪子图像对应的一系列 [CLS] 令牌，得到 [CLS] 令牌集 $F_t=\{f_1^t, f_2^t, \cdots, f_k^t\}$（$f_i^t \in \mathbb{R}^{1 \times C}$）；
5. 采用余弦相似度损失将 $F_t$ 的 [CLS] 令牌与 $F_s$ 的区域特征对齐，损失函数如下：$$\mathcal{L}_{\text{content}}=\frac{1}{k} \sum_{i=1}^k \left(1-\frac{f_i^t \cdot f_i^s}{\|f_i^t\| \cdot \|f_i^s\|}\right) \tag{7}$$

该蒸馏分支的核心思路是：对于图像中的目标，通过图像裁剪块（即 [CLS] 令牌）进行分类的准确率高于通过区域特征分类 [68]—— 这是因为 CLIP 如 2.1 节所述，是基于图像 - 文本对通过对比学习预训练的。因此，通过模仿图像裁剪块的 [CLS] 令牌（$F_t$），$\mathcal{L}_{\text{content}}$ 的蒸馏学习可==增强 CLIP 区域特征（$F_s$）的区分性==。然而，如 2.2 节所述，区域级微调在需要像素级场景理解的图像分割任务中仍有局限。

#### 上下文特征蒸馏

如 2.2 节所述，VFMs 不存在 CLIP 的 “代理令牌” 问题，且能更好地关联语义相关的图像令牌 —— 这有助于==细粒度局部感知==。因此，本文将这些相关性蒸馏到 CLIP 的 $X_{\text{context}}$ 特征中。

如图 5 所示，将同一图像 I 输入 VFM，得到其密集特征表示 $X_{\text{dense}}^{\text{VFM}} \in \mathbb{R}^{D \times H \times W}$。为确保图像块嵌入后令牌数量一致，VFM 和学生模型（CLIP）通常采用不同的输入分辨率。==为将 VFM 的图像令牌间相关性迁移到 CLIP，需引入中间变量表示两个图像令牌间的相关体积 —— 本文采用余弦相似度计算该相关体积==：$$r_{ij}=\frac{x_i \cdot x_j}{\|x_i\| \cdot \|x_j\|} \tag{8}$$其中，$x_i \in \mathbb{R}^{1 \times D}$ 和 $x_j \in \mathbb{R}^{1 \times D}$ 分别表示第 i 个和第 j 个图像块令牌；$r_{ij}$ 表示图像块令牌 $x_i$ 和 $x_j$ 间的相关体积。

采用 L2 损失对齐 $X_{\text{dense}}^{\text{VFM}}$ 和 $X_{\text{context}}$ 的图像令牌间相关体积差异，损失函数如下：$$\mathcal{L}_{\text{context}}=\frac{1}{H W} \sum_{i=1}^H \sum_{j=1}^W \left\|r_{ij}^{\text{VFM}} - r_{ij}^{\text{CLIP}}\right\|_2 \tag{9}$$其中，$r_{ij}^{\text{VFM}}$ 和 $r_{ij}^{\text{CLIP}}$ 分别表示 VFM 和 CLIP 中 $x_i$ 与 $x_j$ 间的相关体积。

最终，DeCLIP 的整体蒸馏学习损失可表示为：$$\mathcal{L}_{\text{total}}=\mathcal{L}_{\text{content}} + \lambda \mathcal{L}_{\text{context}} \tag{10}$$其中，$\lambda$ 为损失缩放超参数（超参数敏感性分析见附录）。

## 4. 实验

### 4.1. 数据集与评估方式

本文在多个开放词汇密集预测基准数据集上进行了广泛评估，涵盖目标检测、语义分割及基于 VLM 特征的分割任务。由于篇幅限制，数据集、评估指标及实现细节的详细描述见附录。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082053899.png)

### 4.2. 基准实验结果

#### 开放词汇目标检测

表 2 展示了 DeCLIP 在 OV-COCO 和 OV-LVIS 基准数据集上的性能：

- 在 OV-COCO 上，DeCLIP 相较于 F-ViT [68] 基准，在新类别上的 mAP 提升了 3.5 和 1.9；相较于 OV-DQUO [65] 基准，新类别 mAP 提升了 6.9 和 2.7；
- 在 OV-LVIS 上，DeCLIP 与 F-ViT 结合时，稀有类别的 mAP 提升了 1.5 和 2.3；与 OV-DQUO 结合时，稀有类别 mAP 提升了 1.3 和 2.2。

此外，在 OV-LVIS 上训练的 F-ViT+DeCLIP 模型在跨数据集（COCO 和 Objects365）评估中（表 3），进一步验证了 DeCLIP 优于现有方法。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082054166.png)

#### 开放词汇语义分割

表 4 展示了以 DeCLIP 为骨干网络的 CAT-Seg [14] 模型在多个开放词汇语义分割基准上的性能。结果表明，DeCLIP 在所有数据集上均显著提升了分割性能：即使采用 DeCLIP 的 ViT-B/16 版本，CAT-Seg 也几乎超过了所有使用 ConvNeXt-L 等更大编码器的现有 SOTA 方法；而采用 DeCLIP 的 ViT-L/14 版本时，模型在开放词汇语义分割任务中实现了新的 SOTA 结果。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082055501.png)

#### 基于 VLM 特征的开放词汇语义分割

参考现有方法 [38,59,63]，本实验为特征图中的每个像素分配余弦相似度最高的类别，并将低分辨率预测结果上采样至原图分辨率以获得最终分割图。如表 5 所示，在 8 个基准数据集的平均 mIoU 指标上，DeCLIP 优于所有现有方法，这验证了本文方法在提升 VLM 特征区分性和空间一致性方面的有效性。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082055176.png)

#### 开放词汇区域分类

在 COCO 全景验证集上，本文评估了 DeCLIP、RegionCLIP [85] 和 CLIPSelf [68] 在不同分辨率下的区域分类性能：采用 RoI Align [28] 和掩码池化（Mask Pooling），基于标注边界框和掩码从特征图中提取局部特征，并根据最大余弦相似度分配类别。如图 6 所示，Top-1 平均准确率（mAcc）结果表明，在所有分辨率下，DeCLIP 在区域识别任务中均持续优于现有方法。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082056994.png)

### 4.3. 消融实验

#### 视觉基础模型（VFMs）的影响

本文分析了不同 VFM 配置对 DeCLIP 性能的影响。如表 6 所示：

- 基于 DINO [5] 蒸馏的 DeCLIP 在分割任务中表现中等，但在区域分类任务中落后于 SAM [36,54] 和 DINOv2 [51]；
- 基于 SAM 蒸馏的 DeCLIP 在区域分类中表现优异，但分割性能低于 DINO；
- DINOv2 在区域分类和分割任务中实现了性能平衡。

此外，图 7 展示了 DINO、SAM、DINOv2 与 DeCLIP 的注意力图可视化对比。实验结果表明，DeCLIP 能有效关注与锚点图像令牌在空间或语义上相关的区域；同时，该实验也揭示了基于 DINOv2 蒸馏的 DeCLIP 性能最优的原因：SAM 缺乏语义关联能力，而 DINO 会无差别地关注图像中的所有主要目标。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082056836.png)

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202510082057114.png)

## 5. 结论

本文从注意力图的角度分析了 CLIP 在密集预测任务中的局限性，发现 CLIP 的 [CLS] 令牌会对图像令牌的注意力图产生负面影响。为解决该问题，本文提出 DeCLIP—— 一种解耦特征增强策略。在开放词汇密集预测基准数据集上的大量实验表明，DeCLIP 优于现有 SOTA 方法，在所有评估任务领域均实现了优异性能。

