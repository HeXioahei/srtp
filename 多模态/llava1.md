# 摘要

使用机器生成的instruction-following数据的指令调整大语言模型（LLM）已被证明可以提高新任务的零样本能力，但这一想法在多模态领域的探索较少。我们首次尝试使用纯语言GPT-4来生成多模态语言图像instruction-following数据。通过对此类生成数据的指令调整，我们引入了LLaVA：大型语言和视觉助手，这是一个端到端训练有素的大型多模态模型，它连接了视觉编码器和LLM，用于通用视觉和语言理解。为了促进未来对视觉指令跟随的研究，我们构建了两个具有多样化和富有挑战性的application-oriented任务的评估基准。 我们的实验表明，LLaVA展示了令人印象深刻的多模态聊天能力，有时在看不见的图像/指令上表现出多模态GPT-4的行为，并在合成的多模态instruction-following数据集上与GPT-4相比产生了85.1%的相对分数。当在科学质量保证上进行微调时，LLaVA和GPT-4的协同作用实现了92.53%的最新准确率。我们公开了GPT-4生成的可视化指令调整数据、我们的模型和代码。

# 1. 引言

人工智能的核心愿望之一是开发一种**通用助手**，它可以有效地**遵循多模态视觉和语言指令**，与人类意图保持一致，在野外完成各种现实世界的任务。

为此，社会各界对开发**语言增强的基础视觉模型**产生了新的兴趣，这些模型在开放世界视觉理解方面具有强大的能力，如分类、检测、分割和字幕，以及视觉生成和编辑。我们建议读者参考[Computer Vision in the Wild 阅读列表](https://github.com/Computer-Vision-in-the-Wild/CVinW_Readings)，以获得更新的文献汇编。在这项工作中，每个任务都由一个**单一的大视觉模型**独立解决，并在模型设计中**隐含地考虑任务指令**。此外，**语言仅用于描述图像内容**。虽然这使得语言在**将视觉信号映射到语言语义**方面发挥了重要作用，但它导致了通常具有**固定接口**的模型，其**交互性和对用户指令的适应性有限**。

另一方面，大语言模型(LLM)表明，语言可以发挥更广泛的作用：**通用助手的通用接口，其中各种任务指令可以明确地用语言表示，并引导端到端训练的神经助理切换到感兴趣的任务来解决它**。例如，最近ChatGPT和GPT-4的成功证明了对齐的LLM在遵循人类指令方面的力量，并激发了人们对开发开源LLM的巨大兴趣。其中，Llama是一个与GPT-3性能相当的开源LLM。Alpaca , Vicuna , GPT-4-LLM **利用各种机器生成的高质量指令遵循样本来提高LLM的对准能力**，与专有LLM相比，报告了令人印象深刻的性能。重要的是，**工作流是纯文本的**。

在本文中，我们提出了 **视觉指令调整（visual instruction-tuning）** ，这是将指令微调扩展到语言-图像多通道空间的首次尝试，为构建通用的视觉辅助工具铺平了道路。具体而言，我们的研究成果主要有以下几点：
* **多通道指令跟随数据（Multimodal instruction-following data）**。一个关键的挑战是缺乏视觉语言指令跟随数据。我们提出了一种数据重组的视角和流水线，**使用ChatGPT/GPT-4将图像-文本对转换为适当的指令遵循格式**。
* **多模态大模型（Large multimodal models）**。我们开发了一个大型多模态模型(LMM)，通过将 **CLIP 的开放集视觉编码器**与**语言解码器 Vicuna** 连接起来，并对生成的指令视觉语言数据进行**端到端的微调**。我们的实证研究验证了使用生成的数据进行LMM指令调优的有效性，并**为构建通用的指令跟随视觉代理提供了实用的提示**。当与GPT-4集成时，我们的方法在Science QA多模推理数据集上实现了SOTA。
* **多模式指令跟随基准（Multimodal instruction-following benchmark）**。我们为**LLaVA-BENCH**提供了两个具有挑战性的基准，以及各种配对图像、说明和详细注释。
* **开源（Open-source）**。我们向公众发布以下资产：生成的多模式教学数据、代码库、模型检查点和可视化聊天演示。

# 2. 相关工作

* **多模态指令跟随代理**。在计算机视觉中，现有的构建指令跟随代理的工作可以大致分为两类：
	* **端到端的训练模型**，针对每个特定的研究主题分别进行探索。例如，视觉语言导航任务和环境要求具体化的AI代理遵循自然语言指令，并采取一系列行动来完成视觉环境中的目标。在图像编辑领域，给定输入图像和告诉代理做什么的书面指令，InstructPix2Pix通过遵循人类的指令来编辑图像。
	* **通过LangChain/LLMS协调各种模型的系统**，如VisChatGPT、X-GPT、MM-Reaction、VisProg和ViperGPT。在构建指令遵循智能体的共同目标的同时，我们专注于开发一个端到端训练的多任务语言视觉多通道模型。
* **指令调优**。在自然语言处理(NLP)领域，为了使诸如GPT-3、T5、Palm和Opt等LLM能够遵循自然语言指令并完成真实世界的任务，研究人员探索了**LLM指令调优**的方法，从而分别产生了InstructGPT/ChatGPT、FRAN-T5、flan-Palm和OPT-IML等指令调优对应的方法。结果表明，这种简单的方法可以有效地**提高LLMS的零样本和少样本泛化能力**。因此，将NLP的想法借用到计算机视觉中是很自然的。更广泛地说，具有基础模型的师生蒸馏思想已经在其他主题中被研究过，例如图像分类。Flamingo可以被视为多模态领域的GPT-3时刻，因为它在零样本任务转移和上下文情境学习方面表现得很好。在图像文本对上训练的其他LMM包括blip-2、from mage和kosmos-1。Palm-E是一种用于[具身AI（embodied AI）](https://zhuanlan.zhihu.com/p/620342675)的LMM。OpenFlamingo和Llama-Adapter是基于最近“最好的”开源LLAMA的努力，它们使LLAMA能够使用图像输入，为构建开源多模态LLM铺平了道路。虽然这些模型呈现出良好的任务迁移泛化性能，但它们并没有与视觉语言指令数据进行明确的调整，而且它们在多模态任务中的表现通常不如只有语言的任务。本文旨在填补这一空白，并对其有效性进行研究。最后，注意到**视觉指令调整**不同于**视觉提示调整**：**前者旨在提高模型的指令跟随能力，而后者旨在提高模型适应的参数效率。**

# 3. 基于GPT的视觉指令数据生成

可用的指令跟随数据集的数量是有限的。部分原因是创建此类数据的过程很耗时，而且当考虑到人工人群搜索时，定义不那么明确。受最近GPT模型在文本标注任务中的成功应用的启发，我们建议**基于广泛存在的图像对数据，利用ChatGPT/GPT-4进行多模态指令跟随数据收集**。

**对于图像Xv及其关联的标题Xc，创建一组问题Xq以指导Xc描述图像内容**。我们敦促GPT-4策划这样一份问题清单[[#附录]]。因此，将图文对扩展为其说明版本的简单方法是：`Human : Xq Xv<Stop>Assistant : Xc<Stop>`。尽管建造成本很低，但这个简单的扩展版本在指令和回应中都缺乏多样性和深入的推理。

为了缓解这个问题，我们利**用纯语言的GPT-4或ChatGPT作为强大的教师**(两者都只接受文本作为输入)，以**创建包含视觉内容的指令跟随数据**。特别地，为了将图像编码为视觉特征，以提示纯文本GPT，我们使用了两种类型的符号表示：
* (i)**字幕**通常从不同的角度描述视觉场景；
* (ii)**边界框**通常定位场景中的对象，每个框编码对象的概念及其空间位置。
*[[#^table14]]的顶部块中显示了一个示例。*

这种符号表示允许我们将图像编码为LLM可识别的序列。我们使用**COCO图像**，并生成**三种类型的指令跟随数据**。[[#^table14]]的底部块显示了每种类型的一个示例。对于每种类型，我们首先手动设计几个示例。它们是我们在数据收集期间拥有的唯一人工注释，并在上下文中学习查询GPT-4时用作种子示例。
* **Conversation**。我们设计了一段 asistant 和 user 之间的对话。回答的语气就像助理正在看到图像并回答问题一样。关于图像的可视内容，包括对象类型、对象计数、对象动作、对象位置、对象之间的相对位置，提出了一组不同的问题。只有有明确答案的问题才会被考虑。
* **Detailed description**。为了包括对图像的丰富和全面的描述，我们创建了一个带有这种意图的问题列表。我们提示GPT-4，然后整理列表(请参阅附录中的详细提示和整理过程[[#^table12]]）。对于每个图像，我们从列表中随机抽取一个问题，要求GPT-4生成详细描述。
* **Complex reasoning**。以上两种类型着眼于视觉内容本身，在此基础上我们进一步创建了深入的推理问题。答案通常需要一个遵循严格逻辑的循序渐进的推理过程。

我们总共收集了**158K个独特的语言-图像指令跟随样本**，其中包括**58K个对话样本**，**23K个详细描述样本**和**77K个复杂推理样本**。在我们的早期实验中，我们取消了ChatGPT和GPT-4的使用，发现GPT-4一致地提供了更高质量的指令跟随数据，如空间推理。

# 4. 视觉指令调整

## 4.1 架构

主要目标是有效地利用预先培训的LLM和可视化模型的功能。网络结构如图所示。我们**选择 Vicuna 作为由ϕ参数化的LLm fϕ(·)**，因为它在公开可用的检查点中具有最好的语言任务指令跟随能力。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091126004.png)

对于输入图像Xv，我们考虑了**预先训练的CLIP视觉编码器Vit-L/14**，它提供了**视觉特征Zv=g(Xv)**。实验中考虑了**最后一层Transformer层前后的网格特征**。我们考虑了**一个简单的线性层**，将图像特征连接到单词嵌入空间。特别地，我们应用**可训练投影矩阵W**将Zv转换为**语言嵌入标记（language embedding tokens）Hv**，其具有与语言模型中的单词嵌入空间相同的维度：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091930522.png)

因此，我们有一系列视觉符号Hv。请注意，我们的**简单投影方案是轻量级的**，它允许我们快速迭代以数据为中心的实验。**也可以考虑更复杂的连接图像和语言表征的方案**，例如Flamingo中的门控交叉注意（gated cross-attention）和BLIP-2中的Q-former。我们把为LLaVA探索可能更有效和更复杂的架构设计留到未来的工作中。

## 4.2 训练

对于每个图像Xv，我们生成多轮对话数据：![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091916226.png)，其中 T 是对话轮数。我们将它们组织成一个序列，将所有答案视为助手的响应。第 t 轮的指令为：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091918357.png)

这就有了[[#^table2]]中所示的多模态指令跟随序列的统一格式。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091925367.png)
^table2

我们**使用LLM的原始自回归训练目标对预测标记执行LLM的指令调整**。具体地说，对于**长度为 L 的序列**，我们通过以下公式计算目标答案Xa的概率：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091930111.png)

其中 θ 是可训练参数，![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091933281.png)和![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091935339.png)分别是在当前预测token xi 之前的所有轮次中的指令令牌和回答令牌。

请参见[[#^table2]]以了解预测令牌的图示。对于(3)中的条件句，我们显式地添加了Xv以强调这样一个事实，即图像对于所有答案都是根植的，我们省略了X_system-message和前面的所有`<Stop>`以获得更好的可读性。

对于LLaVA模型的训练，我们考虑了一个两阶段的指令调整过程：

* **阶段1：特征对齐的预训练**。为了在概念覆盖和训练效率之间取得平衡，我们将CC3M过滤到595k图文对。有关过滤过程的详细信息，请参阅[[#附录]]。使用第3节中描述的朴素展开方法，将这些数据对转换为跟随指令的数据。每个样本可被视为单轮对话。为了构造(2)中的输入X_instruct，对于图像 Xv，随机抽样问题 Xq，这是请求助手简要描述图像的语言指令。地面事实预测（ground-truth prediction）答案 Xa 是原始字幕。在训练中，我们保持视觉编码器和LLM权重不变，并且仅在可训练参数 θ=W (投影矩阵)的情况下最大化(3)的可能性。通过这种方式，图像特征Hv可以与预先训练的LLM字嵌入对齐。这一阶段可以理解为为冻结的LLM训练兼容的视觉记号器（visual tokenizer）。
* **阶段2：端到端的微调**。我们始终保持视觉编码权重不变，并继续更新投影层和LLm的预训练权值，即(3)中的可训练参数为 θ={W，ϕ}。我们考虑两个具体的用例场景：·
	* **多模态聊天机器人**。通过对第三节中的158K语言图像指令跟踪数据的微调，我们开发了一个聊天机器人。在三种类型的应答中，对话是多轮的，另外两种是单轮的。他们在训练中被统一抽样。
	* **科学问答（Science QA）**。我们在 Science QA 基准上研究了我们的方法，这是第一个大规模多模式科学问题数据集，通过详细的讲演和解释来注释答案。每个问题都以自然语言或图像的形式提供了一个上下文。助手以自然语言提供推理过程，并从多个选项中选择答案。对于(2)中的训练，我们将数据组织为单轮对话，将问题和上下文组织为X_instruct，将推理和答案组织为Xa。

# 5. 实验

我们使用两个主要的实验环境：**多模态聊天机器人（multimodal chatbot）** 和**ScienceQA数据集**，评估了LLaVA在**指令遵循**和**视觉推理**能力方面的表现。我们用**8×A100**训练所有模型，遵循Vicuna的超参数。我们在过**滤后的CC-595K子集**上预训练1个epoch，学习率为2E-3，批次大小为128，并在提出的LLaVA-指令-158K数据集上微调3个epoch，学习率为2E-5，批次大小为32。有关更多训练详细信息，请参阅[[#附录]]。

## 5.1 多模态聊天机器人（multimodal chatbot）
我们开发了一个聊天机器人演示，以展示LLaVA的图像理解和对话能力，并研究LLaVA消化视觉输入和展示指令跟随能力的能力。我们首先使用原始GPT-4论文中的示例，如[[#^table3]]所示([[#附录]]中有更多示例)，这些示例需要深入的图像理解。为了进行比较，我们引用了他们论文中的多模态GPT-4的提示和响应，并查询了BLIP-2和OpenFlamingo模型检查点以获得他们的响应。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101440195.png)
^table3

与BLIP-2和OpenFlamingo相比，LLaVA准确地遵循用户的指令，而不是简单地描述场景。LLaVA提供了比GPT-4更全面的反应。即使只是被要求描述图像，LLaVA也能识别图像的非典型方面。

令人惊讶的是，尽管LLAVA是用一个小的多模态指令跟随数据集(∼80K唯一图像)训练的，但它在这些例子上表现出了与多模态GPT-4非常相似的推理结果。请注意，**虽然这些图像超出了LLaVA的范围，但LLaVA仍然能够理解场景并按照问题说明提供合理的回答**。相比之下，BLIP-2和OpenFlamingo专注于描述图像，而不是按照用户的指示以适当的方式回答。

### 5.1.1 定量评估

为了系统地了解LLaVA的性能，我们提出了一个量化度量来衡量该模型在多模式数据上的指令跟随能力。我们**利用GPT-4来衡量生成的响应的质量**。具体地说，我们创建了**由图像、基本事实文本描述和问题组成的三元组**。候选模型(例如，LLaVA)基于问题和图像预测答案。为了提供一个近似的理论上限，我们使用纯文本的GPT-4，基于问题和基本事实的文本描述创建参考预测。在从两个模型获得回答后，我们将问题、可视信息(以文本描述的格式)和从两个助手产生的回答反馈给法官(即纯文本GPT-4)。它评估助理的回答的帮助程度、相关性、准确性和详细程度，并给出从1到10的总分，分数越高表示总体表现越好。它还被要求为评估提供一个全面的解释，以便我们更好地理解模型。我们报告涉及纯文本GPT-4模型的相对分数，使用文本地面真实描述作为可视输入。我们创建了两个基准来评估模型的性能。
* **LLaVA-Bench (COCO)**。我们从Coco-Val-2014中随机选择了30幅图像，对于每一幅图像，我们使用SEC.3中提出的数据生成管道生成了三种类型的问题(对话、详细描述和复杂推理)，共90个问题。该基准测试研究具有一致视觉输入的模型的对齐行为和能力。我们改变训练数据集来研究不同类型的指令跟随数据的有效性，结果如[[#^table4]]所示。首先，通过指令调整，模型对用户指令的跟随能力显著提高了50个点以上。其次，增加少量的详细描述和复杂的推理问题，使得模型的整体性能有了相当大的提高，提高了7分。此外，它还提高了模型在会话问题上的表现，表明推理能力的改进是对会话能力的补充。最后，我们显示拥有所有三种类型的数据会产生最好的性能，达到85.1%。
* **LLaVA-BENCH(in the wild)**。为了评估该模型在更具挑战性的任务中的能力和对新领域的泛化能力，我们收集了24张不同的图像，总共60个问题，包括室内和室外场景、表情包、绘画、素描等，并将每张图像与高度详细的手动策划的描述和适当的问题选择相关联。我们在表5中比较了LLaVA、BLIP和OpenFlamingo。由于可视化指令调优，LLaVA的性能明显好于BLIP-2(+29%)和OpenFlamingo(+48%)。与可以获得基本事实标签的纯文本GPT-4相比，LLaVA在复杂推理问题上的表现令人印象深刻，总分为67.3%。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101502971.png)
^table4

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101502221.png)
^table5

### 5.1.2 limitation
这个LLaVA-BENCH(in the wild)的设计是为了具有挑战性，并揭示模特的弱点。我们提供了两个例子和[[#^table6]]中的相关标题和问题。对于拉面例子(左)，要正确回答餐厅名称，需要模型具有较大的知识覆盖率和多语言理解能力；要正确描述配菜，模型可能需要从互联网检索相关的多模式信息。以冰箱为例(右图)，要想识别出正确的酸奶品牌，模型需要处理高分辨率图像，并且拥有广泛的知识覆盖范围。我们还注意到LLaVA的一个有趣的失败，当被问及是否有草莓口味的酸奶时，它的回答是肯定的，尽管冰箱里只有酸奶和草莓。这表明，LLaVA有时会将图像视为“bag of patch”，无法理解图像中复杂的语义。我们希望LLaVA作为基准的坚实基线，我们的发现可以激励未来开发更有能力的LMM的工作。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101506148.png)
^table6

## 5.2 ScienceQA

Science QA包含21k个多模态多项选择题，具有丰富的领域多样性，涉及3个主题、26个主题、127个类别和379项技能。基准数据集分为训练、验证和测试三个部分，分别有12726个、4241个和4241个例子。我们考虑了两种有代表性的方法，包括有和没有思想链(COT)的GPT-3.5模型(Text-DaVinci-002)、LLaMA-Adapter，以及多模态思想链(MM-CoT)，这是该数据集上当前的SOTA方法。

结果如[[#^table7]]所示。对于LLaVA，我们使用最后一层之前的视觉特征，让模型首先预测原因，然后预测答案，并对其进行12个历元的训练。该方法的准确率为90.92%，与SOTA的91.68%相当接近。为了探索LLMS的局限性，我们还使用两次上下文学习来提示GPT-4，获得了82.69%的准确率，与GPT-3.5的75.17%相比，绝对提高了7.52%。

对于许多问题，我们注意到GPT-4之所以失败，仅仅是因为它报告没有足够的上下文，如图像或情节。我们考虑了两个方案来结合我们的模型和GPT-4的结果。
* (i)GPT-4补充物。每当GPT-4无法提供答案时，我们就使用我们方法的预测。该格式的准确率为90.97%，与单独应用我们的方法几乎相同。
* (ii)担任法官的GPT-4。每当GPT-4和LLaVA产生不同的答案时，我们会再次提示GPT-4，要求它根据问题和两个结果提供自己的最终答案。这种精神与科特相似，但与另一款车型的外部知识相似。令人惊讶的是，该方案能够在所有问题类上提供一致的改进，并实现了92.53%的新SOTA正确率。有趣的是，不能处理图像的纯文本GPT-4提高了模型在以图像为背景的问题上的整体性能。这是因为这些问题中的一些实际上不需要图像上下文来获得正确的答案。GPT-4法官可以识别此类案件，并纠正LLaVA犯下的一些错误。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101511025.png)
^table7

## 5.3 消融

我们在[[#^table8]]中列出了Science QA上的几个设计选择。
* (i)**视觉特征**。我们尝试使用剪辑视觉编码器中的最后一层特征，它的收益率为89.96%，比最后一层之前的特征低0.96%。我们假设这是因为与之前的层相比，Clip的最后一层功能可能更关注全局和抽象的图像属性，这可以更关注有助于理解特定图像细节的本地化属性。
* (ii)**连锁反应**。为了确定模型预测中答案和推理过程之间的顺序，我们运行了两个变量，并观察到答案优先在12个历元中报告了最好的数89.77%的准确率，而推理-优先在6个历元中可以迅速达到89.77%的准确率，但随着训练的增加没有进一步的提高。对模型进行24个历元的训练并不能提高性能。我们的结论是，类似COT的推理优先策略可以在很大程度上改善收敛，但对最终性能的贡献相对较小。
* (iii)**职前培训**。我们跳过预培训，直接从头开始培训Science QA-性能下降到85.81%的准确率。5.11%的绝对降幅表明了我们的预训练阶段在保持大量预训练知识的同时对齐多模特征的重要性。
* (iv)**型号尺寸**。我们保持所有配置与我们最好的13B机型相同，并训练7B机型。该方法的准确率为89.84%，比90.92%的准确率低1.08%，说明了模型尺度的重要性。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505101514033.png)
^table8

# 6. 结论

本文论证了可视化指令调整的有效性。我们提出了一种自动生成语言图像指令跟随数据的流水线，并在此基础上训练了LLaVA，一个跟踪人类意图完成视觉任务的多模态模型。当微调到Science QA时，它实现了新的SOTA准确性；当微调到多模态聊天数据时，它实现了出色的可视聊天能力。此外，我们还提出了第一个研究多模态指令跟随能力的基准。本文是可视化指令调优的第一步，主要关注真实任务。有关LLaVA在学术基准上的更多量化结果，请参考改进的基线和可视化指令调整。我们希望我们的工作能启发未来构建更有能力的多模态模型的研究。
# 附录

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091037256.png)^table11

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091036474.png)^table12

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202505091020049.png)^table14

