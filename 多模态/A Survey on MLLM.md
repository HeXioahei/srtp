博客推荐：[A Survey on Multimodal Large Language Models-全文解读 - 知乎](https://zhuanlan.zhihu.com/p/641866192)
[【综述论文阅读】A Survey on Multimodal Large Language Models 上_prompt-based ensemble expert language models with -CSDN博客](https://blog.csdn.net/weixin_46231495/article/details/145903655?ops_request_misc=&request_id=&biz_id=102&utm_term=a%20survey%20on%20multimodal%20large%20l&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-145903655.142^v102^pc_search_result_base9&spm=1018.2226.3001.4187)


# 架构

典型架构抽象为三个模块，即预训练的**多模态编码器、预训练的大语言模型**以及连接它们的**多模态接口**。

## 编码器

选择编码器时，需要考虑**分辨率、参数量**和**预训练语料库**等因素。分辨率的重要性最大。

**扩展输入分辨率的方法：**
* 直接缩放：**将更高分辨率的图像输入到编码器中**，通常需要进一步调整编码器或替换为更高分辨率的预训练编码器。
* 分块：**高分辨率图像切割成小块，并重用低分辨率编码器**。

## 预训练LLM

**常用公开可用LLM**：
* **FlanT5 系列**：是较早的 LLM，被用于类似 BLIP-2 和 InstructBLIP 的工作中。
* **LLaMA 系列 和 Vicuna 家族**：是具有代表性的开源 LLM，吸引了大量学术界的关注。由于这两类 LLM 主要基于英文语料库进行预训练，因此在多语言支持方面存在局限性，例如对中文的支持较为有限。
* **Qwen**： 是一款双语 LLM，能够很好地支持中文和英文。

扩大 LLM 的参数规模能带来额外的收益，类似于提高输入分辨率的情况。也有一些研究专注于使用较小的 LLM，以方便在移动设备上部署。关于 LLM 的专家混合（MoE, Mixture of Experts）架构的研究逐渐受到关注，与密集模型相比，稀疏架构通过选择性激活参数，能够在不增加计算成本的情况下扩展总参数规模。

## 模态接口

**两种实现模态接口的方法：**
* 在预训练的视觉编码器和LLM之间引入一个**可学习的连接器**。
* 通过**专家模型将图像翻译成语言**，然后将语言发送给LLM。

### 可学习连接器

该模块**将信息投影到LLM能够高效理解的空间中**。

实现这种接口的方法大致有两种：
* **token级融合** ：编码器输出的特征被转换为token，并在发送到LLM之前与文本token连接。
	* **利用一组可学习的查询token以基于查询的方式提取信息**：如BLIP-2。
	* 仅使用**基于MLP的接口**来弥合模态差距：如LLaVA系列。
* **特征级融合**：**通过插入额外的模块**来实现文本特征与视觉特征之间的深度交互和融合。例如：
	* Flamingo 在冻结的 LLM Transformer 层之间插入了额外的**交叉注意力层**，从而利用外部视觉线索增强语言特征。
	* CogVLM 在每个 Transformer 层中插入了一个**视觉专家模块**，以实现视觉特征与语言特征之间的双向交互和融合。为了获得更好的性能，所引入模块的 QKV 权重矩阵初始化自预训练的 LLM。
	* LLaMA-Adapter 在 Transformer 层中引入了**可学习的提示（prompt）**。这些提示首先嵌入了视觉知识，然后作为前缀与文本特征连接。

对于token级融合，模态适配器的类型远不如**视觉token的数量**和**输入分辨率**重要。

### 专家模型

**将多模态输入转换为语言表示，而无需额外训练**。LLM可以通过转换后的语言来理解多模态信息。例如，VideoChat-Text 使用预训练的视觉模型提取动作等视觉信息，并通过语音识别模型丰富描述内容。尽管使用专家模型的方法较为直接，但它可能不如采用可学习接口那样灵活。将其他模态的信息转换为文本会导致信息丢失。

# 训练策略与数据

一个完整的多模态大型语言模型（MLLM）需要经历三个训练阶段，即**预训练**、**指令微调**和**对齐优化**。每个训练阶段都需要不同类型的训练数据，并实现不同的目标。

## 预训练

目标是**对齐不同模态**并**学习多模态世界知识**，通常需要**大规模**的文本配对数据。
### 训练细节

给定一张图像，模型被训练以**自回归**的方式预测该图像的描述（caption），采用标准的交叉熵损失函数进行优化。一种常见的预训练方法是**冻结预训练模块**（例如视觉编码器和 LLM），然后训练一个**可学习的接口**，其核心思想是在不丢失预训练知识的情况下对齐不同模态。 一些方法还会**解冻更多模块**（例如视觉编码器），以增加可用于对齐的可训练参数。

训练方案与**数据质量**密切相关。对于**短且噪声较大**的描述数据，可以采用**较低分辨率**（例如 224）来加速训练过程；而对于**较长且较为干净**的数据，则更适合使用**更高分辨率**（例如 448 或更高）以减少幻觉现象的发生。

### 数据

类型：
* 粗粒度数据：
	* 样本通常来源于互联网爬取。
	* **数据量大**
	* **短且噪声较大
* 细粒度数据生成：
	* 通过**提示强大的 MLLM**（如 GPT-4V）生成高质量的细粒度数据。
	* 包含更长且更准确的图像描述，从而实现图像与文本模态之间更精细的对齐。
	* 需要调用商业用途的 MLLM，成本较高，数据量相对较小。
	* *ShareGPT4V [83] 通过一种折中方法实现了高效的数据生成：首先使用 GPT-4V 生成的 10 万条数据训练一个描述生成模型，然后利用该预训练模型扩展数据量至 120 万条。*
## 指令微调

**指令（Instruction）** 是指任务的描述。指令微调的目标是**教会模型更好地理解用户的指令并完成所要求的任务**。

### 训练细节










本文将最近的代表性 MLLM 分为四种主要类型：多模态指令调整 (MIT)、多模态上下文学习 (M-ICL)、多模态思维链 (M-CoT) 和 [LLM 辅助视觉推理](https://zhida.zhihu.com/search?content_id=230768050&content_type=Article&match_order=1&q=LLM+%E8%BE%85%E5%8A%A9%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86&zhida_source=entity) (LAVR)。

# MIT（Multimodal Instruction Tuning）

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503112035847.png)

Instruction Tuning（指令调优）是一种涉及在指令格式数据集集合上微调预先训练的llm的技术。通过这种方式进行调整，LLM 可以通过遵循新指令泛化到看不见的任务，从而提高零样本性能。Instruction Tuning学习如何泛化到未知的任务，而不是像一对一的拟合特定任务。

M-IT中校准预训练的一种常见方法是保持预训练模块(例如visual encoder和llm)冻结，仅训练一个可学习的接口。

现有VQA和标题数据集的答案通常很简洁，直接使用这些数据集进行指令调优可能会限制MLLM的输出长度。解决这个问题有两种常见的策略：第一个是修改instruction，直接告诉LLM我们要简短、单句。第二种方法是延长现有答案的长度。

Self-Instruction：尽管现有的基准数据集可以提供丰富的数据源，但它们通常不能很好地满足现实场景中的人类需求，例如多轮对话。为了解决这个问题，一些作品通过自我指导来收集样本[60]，这引导llm使用一些手工注释的样本来生成遵循文本指令的数据。

混合指令调优不比多模态数据上单独调优差。

Learnable Interface：当冻结预训练模型的参数时，可学习接口负责连接不同的模态。挑战在于如何有效地将视觉内容翻译成LLM可以理解的文本。一种常见且可行的解决方案是利用一组可学习的query token以基于查询的方式提取信息。此外，一些方法使用基于projection head的桥接方法来缩小模态差距。

Expert Model：借助专家模型来简化视觉信息更简单，但是灵活性很差，而且会存在信息丢失的情况。

# MICL（Multimodal In-Context Learning）
