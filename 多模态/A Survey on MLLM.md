博客推荐：[A Survey on Multimodal Large Language Models-全文解读 - 知乎](https://zhuanlan.zhihu.com/p/641866192)
[【综述论文阅读】A Survey on Multimodal Large Language Models 上_prompt-based ensemble expert language models with -CSDN博客](https://blog.csdn.net/weixin_46231495/article/details/145903655?ops_request_misc=&request_id=&biz_id=102&utm_term=a%20survey%20on%20multimodal%20large%20l&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-145903655.142^v102^pc_search_result_base9&spm=1018.2226.3001.4187)


# 架构

典型架构抽象为三个模块，即预训练的**多模态编码器、预训练的大语言模型**以及连接它们的**多模态接口**。

## 编码器

选择编码器时，需要考虑**分辨率、参数量**和**预训练语料库**等因素。分辨率的重要性最大。

**扩展输入分辨率的方法：**
* 直接缩放：**将更高分辨率的图像输入到编码器中**，通常需要进一步调整编码器或替换为更高分辨率的预训练编码器。
* 分块：**高分辨率图像切割成小块，并重用低分辨率编码器**。

## 预训练LLM

**常用公开可用LLM**：
* **FlanT5 系列**：是较早的 LLM，被用于类似 BLIP-2 和 InstructBLIP 的工作中。
* **LLaMA 系列 和 Vicuna 家族**：是具有代表性的开源 LLM，吸引了大量学术界的关注。由于这两类 LLM 主要基于英文语料库进行预训练，因此在多语言支持方面存在局限性，例如对中文的支持较为有限。
* **Qwen**： 是一款双语 LLM，能够很好地支持中文和英文。

扩大 LLM 的参数规模能带来额外的收益，类似于提高输入分辨率的情况。也有一些研究专注于使用较小的 LLM，以方便在移动设备上部署。关于 LLM 的专家混合（MoE, Mixture of Experts）架构的研究逐渐受到关注，与密集模型相比，稀疏架构通过选择性激活参数，能够在不增加计算成本的情况下扩展总参数规模。

## 模态接口

**两种实现模态接口的方法：**
* 在预训练的视觉编码器和LLM之间引入一个**可学习的连接器**。
* 通过**专家模型将图像翻译成语言**，然后将语言发送给LLM。

### 可学习连接器

该模块**将信息投影到LLM能够高效理解的空间中**。

实现这种接口的方法大致有两种：
* **token级融合** ：编码器输出的特征被转换为token，并在发送到LLM之前与文本token连接。
	* **利用一组可学习的查询token以基于查询的方式提取信息**：如BLIP-2。
	* 仅使用**基于MLP的接口**来弥合模态差距：如LLaVA系列。
* **特征级融合**：**通过插入额外的模块**来实现文本特征与视觉特征之间的深度交互和融合。例如：
	* Flamingo 在冻结的 LLM Transformer 层之间插入了额外的**交叉注意力层**，从而利用外部视觉线索增强语言特征。
	* CogVLM 在每个 Transformer 层中插入了一个**视觉专家模块**，以实现视觉特征与语言特征之间的双向交互和融合。为了获得更好的性能，所引入模块的 QKV 权重矩阵初始化自预训练的 LLM。
	* LLaMA-Adapter 在 Transformer 层中引入了**可学习的提示（prompt）**。这些提示首先嵌入了视觉知识，然后作为前缀与文本特征连接。

对于token级融合，模态适配器的类型远不如**视觉token的数量**和**输入分辨率**重要。

### 专家模型

**将多模态输入转换为语言表示，而无需额外训练**。LLM可以通过转换后的语言来理解多模态信息。例如，VideoChat-Text 使用预训练的视觉模型提取动作等视觉信息，并通过语音识别模型丰富描述内容。尽管使用专家模型的方法较为直接，但它可能不如采用可学习接口那样灵活。将其他模态的信息转换为文本会导致信息丢失。

# 训练策略与数据

一个完整的多模态大型语言模型（MLLM）需要经历三个训练阶段，即**预训练**、**指令微调**和**对齐优化**。每个训练阶段都需要不同类型的训练数据，并实现不同的目标。

## 预训练

目标是**对齐不同模态**并**学习多模态世界知识**，通常需要**大规模**的文本配对数据。
### 训练细节

给定一张图像，模型被训练以**自回归**的方式预测该图像的描述（caption），采用标准的交叉熵损失函数进行优化。一种常见的预训练方法是**冻结预训练模块**（例如视觉编码器和 LLM），然后训练一个**可学习的接口**，其核心思想是在不丢失预训练知识的情况下对齐不同模态。 一些方法还会**解冻更多模块**（例如视觉编码器），以增加可用于对齐的可训练参数。

训练方案与**数据质量**密切相关。对于**短且噪声较大**的描述数据，可以采用**较低分辨率**（例如 224）来加速训练过程；而对于**较长且较为干净**的数据，则更适合使用**更高分辨率**（例如 448 或更高）以减少幻觉现象的发生。

### 数据

类型：
* 粗粒度数据：
	* 样本通常来源于互联网爬取。
	* **数据量大**
	* **短且噪声较大
* 细粒度数据生成：
	* 通过**提示强大的 MLLM**（如 GPT-4V）生成高质量的细粒度数据。
	* 包含更长且更准确的图像描述，从而实现图像与文本模态之间更精细的对齐。
	* 需要调用商业用途的 MLLM，成本较高，数据量相对较小。
	* *ShareGPT4V [83] 通过一种折中方法实现了高效的数据生成：首先使用 GPT-4V 生成的 10 万条数据训练一个描述生成模型，然后利用该预训练模型扩展数据量至 120 万条。*
## 指令微调

**指令（Instruction）** 是指任务的描述。指令微调的目标是**教会模型更好地理解用户的指令并完成所要求的任务**。

* **监督微调（Supervised Fine-tuning）**：通常**需要大量的任务特定数据**来训练一个**任务特定**的模型。
* **提示（Prompting）**：减少了对大规模数据的依赖，并可以通过提示工程完成**特定任务**。在这种情况下，尽管**少样本性能有所提高**，但零样本性能仍然较为普通。
* **指令微调（Instruction Tuning）**：不同于上述两种方法，指令微调专注于学习如何**泛化到未见过的任务**，而不是像前两者那样适应特定任务。此外，指令微调与多任务提示（multi-task prompting）密切相关。

### 训练细节

形式上，一个多模态指令样本可以用三元组的形式表示，即 ((I, M, R))，其中 (I)、(M)、(R) 分别代表**指令**、**多模态输入**和**真实响应**。多模态大型语言模型（MLLM）根据指令和多模态输入预测答案：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503131859242.png)

这里，(A) 表示预测的答案，(θ) 是模型的参数。训练目标通常是用于训练大型语言模型（LLM）的原始自回归目标，在此基础上，MLLM 被鼓励预测响应中的下一个标记。该目标可以表示为：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503131900722.png)

其中，(N) 是真实响应的长度。

### 数据收集

有三种典型的规模化获取指令数据的方法：
* **数据适配**：利用现有的高质量的特定任务数据集来构建指令格式化的数据集。
* **自我指令生成**：利用 LLM 根据少量人工标注的样本生成文本指令数据。
* **数据混合**：多模态指令数据和仅语言数据的混合，包括混合指令微调（将两种类型的数据组合并随机打乱）和顺序指令微调（先用文本数据再用多模态数据）。

### 数据质量

提升数据质量的两个重要方面：
* **提示多样性**：多样化的提示有助于提升模型性能和泛化能力。
* **任务覆盖范围**：关于训练数据中涉及的任务类型，Du 等人进行了一项实证研究，发现视觉推理任务对于提升模型性能的效果优于图像描述和问答任务。此外，该研究建议，增强指令的复杂性可能比增加任务多样性或引入细粒度的空间注释更有益。

## 对齐微调

对齐微调更常用于**让模型与特定人类偏好对齐**的场景，例如减少幻觉的回答。目前，**基于人类反馈的强化学习（RLHF）** 和**直接偏好优化（DPO）** 是对齐调优的两种主要技术。

### RLHF
利用强化学习算法将LLMs与人类偏好对齐，并以人类标注作为训练循环中的监督。如InstructGPT所示，RLHF包含三个关键步骤：
1. **监督微调**：微调预训练模型，以呈现初步期望的输出行为。在RLHF设置中，微调后的模型称为策略模型。*此步骤可能会被跳过。*
2. **奖励建模（Reward Modeling）**
3. **强化学习（Reinforcement Learning）**

### DPO
DPO 使用简单的二分类损失从人类偏好标签中学习。与基于 PPO 的 RLHF 算法相比，DPO 不需要显式学习奖励模型，从而简化了整个流程为两个步骤：人类偏好数据收集和偏好学习。

# 评估

## 封闭集评估

## 开放集评估
### 人工评估
### GPT评分
### 案例研究


# 扩展多模态模型

* **粒度支持**：即对输入和输出粒度进行更精细支持的技术。
* **模态支持**：调整MLLM以支持更多多模态内容的输入，或生成更多模态的响应。
* **语言支持**：开发多语言模型，以覆盖更广泛的用户群体。
* **场景支持**：除了开发通用的通用助手外，一些研究还集中在需要考虑实际条件的更具体场景中，而另一些研究则将MLLM扩展到具有特定专业知识的下游任务中。

# 多模态幻觉

由MLLMs生成的响应与图像内容不一致的现象。

## 幻觉类型
* **存在幻觉**：最基本的形式，指的是模型错误地声称图像中存在某些物体。
* **属性幻觉**：指的是错误地描述某些物体的属性。它通常与存在幻觉相关，因为对属性的描述应基于图像中存在的物体。
* **关系幻觉**：一种更复杂的类型，同样基于物体的存在。它指的是对物体之间关系（如相对位置和互动）的错误描述。

## 评估方法
* **CHAIR**： 是一种早期用于评估开放式描述中幻觉水平的指标。该指标衡量的是在提到的所有物体中，包含幻觉物体的句子所占的比例。
* **POPE**： 是一种评估封闭式选择的方法。该方法通过构建多个带有二元选择的提示，每个提示都询问图像中是否存在特定物体。该方法还涵盖了更具挑战性的设置，以评估MLLMs的鲁棒性，并考虑了数据统计。最终评估使用了一种简单的关键词检测机制，即通过检测“是/否”关键词，将开放式回答转换为封闭式二元选择。
* **MME**：提供了更全面的评估，涵盖了存在性、数量、位置和颜色等方面。
* **HaELM**：不使用匹配机制来检测幻觉，提出使用纯文本LLM作为判断器，自动判断MLLM生成的描述是否与参考描述一致。
* **Woodpecker**：使用GPT-4V直接评估基于图像的模型响应，解决纯文本LLM评估只能访问有限的图像上下文并需要参考注释的问题。
* **FaithScore**：是一种更细粒度的指标，基于一种将描述性子句分解并分别评估每个子句的流程。
* **AMBER**：是一个无需LLM的基准，涵盖了判别任务和生成任务，并涉及三种幻觉类型。

## 缓解幻觉的方法

将目前已有方法大致分为三类：预校正、过程校正和后校正。

### 预校正
一种直观且直接的解决幻觉问题的方法是**收集专门的数据**（例如负面数据），并使用这些数据进行微调，从而生成幻觉较少的模型。
* **LRV-Instruction**：引入了一个视觉指令微调数据集。除了常见的正面指令外，该数据集还融入了在不同语义层次上精心设计的负面指令，以鼓励模型生成更符合图像内容的响应。
* **LLaVA-RLHF**：收集了人类偏好对，并通过强化学习技术对模型进行微调，从而使模型的回答更加贴近人类偏好，同时减少幻觉现象。

### 过程内校正
另一条研究路线是通过**改进架构设计或特征表示**来减少幻觉现象。这些工作试图探索幻觉产生的原因，并在生成过程中设计相应的解决方案以减轻其影响。
* **HallE-Switch**：对可能导致物体存在幻觉的因素进行了实证分析，并假设存在幻觉是因为物体未被视觉编码器正确锚定，而这些物体实际上是基于 LLM 的知识推断出来的。基于这一假设，该研究引入了一个连续控制因子及相应的训练方案，用于在推理过程中控制模型输出中的想象程度。
* **VCD**：指出物体幻觉主要由两个原因引起，即训练语料库中的统计偏差和 LLM 中嵌入的强大语言先验。作者注意到，当向图像中注入噪声时，MLLMs 更倾向于依赖语言先验而非图像内容生成响应，从而导致幻觉现象。为此，该研究设计了一种“放大-对比”解码方案，以抵消这种错误偏差。
* **HACL**：研究了视觉与语言的嵌入空间。基于观察结果，提出了一种对比学习方案，旨在将配对的跨模态表示拉得更近，同时将非幻觉文本表示与幻觉文本表示推开，从而减少幻觉现象的发生。

### 后校正
在输出生成后对幻觉进行纠正。
* **Woodpecker**：这是一个无需训练的通用幻觉修正框架。具体来说，该方法结合专家模型来补充图像的上下文信息，并设计了一条逐步修正幻觉的流水线。该方法具有可解释性，因为每一步的中间结果可以被检查，并且对象能够在图像中得到锚定。
* **LURE**：该方法训练了一个专门的修订器，用于屏蔽描述中不确定性较高的对象，并重新生成响应。

# 扩展技术

## 多模态Few-Shot学习（多模态上下文学习，Multimodal In-Context Learning，M-ICL）

ICL 具有两个显著特点：
* **与传统监督学习的区别**：不同于从大量数据中学习隐式模式的传统监督学习方法，ICL 的核心在于通过**类比**进行学习。LLMs 通过少量示例（以及可选的指令）进行学习，并将其推广到新问题上，从而以少样本的方式解决复杂且未见过的任务。
* **无需训练**：ICL 通常以无训练的方式实现，因此可以在推理阶段灵活集成到不同的框架中。

指令微调能够增强 ICL 能力。

ICL 被扩展到更多模态，形成了 多模态 Few-Shot 学习（Multimodal ICL, M-ICL）。

### 提升ICL能力

越来越多的研究专注于在各种场景下增强上下文学习（ICL）的性能。
* **MIMIC-IT**：通过构建包含多模态上下文格式的指令数据集，将上下文学习与指令微调相结合。在该数据集上进行指令微调后的模型在标题生成任务中表现出更强的少样本学习能力。
* **Emu**：扩展了 Flamingo的思想，在模型生成和对应的训练语料库中引入额外的模态。借助引入的视觉解码器（即 Stable Diffusion），模型能够从额外的视觉监督中学习，并支持更灵活的输出格式和上下文推理。具体而言，除了以纯文本形式回答问题外，模型还可以以图像形式给出响应。
* **Sheng 等人**：采用类似的思想，尝试将输出模态扩展为文本和图像两种形式。与使用专门的图像编码器不同，该研究采用统一的量化方案，并共享嵌入层。

此外，还有一些其他工作探索在特定设置下提升少样本学习性能的方法：
* **Link-context Learning**：专注于强化图像-标签对之间的因果联系，通过构建正负图像-描述对，提出了一种对比训练方案。
* **MMICL**：旨在增强模型处理多张相关图像的推理能力。为了加强图像与文本之间的联系，该研究提出了一种上下文方案，将交错的图像-文本数据转换为统一格式。
* **Jeong**：发现当插入少量不连贯的图像或文本作为噪声时，MLLMs 可能会被误导，从而给出与上下文不一致的回答。基于这一观察，该研究提出了一种预过滤方法，用于去除无关上下文，从而促进更连贯的响应。

### 应用
M-ICL主要应用于两种场景：
* **解决各种视觉推理任务**：通常涉及从少量特定任务的示例中学习，并将所学知识推广到一个新的但类似的提问。通过指令和示范提供的信息，LLMs 能够理解任务的目标以及输出的模板格式，最终生成预期的答案。
* **教会 LLMs 使用外部工具**：相比之下，工具使用的示例更为精细，通常包含一系列可按顺序执行的步骤以完成任务。因此，第二种场景与思维链密切相关。

## 多模态思维链（Multimodal Chain of Thought）

CoT 的核心思想是提示 LLMs 不仅输出最终答案，还输出得出该**答案的推理过程**，类似于人类的认知过程。

### 获取思维链的学习范式
获取 M-CoT 能力主要有三种方式：微调、无训练的少样本/零样本学习。这三种方式对样本量的需求依次递减。

**直观来看，微调方法通常涉及为 M-CoT 学习构建特定的数据集。
**Lu 等人 [116] 构建了一个包含讲座和解释的科学问答数据集 ScienceQA，这些内容可以作为学习 CoT 推理的来源，并在此数据集上对模型进行微调。
Multimodal-CoT [185] 同样使用 ScienceQA 基准，但以两步方式生成输出，即先生成推理依据（推理步骤链），再基于此生成最终答案。
CoT-PT [187] 通过结合提示微调和步骤特定的视觉偏差来学习隐式的推理链。

与微调相比，少样本/零样本学习在计算效率上更具优势。两者的主要区别在于，少样本学习通常需要手工设计一些上下文示例，以便模型能够更容易地逐步学习推理过程。相比之下，零样本学习不需要任何特定的 CoT 学习示例。在这种情况下，模型通过设计的指令（如“让我们逐帧思考”或“这两帧之间发生了什么”[184], [186]）提示，利用嵌入的知识和推理能力进行学习。类似地，一些研究 [22], [188] 提示模型结合任务描述和工具使用，将复杂任务分解为子任务。

7.2.2 思维链配置
结构和长度是推理链的两个关键方面。

就结构而言，当前的方法可以分为单链和树形方法。使用单链进行推理是各种方法中广泛使用的范式 [116], [185]。具体而言，逐步推理过程形成一个单一的问题-理由-答案链。最近，一些方法探索了使用更复杂的方案，即树形链进行推理。具体来说，DDCoT [189] 将一个问题分解为多个子问题，每个子问题由LLM自身或视觉专家解决以生成理由。然后，LLM聚合这些理由并进行推理以形成最终答案。

就链的长度而言，可以分为自适应和预定义两种形式。前者配置要求LLM自行决定何时停止推理链 [22], [116], [169], [170], [185], [188]，而后者设置则以预定义的长度停止推理链 [79], [184], [186], [187]。

7.2.3 生成模式
链的构建方式是一个值得研究的问题。我们将当前的工作总结为（1）基于填充的模式和（2）基于预测的模式。

基于填充的模式要求根据上下文（前序和后序步骤）推断步骤以填补逻辑空白[184], [186]。

基于预测的模式需要在给定条件（如指令和先前的推理历史）下扩展推理链[22], [116], [169], [170], [185], [188]。这两种模式都要求生成的步骤必须一致且正确。





本文将最近的代表性 MLLM 分为四种主要类型：多模态指令调整 (MIT)、多模态上下文学习 (M-ICL)、多模态思维链 (M-CoT) 和 [LLM 辅助视觉推理](https://zhida.zhihu.com/search?content_id=230768050&content_type=Article&match_order=1&q=LLM+%E8%BE%85%E5%8A%A9%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86&zhida_source=entity) (LAVR)。

# MIT（Multimodal Instruction Tuning）

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503112035847.png)

Instruction Tuning（指令调优）是一种涉及在指令格式数据集集合上微调预先训练的llm的技术。通过这种方式进行调整，LLM 可以通过遵循新指令泛化到看不见的任务，从而提高零样本性能。Instruction Tuning学习如何泛化到未知的任务，而不是像一对一的拟合特定任务。

M-IT中校准预训练的一种常见方法是保持预训练模块(例如visual encoder和llm)冻结，仅训练一个可学习的接口。

现有VQA和标题数据集的答案通常很简洁，直接使用这些数据集进行指令调优可能会限制MLLM的输出长度。解决这个问题有两种常见的策略：第一个是修改instruction，直接告诉LLM我们要简短、单句。第二种方法是延长现有答案的长度。

Self-Instruction：尽管现有的基准数据集可以提供丰富的数据源，但它们通常不能很好地满足现实场景中的人类需求，例如多轮对话。为了解决这个问题，一些作品通过自我指导来收集样本[60]，这引导llm使用一些手工注释的样本来生成遵循文本指令的数据。

混合指令调优不比多模态数据上单独调优差。

Learnable Interface：当冻结预训练模型的参数时，可学习接口负责连接不同的模态。挑战在于如何有效地将视觉内容翻译成LLM可以理解的文本。一种常见且可行的解决方案是利用一组可学习的query token以基于查询的方式提取信息。此外，一些方法使用基于projection head的桥接方法来缩小模态差距。

Expert Model：借助专家模型来简化视觉信息更简单，但是灵活性很差，而且会存在信息丢失的情况。

# MICL（Multimodal In-Context Learning）
