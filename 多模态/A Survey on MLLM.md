博客推荐：[A Survey on Multimodal Large Language Models-全文解读 - 知乎](https://zhuanlan.zhihu.com/p/641866192)
[【综述论文阅读】A Survey on Multimodal Large Language Models 上_prompt-based ensemble expert language models with -CSDN博客](https://blog.csdn.net/weixin_46231495/article/details/145903655?ops_request_misc=&request_id=&biz_id=102&utm_term=a%20survey%20on%20multimodal%20large%20l&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-145903655.142^v102^pc_search_result_base9&spm=1018.2226.3001.4187)


# 架构

典型架构抽象为三个模块，即预训练的**多模态编码器、预训练的大语言模型**以及连接它们的**多模态接口**。

## 编码器

选择编码器时，需要考虑**分辨率、参数量**和**预训练语料库**等因素。分辨率的重要性最大。

**扩展输入分辨率的方法：**
* 直接缩放：**将更高分辨率的图像输入到编码器中**，通常需要进一步调整编码器或替换为更高分辨率的预训练编码器。
* 分块：**高分辨率图像切割成小块，并重用低分辨率编码器**。

## 预训练LLM

**常用公开可用LLM**：
* **FlanT5 系列**：是较早的 LLM，被用于类似 BLIP-2 和 InstructBLIP 的工作中。
* **LLaMA 系列 和 Vicuna 家族**：是具有代表性的开源 LLM，吸引了大量学术界的关注。由于这两类 LLM 主要基于英文语料库进行预训练，因此在多语言支持方面存在局限性，例如对中文的支持较为有限。
* **Qwen**： 是一款双语 LLM，能够很好地支持中文和英文。

扩大 LLM 的参数规模能带来额外的收益，类似于提高输入分辨率的情况。也有一些研究专注于使用较小的 LLM，以方便在移动设备上部署。关于 LLM 的专家混合（MoE, Mixture of Experts）架构的研究逐渐受到关注，与密集模型相比，稀疏架构通过选择性激活参数，能够在不增加计算成本的情况下扩展总参数规模。

## 模态接口

**两种实现模态接口的方法：**
* 在预训练的视觉编码器和LLM之间引入一个**可学习的连接器**。
* 通过**专家模型将图像翻译成语言**，然后将语言发送给LLM。

### 可学习连接器

该模块**将信息投影到LLM能够高效理解的空间中**。

实现这种接口的方法大致有两种：
* **token级融合** ：编码器输出的特征被转换为token，并在发送到LLM之前与文本token连接。
	* **利用一组可学习的查询token以基于查询的方式提取信息**：如BLIP-2。
	* 仅使用**基于MLP的接口**来弥合模态差距：如LLaVA系列。
* **特征级融合**：**通过插入额外的模块**来实现文本特征与视觉特征之间的深度交互和融合。例如：
	* Flamingo 在冻结的 LLM Transformer 层之间插入了额外的**交叉注意力层**，从而利用外部视觉线索增强语言特征。
	* CogVLM 在每个 Transformer 层中插入了一个**视觉专家模块**，以实现视觉特征与语言特征之间的双向交互和融合。为了获得更好的性能，所引入模块的 QKV 权重矩阵初始化自预训练的 LLM。
	* LLaMA-Adapter 在 Transformer 层中引入了**可学习的提示（prompt）**。这些提示首先嵌入了视觉知识，然后作为前缀与文本特征连接。

对于token级融合，模态适配器的类型远不如**视觉token的数量**和**输入分辨率**重要。

### 专家模型

**将多模态输入转换为语言表示，而无需额外训练**。LLM可以通过转换后的语言来理解多模态信息。例如，VideoChat-Text 使用预训练的视觉模型提取动作等视觉信息，并通过语音识别模型丰富描述内容。尽管使用专家模型的方法较为直接，但它可能不如采用可学习接口那样灵活。将其他模态的信息转换为文本会导致信息丢失。

# 训练策略与数据

一个完整的多模态大型语言模型（MLLM）需要经历三个训练阶段，即**预训练**、**指令微调**和**对齐优化**。每个训练阶段都需要不同类型的训练数据，并实现不同的目标。

## 预训练

目标是**对齐不同模态**并**学习多模态世界知识**，通常需要**大规模**的文本配对数据。
### 训练细节

给定一张图像，模型被训练以**自回归**的方式预测该图像的描述（caption），采用标准的交叉熵损失函数进行优化。一种常见的预训练方法是**冻结预训练模块**（例如视觉编码器和 LLM），然后训练一个**可学习的接口**，其核心思想是在不丢失预训练知识的情况下对齐不同模态。 一些方法还会**解冻更多模块**（例如视觉编码器），以增加可用于对齐的可训练参数。

训练方案与**数据质量**密切相关。对于**短且噪声较大**的描述数据，可以采用**较低分辨率**（例如 224）来加速训练过程；而对于**较长且较为干净**的数据，则更适合使用**更高分辨率**（例如 448 或更高）以减少幻觉现象的发生。

### 数据

类型：
* 粗粒度数据：
	* 样本通常来源于互联网爬取。
	* **数据量大**
	* **短且噪声较大
* 细粒度数据生成：
	* 通过**提示强大的 MLLM**（如 GPT-4V）生成高质量的细粒度数据。
	* 包含更长且更准确的图像描述，从而实现图像与文本模态之间更精细的对齐。
	* 需要调用商业用途的 MLLM，成本较高，数据量相对较小。
	* *ShareGPT4V [83] 通过一种折中方法实现了高效的数据生成：首先使用 GPT-4V 生成的 10 万条数据训练一个描述生成模型，然后利用该预训练模型扩展数据量至 120 万条。*
## 指令微调

**指令（Instruction）** 是指任务的描述。指令微调的目标是**教会模型更好地理解用户的指令并完成所要求的任务**。

* **监督微调（Supervised Fine-tuning）**：通常**需要大量的任务特定数据**来训练一个**任务特定**的模型。
* **提示（Prompting）**：减少了对大规模数据的依赖，并可以通过提示工程完成**特定任务**。在这种情况下，尽管**少样本性能有所提高**，但零样本性能仍然较为普通。
* **指令微调（Instruction Tuning）**：不同于上述两种方法，指令微调专注于学习如何**泛化到未见过的任务**，而不是像前两者那样适应特定任务。此外，指令微调与多任务提示（multi-task prompting）密切相关。

### 训练细节

形式上，一个多模态指令样本可以用三元组的形式表示，即 ((I, M, R))，其中 (I)、(M)、(R) 分别代表**指令**、**多模态输入**和**真实响应**。多模态大型语言模型（MLLM）根据指令和多模态输入预测答案：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503131859242.png)

这里，(A) 表示预测的答案，(θ) 是模型的参数。训练目标通常是用于训练大型语言模型（LLM）的原始自回归目标，在此基础上，MLLM 被鼓励预测响应中的下一个标记。该目标可以表示为：

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503131900722.png)

其中，(N) 是真实响应的长度。

### 数据收集

有三种典型的规模化获取指令数据的方法：
* **数据适配**：利用现有的高质量的特定任务数据集来构建指令格式化的数据集。
* **自我指令生成**：利用 LLM 根据少量人工标注的样本生成文本指令数据。
* **数据混合**：多模态指令数据和仅语言数据的混合，包括混合指令微调（将两种类型的数据组合并随机打乱）和顺序指令微调（先用文本数据再用多模态数据）。

### 数据质量

提升数据质量的两个重要方面：
* **提示多样性**：多样化的提示有助于提升模型性能和泛化能力。
* **任务覆盖范围**：关于训练数据中涉及的任务类型，Du 等人进行了一项实证研究，发现视觉推理任务对于提升模型性能的效果优于图像描述和问答任务。此外，该研究建议，增强指令的复杂性可能比增加任务多样性或引入细粒度的空间注释更有益。

## 对齐微调

对齐微调更常用于**让模型与特定人类偏好对齐**的场景，例如减少幻觉的回答。目前，**基于人类反馈的强化学习（RLHF）** 和**直接偏好优化（DPO）** 是对齐调优的两种主要技术。

### RLHF
利用强化学习算法将LLMs与人类偏好对齐，并以人类标注作为训练循环中的监督。如InstructGPT所示，RLHF包含三个关键步骤：
1. 监督微调：微调预训练模型，以呈现初步期望的输出行为。在RLHF设置中，微调后的模型称为策略模型。*此步骤可能会被跳过，因为监督策略模型πSFT可以从指令微调模型初始化。*
2. 奖励建模（Reward Modeling）
使用偏好对训练奖励模型(r_θ) 。给定一个多模态提示（如图像和文本）(x) 和一对回答 ((y_w, y_l))，奖励模型 (r_θ) 学习为更优的回答 (y_w) 分配更高的奖励，而为次优回答 (y_l) 分配较低的奖励。其目标函数如下： 
其中，(D = {(x, y_w, y_l)}) 是由人工标注者标记的比较数据集。实际上，奖励模型 (r_θ) 的结构通常与策略模型类似。

3、强化学习（Reinforcement Learning）
采用近端策略优化（PPO）算法优化强化学习策略模型 (πRL)。为了防止策略偏离原始模型过远，通常在训练目标中添加逐token的 KL 惩罚项 [95]，最终目标函数如下： 
其中，(β) 是 KL 惩罚项的系数。通常，强化学习策略 (πRL) 和参考模型 (πREF) 都从监督模型 (πSFT) 初始化。通过这一调整过程，获得的强化学习策略模型应能更好地与人类偏好对齐。

DPO
DPO 使用简单的二分类损失从人类偏好标签中学习。与基于 PPO 的 RLHF 算法相比，DPO 不需要显式学习奖励模型，从而简化了整个流程为两个步骤：人类偏好数据收集和偏好学习。其学习目标如下： 

RLHF-V [114] 通过修正模型响应中的幻觉，生成细粒度（片段级）偏好数据对，并使用这些数据执行密集的 DPO。
Silkie [115] 则通过提示 GPT-4V 收集偏好数据，并通过 DPO 将偏好监督蒸馏到指令微调模型中。





本文将最近的代表性 MLLM 分为四种主要类型：多模态指令调整 (MIT)、多模态上下文学习 (M-ICL)、多模态思维链 (M-CoT) 和 [LLM 辅助视觉推理](https://zhida.zhihu.com/search?content_id=230768050&content_type=Article&match_order=1&q=LLM+%E8%BE%85%E5%8A%A9%E8%A7%86%E8%A7%89%E6%8E%A8%E7%90%86&zhida_source=entity) (LAVR)。

# MIT（Multimodal Instruction Tuning）

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503112035847.png)

Instruction Tuning（指令调优）是一种涉及在指令格式数据集集合上微调预先训练的llm的技术。通过这种方式进行调整，LLM 可以通过遵循新指令泛化到看不见的任务，从而提高零样本性能。Instruction Tuning学习如何泛化到未知的任务，而不是像一对一的拟合特定任务。

M-IT中校准预训练的一种常见方法是保持预训练模块(例如visual encoder和llm)冻结，仅训练一个可学习的接口。

现有VQA和标题数据集的答案通常很简洁，直接使用这些数据集进行指令调优可能会限制MLLM的输出长度。解决这个问题有两种常见的策略：第一个是修改instruction，直接告诉LLM我们要简短、单句。第二种方法是延长现有答案的长度。

Self-Instruction：尽管现有的基准数据集可以提供丰富的数据源，但它们通常不能很好地满足现实场景中的人类需求，例如多轮对话。为了解决这个问题，一些作品通过自我指导来收集样本[60]，这引导llm使用一些手工注释的样本来生成遵循文本指令的数据。

混合指令调优不比多模态数据上单独调优差。

Learnable Interface：当冻结预训练模型的参数时，可学习接口负责连接不同的模态。挑战在于如何有效地将视觉内容翻译成LLM可以理解的文本。一种常见且可行的解决方案是利用一组可学习的query token以基于查询的方式提取信息。此外，一些方法使用基于projection head的桥接方法来缩小模态差距。

Expert Model：借助专家模型来简化视觉信息更简单，但是灵活性很差，而且会存在信息丢失的情况。

# MICL（Multimodal In-Context Learning）
