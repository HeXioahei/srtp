VLA（Vision‑Language‑Action，视觉‑语言‑动作模型）是一类 **多模态深度学习模型**，它在同一个网络中同时具备三种能力：

1. **视觉感知**：处理摄像头、雷达等传感器采集的图像或视频，提取环境的空间信息。  
2. **语言理解**：解析自然语言指令或描述，获取任务意图。  
3. **动作生成**：将视觉与语言的理解结果直接映射为机器人或自动驾驶系统的低层控制指令（如关节运动、轨迹规划、车辆转向等）。

VLA 的核心思想是把 **视觉‑语言模型（VLM）** 与 **端到端控制模型** 融合，实现从 **感知 → 语义 → 行动** 的闭环。它最早由 Google DeepMind 在 2023 年的 RT‑2（Robotic Transformer‑2）中提出，随后在具身智能、机器人、智能驾驶等领域快速扩展。  

**主要特征**  
- **多模态融合**：在特征层面或决策层面同时利用视觉特征和语言特征。  
- **端到端训练**：模型直接输出动作序列，省去传统的感知‑规划‑控制分层流水线。  
- **通用性**：同一模型可适配不同任务，只需在相应的多模态数据上微调。  

**应用场景**  
- 机器人执行自然语言指令（如“把杯子放到桌子上”）  
- 自动驾驶系统在接受语音/文字指令的同时进行感知与决策  
- 家庭助手机器人、工业协作机器人、交互式游戏角色等  

因此，VLA 代表了深度学习向 **具身智能** 迈进的关键技术范式，旨在让 AI 能够像人类一样同时“看、听、说、做”。