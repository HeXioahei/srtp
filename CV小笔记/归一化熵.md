归一化熵（Normalized Entropy）是对信息熵进行尺度化处理后得到的度量，用来衡量概率分布或离散随机变量的不确定性程度，并将其值限定在一个统一的区间（通常是 0 到 1 或 0 %到 100 %），便于不同系统或不同尺度之间的比较。

常见的定义形式有：

$$
\eta(\mathbf{p}) = -\frac{\displaystyle\sum_{i=1}^{k} p_i \log p_i}{\log k}
$$

其中  
- $\mathbf{p} = (p_1, p_2, \dots, p_k)$ 为离散概率向量，满足 $\sum_i p_i = 1$；  
- $k$ 为可能取值的总数（即事件的类别数）；  
- 分子是香农熵 $H(\mathbf{p})$，分母 $\log k$ 是该分布在均匀情况下的最大熵。

这样得到的 $\eta(\mathbf{p})$ 取值范围为 $[0,1]$。  
- 当分布极度确定（如某一 $p_i=1$，其余为 0）时，熵为 0，归一化熵也为 0，表示没有不确定性。  
- 当分布完全均匀（所有 $p_i = 1/k$）时，熵达到最大值 $\log k$，归一化熵为 1，表示不确定性最高。

在实际应用中，归一化熵常用于：

1. **模型置信度评估**：在深度学习的分支网络或边缘推理中，归一化熵接近 0 表示模型对当前样本预测有较高置信度，接近 1 则表示预测不可靠[[1]]。  
2. **复杂系统分析**：用于衡量时间序列、网络匿名程度、交通出行分布等的均衡性或混沌程度，数值越大说明系统状态越均匀或不确定性越高[[2]][[3]]。  
3. **信息论度量**：在聚类、互信息等评价指标中，将熵归一化后可以得到 0‑1 区间的相似度或一致性度量，便于不同数据集之间的比较[[4]]。

简而言之，归一化熵是对香农熵的标准化，使其在统一尺度上反映随机性的相对大小，广泛用于信息论、机器学习、复杂系统等领域的定量分析。