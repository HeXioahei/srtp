[归纳偏置](https://zhida.zhihu.com/search?content_id=747007945&content_type=Answer&match_order=1&q=%E5%BD%92%E7%BA%B3%E5%81%8F%E7%BD%AE&zhida_source=entity)说白了就是算法的"偏见"——面对新数据时，它倾向于做什么假设。这个概念很关键，比如说前几年的[ViT](https://zhida.zhihu.com/search?content_id=747007945&content_type=Answer&match_order=1&q=ViT&zhida_source=entity)和[CNN](https://zhida.zhihu.com/search?content_id=747007945&content_type=Answer&match_order=1&q=CNN&zhida_source=entity)之争，直接决定了为什么CNN和ViT在不同场景下表现差这么多。

## 什么是归纳偏置

机器学习模型本质上是在做模式匹配，但训练数据永远是有限的。归纳偏置就是模型的"先入为主"，帮它在数据不够的时候也能做出合理推测。

比如人看到几只鸟就能认出新的鸟，因为我们天生假设"有羽毛会飞的大概率是鸟"。模型也需要这种内置假设来泛化。

关键在于：好的归纳偏置让模型学得快泛化好，不合适的偏置会限制模型潜力。

## CNN的强归纳偏置

CNN骨子里就是为视觉任务设计的，内置了几个硬编码的假设：

**局部连接性**：卷积核只看局部区域，默认相邻像素更相关。这个假设在绝大多数视觉任务中都成立——边缘、纹理这些基础特征确实是局部的。

**平移等变性**：同一个特征（比如一只猫）出现在图像任何位置都应该被同样识别。weight sharing机制天然保证了这点。

**层次化特征提取**：从低层的线条边缘到高层的复杂模式，这种由简单到复杂的特征层次基本符合人类视觉系统的工作方式。

这些偏置在视觉领域非常有效，所以CNN用相对少的数据就能达到不错效果。

## ViT的弱归纳偏置

ViT本质上就是把图像切成patch然后当序列处理，架构上更通用但视觉特定的假设更少。

**缺乏空间偏置**：[self-attention](https://zhida.zhihu.com/search?content_id=747007945&content_type=Answer&match_order=1&q=self-attention&zhida_source=entity)机制让每个patch都能直接和其他任意patch交互，没有内置的"相邻更重要"假设。这意味着ViT需要从数据中学会什么是局部性，什么是空间关系。

**更通用的表征学习**：[transformer](https://zhida.zhihu.com/search?content_id=747007945&content_type=Answer&match_order=1&q=transformer&zhida_source=entity)本来就是为NLP设计的，套用到视觉上时没有太多视觉特定的先验知识。

最新研究显示，ViT的归纳偏置确实比CNN弱很多，特别是在空间结构理解方面。

## 实际性能差异

这个差异在实践中的表现很明显：

**小数据集场景**：CNN表现明显更好。内置的视觉偏置让它能用更少数据学到有效特征。ViT在小数据集上容易过拟合，需要大量数据增强和正则化才能勉强跟上。

**大数据集场景**：数据足够多时，ViT开始发威。它能从海量数据中学习到比CNN内置假设更精细的模式。Google在ImageNet-21K这种大规模数据集上的实验就证明了这点。

**训练成本**：ViT需要的计算资源和数据量都比CNN大得多，这就是为什么ViT火了这么久，实际部署还是以CNN为主的原因。

从更深层看，这反映了machine learning中一个根本权衡：强归纳偏置提供快速学习和数据效率，但可能限制模型表达能力；弱归纳偏置理论上表达能力更强，但需要更多数据和计算来发挥优势。

所以可以看到目前来说就是训练数据越来越大，而基本上没有见到CNN的新架构了，因为数据量到了ViT就体现出来了。