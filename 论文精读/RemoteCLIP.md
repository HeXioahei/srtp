# 博客地址

[遥感论文 | Arxiv | RemoteCLIP：针对遥感的视觉-语言大模型来了！代码已开源！ - 知乎](https://zhuanlan.zhihu.com/p/683151563)

# 摘要
存在的问题：主要学习低级特征，需要注释数据进行微调；缺乏语言理解，不适用于线性检测和零样本。
解决办法：通过丰富的语义学和对齐的文本嵌入来学习稳健的视觉特征。
存在的问题：预训练数据稀缺。
解决办法：利用数据缩放，该缩放将异构注释转换为基于框到字幕（B2C）和掩码到框（M2B）转换的统一图像-字幕数据格式。

# 引言
foundation模型越来越重要，提倡“一举多得”的通用模型。遥感界也是如此，受SSL启发，主流方法MIM已应用于多个模型。但MIM学习低级特征，缺乏语义学，更喜欢学习高频纹理特征而不是捕获全局信息。作者的目标是学习具有丰富的卫星想象视觉概念语义学的鲁棒视觉特征，同时学习与视觉特征很好地对齐的文本嵌入。但这需要大量的数据。作者将异构注释（包括目标检测边界框和语义分割图）转换为基于M2C和B2C生成策略的统一图像字幕数据格式，还结合了无人机（UAV）图像，以进一步增强预训练数据的多样性。

**主要贡献**：
* 遥感领域的大规模数据集
* 一种新的遥感视觉语言基础模型
* 遥感的多种下游应用

# 相关工作

遥感自监督基础模型：对比学习、生成学习
遥感视觉语言模型：遥感图像-文本检索模型、基于CLIP的模型

# RemoteCLIP模型

CLIP模型，在从十亿规模的图像-文本对中挖掘的跨模态监督下学习将语义相似样本的表示组合和对齐在一起。其优化了一个简单的InfoNCE损失函数，使配对的图像-文本样本对齐，并将不匹配的样本推开。

![image.png](https://youki-1330066034.cos.ap-guangzhou.myqcloud.com/machine-learning/202503180024407.png)

LInfoNCE用来实现对表示的对齐与分组。

大型CLIP也是遥感任务的强大模型：
* 大规模对比图像文本预训练产生了适用于遥感领域的高质量视觉表示。
* 结合大规模模型和大规模预训练数据显然优于复杂的网络结构或采用多种损失函数。

小型CLIP上的持续预训练进一步提高了性能：


# 思路流程

首先开发了一个数据扩展的流程，然后在扩展的数据集上调整CLIP模型，结果得到RemoteCLIP模型，其在下游任务上表现出了优越的结果。



---


# Box-to-Caption

Box-to-Caption是指一种将边界框注释转换为自然语言标题的方法，用于生成用于训练视觉-语言模型的数据。具体来说， **Box-to-Caption（B2C）** 是指将物体检测数据集中标注有边界的图片转换成带有描述性的文本标题的过程。这个过程通常涉及以下几个步骤：

- 首先，系统会根据给定的边界框（即标注出的物体位置）来定位图片中的特定对象。
- 然后，系统依据这些边界框的位置及其对应的类别标签，自动生成一系列描述该对象的文本片段。 
- 最终生成的每一幅图片都会有一个或多个与其包含的对象相对应的文字描述，从而形成“图像-标题”的配对数据集。

这一技术的关键在于如何有效地将视觉信息（边框和物体位置）转化为人类易于理解的语言表达，以便机器学习算法能更好地捕捉到跨模态之间的关联。这种方法不仅有助于提高视觉-语言模型的理解能力和泛化能力，也为处理复杂和高维度的空间数据提供了新的思路。通过对大量数据的迭代学习，Box-to-Caption生成的标题能够反映出图片中物体的存在、位置及属性等多种细节，使得模型能在未来的任务中更加精准地完成诸如物体检测、场景理解甚至语言生成等工作。



# Mask-to-Box

Mask-to-Box是指在论文《RemoteCLIP: A Vision Language Foundation Model for Remote Sensing》中提出的一种数据转换方法。具体来说，它是一种将语义分割掩模转换为边界框标注的技术，以便于在视觉-语言模型训练中统一处理不同类型的标注数据。Mask-to-Box方法的工作原理如下：

- **输入**：语义分割掩模，即一个或多个二值图像，其中目标区域被标记为1，背景区域被标记为0。

- **输出**：边界框（BBox），即每个目标区域的矩形边界坐标（x_min, y_min, x_max, y_max）。

这个转换过程允许模型从语义分割数据中学习到目标的精确位置，同时保持了边界框数据的简单性和高效性。这种转换有助于提高模型在遥感图像中的目标检测和语义理解能力。在《RemoteCLIP》中，Mask-to-Box转换与Box-to-Caption（B2C）转换一起，用于将异构的遥感数据集转换为统一的图像-标题格式，从而丰富了预训练数据集的内容，提高了模型的鲁棒性和泛化能力。



# MIM

MIM是指Masked Image Modeling（掩码图像建模）。具体来说，MIM是一种通过在图像中随机遮盖部分像素并让模型预测被遮盖部分的像素值来训练视觉模型的方法。这种方法旨在通过学习图像中缺失部分的上下文信息来增强模型的表征能力。然而，在原文中提到，基于MIM的遥感基础模型存在两个关键限制：首先，遥感图像的俯视角度使得遮挡不变性变得不那么必要，因此MIM中的遮挡机制可能不是最佳的。其次，理论和实证研究表明，MIM倾向于学习低级特征，缺乏高级语义，这在高级语义识别任务中表现不佳，尤其是在线性探测和少样本学习设置中。此外，MIM方法倾向于学习高频纹理特征，而不是捕捉长距离全局模式，这限制了模型的性能和鲁棒性。因此，MIM虽然是一种有效的图像建模方法，但在遥感应用中可能需要进一步的改进或替代方案来更好地捕捉高级语义信息。



# Suzuki-Abe算法

Suzuki的边界跟踪算法（通常称为Suzuki-Abe算法）是一种用于二值图像中轮廓提取的高效方法，能够识别并分层组织轮廓的内外关系。以下是该算法的关键点总结：

### **核心思想**

- **拓扑结构分析**：在跟踪轮廓的同时，建立轮廓之间的层级关系（父子关系），区分外轮廓（物体外围）和内轮廓（孔洞）。
- **光栅扫描策略**：按光栅顺序（从左到右、从上到下）扫描图像，触发新轮廓的跟踪。

---

### **算法步骤**

1. **初始化**：
   
   - 标记所有像素为未访问状态。
   - 按光栅顺序扫描图像，寻找未访问的前景像素（物体像素）。

2. **触发轮廓跟踪**：
   
   - 发现未访问的前景像素时，判断其是否为**新外轮廓起点**（如左侧或上侧为背景）。
   - 若为内轮廓起点（如被外轮廓包围），则开始跟踪内轮廓。

3. **边界跟踪**：
   
   - 沿顺时针或逆时针方向（通常为顺时针）搜索邻域，找到下一个边界点。
   - 使用**八邻域**或**四邻域**定义（常见为四邻域跟踪，八邻域连接）。
   - 记录轮廓点并标记为已访问，同时避免重复处理。

4. **区分内外轮廓**：
   
   - **外轮廓**：物体最外层边界，父轮廓为`NULL`。
   - **内轮廓**（孔洞）：被外轮廓包围，父轮廓指向其直接外层轮廓。
   - 通过扫描时的上下文（如周围像素状态）判断轮廓类型。

5. **层级关系构建**：
   
   - 为每个内轮廓分配父轮廓，形成树状结构（如OpenCV中的`hierarchy`数组）。

---

### **关键特性**

- **层级管理**：支持多级嵌套（如孔洞中的孔洞）。
- **高效性**：通过单次扫描和标记避免重复计算。
- **拓扑完整性**：保留轮廓的闭合性和相对位置关系。

---

### **应用场景**

- **图像分割**：提取物体轮廓用于形状分析。
- **OCR**：定位文字区域。
- **医学图像处理**：识别器官或病变区域的边界。
- **OpenCV实现**：`cv2.findContours()`函数采用此算法（`mode=cv2.RETR_TREE`或`cv2.RETR_CCOMP`时）。

---

### **示例（OpenCV）**

```python
import cv2

# 读取二值图像
image = cv2.imread("binary_image.png", 0)

# 应用Suzuki算法提取轮廓及层级
contours, hierarchy = cv2.findContours(
    image, 
    mode=cv2.RETR_TREE,  # 提取所有轮廓并构建层级树
    method=cv2.CHAIN_APPROX_SIMPLE
)
```

---

### **与其他算法的对比**

| 特性        | Suzuki-Abe算法 | Moore边界跟踪 | 平方追踪算法 |
| --------- | ------------ | --------- | ------ |
| **层级支持**  | 是（内外轮廓）      | 否         | 否      |
| **处理孔洞**  | 是            | 是（需额外处理）  | 否      |
| **时间复杂度** | O(N)（N为像素数）  | O(N)      | O(N)   |
| **实现复杂度** | 较高（需管理层级）    | 中等        | 简单     |

---

通过结合光栅扫描和层级管理，Suzuki的算法在复杂图像中高效提取轮廓结构，成为图像处理中的经典方法。



# p-Hash

好的！我来逐步拆解并解释这段关于**样本去重（Sample De-duplication）**的技术细节，结合其核心思想和实际应用场景，尽量用通俗的语言和例子帮助你理解。

---

### **核心问题：为什么需要去重？**

在训练大模型（如RemoteCLIP）时，如果数据集中存在重复样本（例如同一张图片出现在多个来源的数据集中），会导致两个问题：

1. **测试集污染（Test Set Contamination）**：如果训练数据中混入了测试集中的图片，模型会在训练时“记住”这些测试样本，导致评估结果虚高（不真实的高准确率）。
2. **模型过拟合**：重复样本会让模型过度关注这些数据，降低泛化能力。

因此，去重的目标是**剔除数据集中重复或高度相似的图像**，确保模型训练的公平性和有效性。

---

### **基于p-Hash的分块局部检测方法**

以下是具体步骤的详细解释：

#### **1. 生成p-Hash值**

- **p-Hash（感知哈希）**：一种图像指纹生成算法，能够将图像内容映射为一个固定长度的哈希值（如64位二进制字符串）。**关键特性**是：相似图像的哈希值汉明距离（Hamming Distance）小，不同图像的汉明距离大。
  - **示例**：假设两张图片的p-Hash分别为 `10101010` 和 `10101011`，它们的汉明距离为1（仅最后一位不同），说明它们几乎相同。

#### **2. 分块（Segmentation）**

- **分块策略**：将每个p-Hash值均匀分割成 **N个段**（例如64位哈希值分成4段，每段16位）。
  - **目的**：加速重复检测。直接比较所有图像的两两哈希值计算量太大（复杂度O(N²)），分块后可以**局部比较**，减少计算量。
  - **示例**：若哈希值为64位，分成4段后，每段为16位，建立4个字典，每个字典的键是段的位置（如段1、段2等），值是该段对应的所有哈希值片段。

#### **3. 建立字典加速搜索**

- **字典结构**：创建N个字典（对应N个分块），每个字典存储：
  - **键（Key）**：段的索引（如第1段、第2段）。
  - **值（Value）**：所有图像在该段的哈希值片段，以及对应的图像ID。
- **作用**：通过分块字典，只需比较**相同段内**的哈希值片段，避免全局比较。

#### **4. 计算汉明距离**

- **汉明距离**：两个等长字符串在相同位置上不同字符的数量。例如：
  - `1010` vs `1001` → 汉明距离为2。
- **阈值设定**：若两图像的汉明距离**小于2**，则认为它们是重复的。这是经验性阈值：
  - **阈值过大（如>2）**：可能将不相似的图像误判为重复（过度去重）。
  - **阈值过小（如<2）**：可能漏掉真实重复（去重不足）。

#### **5. 去重流程**

1. **遍历所有字典**：对每个分块段，检查该段内所有哈希值片段。
2. **局部匹配**：在同一段内，计算每对哈希值的汉明距离。
3. **标记重复**：若某对哈希值的距离<2，则标记对应图像为重复。
4. **全局去重**：合并所有段的重复标记结果，最终删除重复图像。

---

### **关键细节解析**

#### **为什么分块？**

- **计算效率**：假设有100万张图片，两两比较需要约5×10¹¹次计算。分块后，只需在局部段内比较，计算量大幅降低（例如分4段后，每段只需比较25万张图片的局部片段）。
- **并行化**：不同分块可以并行处理，加速去重。

#### **为什么用p-Hash而不是普通哈希？**

- **普通哈希（如MD5）**：对文件内容敏感，即使图像内容相同但格式不同（如JPEG vs PNG），哈希值也会完全不同。
- **p-Hash**：基于图像内容生成，对缩放、格式转换、轻微压缩等操作鲁棒，更适合图像去重。

#### **为什么不同数据集移除的重复样本数量差异大？**

- **数据来源差异**：某些数据集可能本身重复率高（如爬虫抓取的网络图片），而另一些数据集更干净。
- **阈值敏感性**：汉明距离阈值设为2是经验值，可能对不同类型数据集的敏感度不同。

---

### **实例辅助理解**

假设有以下3张图片的p-Hash值（简化为8位）：

- 图片A: `10101010`
- 图片B: `10101011`（与A仅最后一位不同）
- 图片C: `11110000`

**分块策略**：将8位哈希值分为2段（每段4位）：

- 段1（前4位）：图片A=`1010`, 图片B=`1010`, 图片C=`1111`
- 段2（后4位）：图片A=`1010`, 图片B=`1011`, 图片C=`0000`

**字典建立**：

- 字典1（段1）：
  - 键=`1010` → 值=[图片A, 图片B]
  - 键=`1111` → 值=[图片C]
- 字典2（段2）：
  - 键=`1010` → 值=[图片A]
  - 键=`1011` → 值=[图片B]
  - 键=`0000` → 值=[图片C]

**比较过程**：

- 在字典1中，键`1010`对应的图片A和B会被比较，计算汉明距离为1（小于2），标记为重复。
- 图片C与其他图片在段1或段2中均不匹配。

**结果**：删除图片B，保留图片A和C。

---

### **总结**

- **核心目标**：高效、准确地剔除重复图像，防止模型过拟合和测试集污染。
- **技术亮点**：p-Hash感知相似性 + 分块字典加速搜索 + 汉明距离阈值控制精度。
- **实际效果**：根据数据集的重复程度，移除40到3000个不等的重复样本。

如果有其他疑问（例如p-Hash具体算法、如何选择分块数N等），可以进一步探讨！



# AMP

自动混合精度（Automatic Mixed Precision，AMP）是一种优化深度学习模型训练的技术，通过智能结合**单精度（FP32）**和**半精度（FP16）**计算，在保持模型精度的同时，显著提升训练速度并减少内存占用。以下是其核心原理和应用详解：

---

### **1. 为什么需要混合精度？**

- **半精度（FP16）的优势**：
  - **内存减半**：FP16占用的内存是FP32的一半，允许更大的批次（Batch Size）或更复杂的模型。
  - **计算加速**：现代GPU（如NVIDIA Volta+架构的Tensor Core）针对FP16计算优化，吞吐量是FP32的2-8倍。
- **FP16的缺陷**：
  - **数值范围小**：FP16的表示范围（~5.96×10⁻⁸ 到 6.55×10⁴）远小于FP32（~1.4×10⁻⁴⁵ 到 3.4×10³⁸），可能导致梯度下溢（接近零）或溢出（超出范围）。
  - **精度损失**：某些计算（如小数值累加）在FP16中误差较大。

**混合精度的核心思想**：  
在关键计算（如梯度更新）中使用FP32保证数值稳定性，其他计算（如矩阵乘法）使用FP16加速。

---

### **2. AMP的工作原理**

AMP通过以下步骤自动管理精度：

#### **(1) 操作类型自动分配**

- **FP16加速的操作**：矩阵乘法、卷积等计算密集型操作。
- **FP32保留的操作**：  
  需要高精度的操作（如Softmax、Logits计算）、梯度累加、参数更新等。

#### **(2) 动态损失缩放（Gradient Scaling）**

- **问题**：FP16的梯度可能过小（如1e-7），导致下溢变为零。
- **解决方案**：  
  在反向传播前，对损失值乘以一个缩放因子（如1024），放大梯度到FP16的有效范围内；反向传播后，再将梯度除以缩放因子恢复原量级。

#### **(3) 自动类型转换（Autocast）**

- 在计算过程中，AMP自动将输入数据转换为合适的精度。例如：
  
  ```python
  with torch.autocast(device_type='cuda', dtype=torch.float16):
      outputs = model(inputs)  # inputs自动转为FP16
      loss = loss_fn(outputs, labels)
  ```

---

### **3. AMP的优势**

- **训练速度提升**：利用FP16加速计算，吞吐量提高2-3倍。
- **内存占用降低**：FP16减少显存需求，支持更大批次或更大模型。
- **几乎无损精度**：通过动态损失缩放和关键操作保留FP32，最终模型精度与纯FP32训练接近。

---

### **4. 如何启用AMP（以PyTorch为例）**

PyTorch通过`torch.cuda.amp`模块实现AMP：

```python
import torch
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()  # 初始化梯度缩放器

for inputs, labels in data_loader:
    inputs = inputs.cuda()
    labels = labels.cuda()

    # 前向传播（自动混合精度）
    with autocast():
        outputs = model(inputs)
        loss = loss_fn(outputs, labels)

    # 反向传播与梯度缩放
    scaler.scale(loss).backward()  # 缩放损失后反向传播

    # 梯度更新（自动恢复缩放）
    scaler.step(optimizer)  # 先unscale梯度，再更新参数
    scaler.update()         # 调整缩放因子（动态调整）
    optimizer.zero_grad()
```

---

### **5. 注意事项**

- **兼容性检查**：确保GPU支持FP16加速（如NVIDIA Pascal及以上架构）。
- **避免手动类型转换**：AMP会自动处理，手动强制转换可能破坏优化。
- **监控梯度**：若出现NaN损失，需调整缩放因子或检查模型稳定性。
- **特殊操作处理**：某些自定义操作可能需要显式指定精度（如注册为FP32）。

---

### **6. 实际应用场景**

- **大模型训练**：如BERT、GPT-3等，显存需求高，AMP可节省资源。
- **多任务/大数据集**：加速迭代，缩短实验周期。
- **资源受限环境**：在显存较小的GPU上训练更大批次。

---

### **总结**

自动混合精度（AMP）通过智能结合FP16和FP32，在几乎不损失模型精度的情况下，显著提升了训练效率和资源利用率。借助现代深度学习框架（如PyTorch、TensorFlow）的内置支持，开发者只需添加少量代码即可启用AMP，是训练大规模模型的必备优化技术。
